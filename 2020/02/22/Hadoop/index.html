<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  
<script type="text/javascript" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js"></script>

  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"gjq-666.github.io","root":"/","scheme":"Muse","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="一、概述1.1 大数据概念大数据是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力来适应海量、高增长率和多样化的信息资产。 无法在一定时间范围内，使用常规软件工具对其内容进行抓取，管理和处理的数据集。 1.2 大数据面临的问题存储：单机存储有限，需要使用集群（多台机器）存储数据；硬件上必须有足够的存储容量，软件上有对应的容灾机制。 分析：单机算力有限，也需要使用集群进行计算（需要在合理">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop">
<meta property="og:url" content="https://gjq-666.github.io/2020/02/22/Hadoop/index.html">
<meta property="og:site_name" content="welcome gjq-666">
<meta property="og:description" content="一、概述1.1 大数据概念大数据是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力来适应海量、高增长率和多样化的信息资产。 无法在一定时间范围内，使用常规软件工具对其内容进行抓取，管理和处理的数据集。 1.2 大数据面临的问题存储：单机存储有限，需要使用集群（多台机器）存储数据；硬件上必须有足够的存储容量，软件上有对应的容灾机制。 分析：单机算力有限，也需要使用集群进行计算（需要在合理">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/622762d0f703918f3c528de35c3d269759eec41c.jpg">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/ssh%E5%85%8D%E5%AF%86%E7%99%BB%E9%99%86-1568702316720.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/hdfsarchitecture.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568773592079.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568776795084.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568778812355.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568789485184.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568860846937.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/yarn_architecture.gif">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/webwxgetmsgimg.jpg">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568944749720.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1768269-20190829210253564-211954667.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568950626404.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568963134494.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568963471055.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568963766792.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568964000325.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568964277657.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568964431696.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568966050505.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568964863438.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568964930859.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568965043669.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568965159913.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568970205814.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1568971578955.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1569136700216.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1569138853383.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1569140142429.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1569206721162.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1569206773669.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1569208873544.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1558528156916.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1569221806545.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1569222214803.png">
<meta property="og:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/1569228212103.png">
<meta property="article:published_time" content="2020-02-22T11:16:55.544Z">
<meta property="article:modified_time" content="2019-11-22T01:39:51.703Z">
<meta property="article:author" content="骚白">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gjq-666.github.io/2020/02/22/Hadoop/622762d0f703918f3c528de35c3d269759eec41c.jpg">

<link rel="canonical" href="https://gjq-666.github.io/2020/02/22/Hadoop/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Hadoop | welcome gjq-666</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
    <!-- <a href="https://github.com/" target="_blank" rel="noopener"><img width="149" height="149" src="https://github.blog/wp-content/uploads/2008/12/forkme_left_darkblue_121621.png?resize=149%2C149" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a> -->

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Navigationsleiste an/ausschalten">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">welcome gjq-666</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Startseite</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archiv</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://gjq-666.github.io/2020/02/22/Hadoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/timg2.jpg">
      <meta itemprop="name" content="骚白">
      <meta itemprop="description" content="生活不是等待暴风雨过去，而是学会在暴风雨中翱翔">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome gjq-666">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Hadoop
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>

              <time title="Erstellt: 2020-02-22 19:16:55" itemprop="dateCreated datePublished" datetime="2020-02-22T19:16:55+08:00">2020-02-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Bearbeitet am</span>
                <time title="Geändert am: 2019-11-22 09:39:51" itemprop="dateModified" datetime="2019-11-22T09:39:51+08:00">2019-11-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h1><h2 id="1-1-大数据概念"><a href="#1-1-大数据概念" class="headerlink" title="1.1 大数据概念"></a>1.1 大数据概念</h2><p>大数据是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力来适应海量、高增长率和多样化的信息资产。</p>
<p>无法在一定时间范围内，使用常规软件工具对其内容进行抓取，管理和处理的数据集。</p>
<h2 id="1-2-大数据面临的问题"><a href="#1-2-大数据面临的问题" class="headerlink" title="1.2 大数据面临的问题"></a>1.2 大数据面临的问题</h2><p><code>存储</code>：单机存储有限，需要使用集群（多台机器）存储数据；硬件上必须有足够的存储容量，软件上有对应的容灾机制。</p>
<p><code>分析</code>：单机算力有限，也需要使用集群进行计算（需要在合理的时间内将数据变废为宝）</p>
<h2 id="1-3-大数据的特点"><a href="#1-3-大数据的特点" class="headerlink" title="1.3 大数据的特点"></a>1.3 大数据的特点</h2><blockquote>
<p>4V Volume 数据量大  Velocity 时效性   Variety 多样性 Value 价值大</p>
</blockquote>
<h3 id="1）数据量大"><a href="#1）数据量大" class="headerlink" title="1）数据量大"></a>1）数据量大</h3><p>B-KB-MB-GB-TB-PB-EB-ZB….</p>
<p>各种个人云存储解决方案：百度网盘、腾讯微云、115、lanzou、诚通、OneDriver、GoogleDriver 等</p>
<p>大数据产生于21世纪的互联网时代，日益进步的科技和日益增长的物质文化需求，导致了数据的大爆炸；</p>
<p>淘宝、支付宝、微信、QQ、抖音这些App是目前国内顶尖的流量，使用人数及其的庞大，每天可以产生极多的数据量。</p>
<h3 id="2）数据时效性"><a href="#2）数据时效性" class="headerlink" title="2）数据时效性"></a>2）数据时效性</h3><p>双十一、618</p>
<p>大数据是在短时间内迅速产生（产生的时效性非常高），分析的时效性就必须因场景而异，需要在合理的时间内分析出有价值的数据。</p>
<h3 id="3）数据多样性"><a href="#3）数据多样性" class="headerlink" title="3）数据多样性"></a>3）数据多样性</h3><h4 id="（1）数据存储类型多样性"><a href="#（1）数据存储类型多样性" class="headerlink" title="（1）数据存储类型多样性"></a>（1）数据存储类型多样性</h4><p>结构化的数据：表格、文本、SQL等</p>
<p>非结构化数据：视频、音频、图片</p>
<h4 id="（2）数据分析类型多样性"><a href="#（2）数据分析类型多样性" class="headerlink" title="（2）数据分析类型多样性"></a>（2）数据分析类型多样性</h4><p>地理位置：来自北京、中国、上海</p>
<p>设备信息：来自PC、手机、平板、手表、手环、眼镜</p>
<p>个人喜好：美女、面膜、ctrl、 数码、篮球、足球</p>
<p>社交网络：A可能认识B 、C ，B就可能认识C</p>
<p>电话号码：110,11086</p>
<p>网络身份证：设备MAC+电话+IP+地区</p>
<h3 id="4）数据价值"><a href="#4）数据价值" class="headerlink" title="4）数据价值"></a>4）数据价值</h3><p>警察叔叔：只关注的是否哪里违规</p>
<p>AI研究：犯罪预测、下棋、无人驾驶</p>
<p>所以在海量数据中有用的数据最为关键、这是分析数据的第一步，也就是对数据进行降噪处理（数据清洗|数据预处理）</p>
<h2 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4 应用场景"></a>1.4 应用场景</h2><h3 id="1）个人推荐"><a href="#1）个人推荐" class="headerlink" title="1）个人推荐"></a>1）个人推荐</h3><p>根据用户喜好，推荐相关资源</p>
<p>千人一面、千人千面、一人千面</p>
<h3 id="2）风控"><a href="#2）风控" class="headerlink" title="2）风控"></a>2）风控</h3><p>大数据实时流处理，根据用户行为模型进行支撑，判断该行为是否正常 </p>
<h3 id="3）成本预测"><a href="#3）成本预测" class="headerlink" title="3）成本预测"></a>3）成本预测</h3><h3 id="4）气候预测"><a href="#4）气候预测" class="headerlink" title="4）气候预测"></a>4）气候预测</h3><h3 id="5）人工智能"><a href="#5）人工智能" class="headerlink" title="5）人工智能"></a>5）人工智能</h3><h2 id="1-5-工作方向"><a href="#1-5-工作方向" class="headerlink" title="1.5 工作方向"></a>1.5 工作方向</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 业务</span><br><span class="line">电商推荐、智能广告系统、专家系统、智能交通、智能医疗</span><br><span class="line">2 工作方向</span><br><span class="line"> 大数据开发工程师（实时计算、批处理、ETL、数据挖掘）、大数据运维工程师</span><br></pre></td></tr></table></figure>

<h2 id="1-6分布式"><a href="#1-6分布式" class="headerlink" title="1.6分布式"></a>1.6分布式</h2><p>为了解决大数据存储和计算的问题，需要使用一定数量的机器，硬件设施必须足够，那软件解决方案怎么办？</p>
<p>如何使用软件去解决存储和分析的问题？</p>
<h1 id="二、Hadoop"><a href="#二、Hadoop" class="headerlink" title="二、Hadoop"></a>二、Hadoop</h1><p><img src="/2020/02/22/Hadoop/622762d0f703918f3c528de35c3d269759eec41c.jpg" alt="img"></p>
<p>Hadoop由 Apache Software Foundation 公司于 2005 年秋天作为<a href="https://baike.baidu.com/item/Lucene" target="_blank" rel="noopener">Lucene</a>的子项目<a href="https://baike.baidu.com/item/Nutch" target="_blank" rel="noopener">Nutch</a>的一部分正式引入。它受到最先由 Google Lab 开发的 Map/Reduce 和 Google File System(<a href="https://baike.baidu.com/item/GFS" target="_blank" rel="noopener">GFS</a>) 的启发。</p>
<p>2006 年 3 月份，Map/Reduce 和 Nutch Distributed File System (NDFS) 分别被纳入称为 Hadoop 的项目中。</p>
<p>Hadoop 是最受欢迎的在 Internet 上对搜索<a href="https://baike.baidu.com/item/关键字" target="_blank" rel="noopener">关键字</a>进行内容分类的工具，但它也可以解决许多要求极大伸缩性的问题。例如，如果您要 grep 一个 10TB 的巨型文件，会出现什么情况？在传统的系统上，这将需要很长的时间。但是 Hadoop 在设计时就考虑到这些问题，采用<a href="https://baike.baidu.com/item/并行执行" target="_blank" rel="noopener">并行执行</a>机制，因此能大大提高效率。</p>
<p><code>HDFS</code>：Hadoop Distributed File System  作为Hadoop 生态体系中数据的存储的软件解决方案</p>
<p><code>MapReduce</code>：Hadoop中分布式计算框架（只需要实现少量的代码，就可以开发一个分布式的应用程序），对海量数据并行分析和计算</p>
<h2 id="2-1-Hadoop生态系统"><a href="#2-1-Hadoop生态系统" class="headerlink" title="2.1 Hadoop生态系统"></a>2.1 Hadoop生态系统</h2><p><code>HDFS</code>：Hadoop Distributed File System  作为Hadoop 生态体系中数据的存储的软件解决方案</p>
<p><code>MapReduce</code>：Hadoop中分布式计算框架（只需要实现少量的代码，就可以开发一个分布式的应用程序），对海量数据并行分析和计算</p>
<p><code>HBase</code>: 基于HDFS 的列式存储的NoSql</p>
<p><code>Hive</code>:是一款SQL解释引擎，能够将SQL语句翻译成MR代码</p>
<p><code>Flume</code>:分布式的日志收集系统，用于收集海量日志数据，并将其存储在hdfS中</p>
<p><code>kafka</code>:消息对列，实现对分布式应用程序间的解耦和数据缓冲</p>
<p><code>Zookeeper</code>：分布式协调服务，用户注册中心、配置中心、集群选举、状态检测、分布式锁</p>
<h2 id="2-2-大数据分析方案"><a href="#2-2-大数据分析方案" class="headerlink" title="2.2 大数据分析方案"></a>2.2 大数据分析方案</h2><p><code>MapReduce</code>:大数据离线批处理（代表基于磁盘，延迟30分钟+）</p>
<p><code>Spark</code>：大数据离线批处理（代表基于内存，速度相对于MR来说快的多）</p>
<p><code>Strom/Spark Streaming/Kafka Streaming/Flink</code>:实时流处理框架，达到对记录级别消息的毫秒级处理</p>
<h1 id="三、HDFS"><a href="#三、HDFS" class="headerlink" title="三、HDFS"></a>三、HDFS</h1><h2 id="3-1-安装（伪集群）"><a href="#3-1-安装（伪集群）" class="headerlink" title="3.1 安装（伪集群）"></a>3.1 安装（伪集群）</h2><h3 id="1）准备虚拟机"><a href="#1）准备虚拟机" class="headerlink" title="1）准备虚拟机"></a>1）准备虚拟机</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">更改IP</span><br><span class="line">删除MAC地址 </span><br><span class="line">更改主机名     vi &#x2F;etc&#x2F;sysconfig&#x2F;network</span><br></pre></td></tr></table></figure>

<h3 id="2）安装JDK-8"><a href="#2）安装JDK-8" class="headerlink" title="2）安装JDK 8"></a>2）安装JDK 8</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure>

<h3 id="3）配置Java环境变量"><a href="#3）配置Java环境变量" class="headerlink" title="3）配置Java环境变量"></a>3）配置Java环境变量</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">export</span> <span class="string">JAVA_HOME=/usr/java/jdk1.8.0_171-amd64/</span></span><br><span class="line"><span class="attr">export</span> <span class="string">PATH=$PATH:$JAVA_HOME/bin</span></span><br></pre></td></tr></table></figure>

<h3 id="4）配置主机名与IP的映射关系"><a href="#4）配置主机名与IP的映射关系" class="headerlink" title="4）配置主机名与IP的映射关系"></a>4）配置主机名与IP的映射关系</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]#  vi /etc/sysconfig/network</span><br><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=HadoopNode00</span><br><span class="line"></span><br><span class="line">[root@HadoopNode00 ~]# vi /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.11.20 HadoopNode00</span><br></pre></td></tr></table></figure>

<h3 id="5）关闭防火墙"><a href="#5）关闭防火墙" class="headerlink" title="5）关闭防火墙"></a>5）关闭防火墙</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# service iptables stop   #  关闭防火墙</span><br><span class="line">[root@HadoopNode00 ~]# chkconfig iptables off  # 关闭防火墙开机自动启动</span><br></pre></td></tr></table></figure>

<h3 id="6）ssh免密登陆"><a href="#6）ssh免密登陆" class="headerlink" title="6）ssh免密登陆"></a>6）ssh免密登陆</h3><p>SSH是Secure Shell 的缩写，SSH为建立在应用层山的安全协议，专为远程登陆会话和其他网络服务提供安全协议支持。</p>
<p><code>基于口令的安全验证</code>：基于用户名和密码  root | 123456</p>
<p>基于密钥的安全验证：需要依靠密钥</p>
<p><img src="/2020/02/22/Hadoop/ssh%E5%85%8D%E5%AF%86%E7%99%BB%E9%99%86-1568702316720.png" alt="ssh免密登陆"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# ssh-keygen -t rsa   # 生成密钥</span><br><span class="line">[root@HadoopNode00 ~]# ssh-copy-id HadoopNOde00</span><br></pre></td></tr></table></figure>

<h3 id="7）解压Hadoop"><a href="#7）解压Hadoop" class="headerlink" title="7）解压Hadoop"></a>7）解压Hadoop</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">解压Hadoop到指定目录</span><br><span class="line">[root@HadoopNode00 ~]# mkdir /home/hadoop/</span><br><span class="line">[root@HadoopNode00 ~]# tar -zxvf /home/hadoop/hadoop-2.6.0.tar.gz  -C /home/hadoop</span><br></pre></td></tr></table></figure>

<h3 id="8）配置Hadoop环境变量"><a href="#8）配置Hadoop环境变量" class="headerlink" title="8）配置Hadoop环境变量"></a>8）配置Hadoop环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/home/hadoop/hadoop-2.6.0</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>HADOOP_HOME</code>环境变量别第三方依赖，hbase hive flume在集成HADOOP的时候，是通过HADOOP_HOME找到hadoop的位置</p>
</blockquote>
<h3 id="9）配置-etc-hadoop-core-site-xml"><a href="#9）配置-etc-hadoop-core-site-xml" class="headerlink" title="9）配置 etc/hadoop/core-site.xml"></a>9）配置 etc/hadoop/core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://HadoopNode00:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop-2.6.0/hadoop-$&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="10）配置-etc-hadoop-hdfs-site-xml"><a href="#10）配置-etc-hadoop-hdfs-site-xml" class="headerlink" title="10）配置 etc/hadoop/hdfs-site.xml"></a>10）配置 etc/hadoop/hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="11）格式化namenode"><a href="#11）格式化namenode" class="headerlink" title="11）格式化namenode"></a>11）格式化namenode</h3><blockquote>
<p>第一次启动hdfs的时候，需要格式化namenode</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hdfs namenode -format</span><br><span class="line">[root@HadoopNode00 ~]# tree /home/hadoop/hadoop-2.6.0/hadoop-root</span><br><span class="line">/home/hadoop/hadoop-2.6.0/hadoop-root</span><br><span class="line">└── dfs</span><br><span class="line">    └── name</span><br><span class="line">        └── current</span><br><span class="line">            ├── fsimage_0000000000000000000</span><br><span class="line">            ├── fsimage_0000000000000000000.md5</span><br><span class="line">            ├── seen_txid</span><br><span class="line">            └── VERSION</span><br><span class="line"></span><br><span class="line">3 directories, 4 files</span><br></pre></td></tr></table></figure>

<h3 id="12）启动hdfs"><a href="#12）启动hdfs" class="headerlink" title="12）启动hdfs"></a>12）启动hdfs</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh   # 开启HDFS </span><br><span class="line">stop-dfs.sh    # 关闭hdfs</span><br></pre></td></tr></table></figure>

<p>进入web界面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;主机名:50070</span><br></pre></td></tr></table></figure>

<blockquote>
<p>windows下 配置域名与ip的映射：C:\Windows\System32\drivers\etc \hosts</p>
</blockquote>
<h2 id="3-2-HDFS-Shell-相关操作"><a href="#3-2-HDFS-Shell-相关操作" class="headerlink" title="3.2 HDFS Shell 相关操作"></a>3.2 HDFS Shell 相关操作</h2><h3 id="1）hdfs-shell"><a href="#1）hdfs-shell" class="headerlink" title="1）hdfs shell"></a>1）hdfs shell</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">        [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">        [-checksum &lt;src&gt; ...]</span><br><span class="line">        [-chgrp [-R] GROUP PATH...]</span><br><span class="line">        [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">        [-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">        [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">        [-count [-q] [-h] &lt;path&gt; ...]</span><br><span class="line">        [-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">        [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">        [-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">        [-du [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">        [-expunge]</span><br><span class="line">        [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">        [-getfacl [-R] &lt;path&gt;]</span><br><span class="line">        [-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">        [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">        [-help [cmd ...]]</span><br><span class="line">        [ ..]</span><br><span class="line">        [-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value for given property</span><br><span class="line">-fs &lt;local|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include in the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general command line syntax is</span><br><span class="line">bin/hadoop command [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>



<h3 id="2）上传文件"><a href="#2）上传文件" class="headerlink" title="2）上传文件"></a>2）上传文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 上传 root目录下的install.log  到hdfs 根目录下</span></span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -put  /root/install.log  /1.txt</span><br></pre></td></tr></table></figure>

<h3 id="3-）-ls文件"><a href="#3-）-ls文件" class="headerlink" title="3 ） ls文件"></a>3 ） ls文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 找到到了刚才上传为文件命名为1.txt</span></span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup       8901 2019-09-17 23:28 /1.txt</span><br></pre></td></tr></table></figure>

<h3 id="4）下载文件"><a href="#4）下载文件" class="headerlink" title="4）下载文件"></a>4）下载文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs -get  /1.txt /root/baizhi.txt</span><br></pre></td></tr></table></figure>

<h3 id="5）删除文件"><a href="#5）删除文件" class="headerlink" title="5）删除文件"></a>5）删除文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs -rm /2.txt</span><br><span class="line">19/09/17 23:36:05 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.</span><br><span class="line">Deleted /2.txt</span><br></pre></td></tr></table></figure>

<h3 id="6）查看文件"><a href="#6）查看文件" class="headerlink" title="6）查看文件"></a>6）查看文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs -cat /1.txt</span><br><span class="line">Installing libgcc-4.4.7-23.el6.x86_64</span><br><span class="line">warning: libgcc-4.4.7-23.el6.x86_64: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY</span><br><span class="line">Installing setup-2.8.14-23.el6.noarch</span><br></pre></td></tr></table></figure>

<h3 id="7）创建文件夹"><a href="#7）创建文件夹" class="headerlink" title="7）创建文件夹"></a>7）创建文件夹</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs -mkdir /baizhi</span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 root supergroup       8901 2019-09-17 23:28 /1.txt</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-09-17 23:37 /baizhi</span><br></pre></td></tr></table></figure>

<h3 id="8）复制文件"><a href="#8）复制文件" class="headerlink" title="8）复制文件"></a>8）复制文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs -cp /1.txt /baizhi/</span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 root supergroup       8901 2019-09-17 23:28 /1.txt</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-09-17 23:38 /baizhi</span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -ls /baizhi</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup       8901 2019-09-17 23:38 /baizhi/1.txt</span><br></pre></td></tr></table></figure>



<h3 id="9）开启回收站机制"><a href="#9）开启回收站机制" class="headerlink" title="9）开启回收站机制"></a>9）开启回收站机制</h3><p>core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>设置一分钟延迟</p>
</blockquote>
<h2 id="3-3-Java-API-操作HDFS"><a href="#3-3-Java-API-操作HDFS" class="headerlink" title="3.3  Java API 操作HDFS"></a>3.3  Java API 操作HDFS</h2><h3 id="（1）-依赖"><a href="#（1）-依赖" class="headerlink" title="（1） 依赖"></a>（1） 依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="（2）Windows-配置Hadoop环境"><a href="#（2）Windows-配置Hadoop环境" class="headerlink" title="（2）Windows 配置Hadoop环境"></a>（2）Windows 配置Hadoop环境</h3><ul>
<li>解压hadoop到指定的目录</li>
<li>拷贝hadoop.dll和winutils.exe到hadoop/bin 目录下</li>
<li>配置Hadoop环境变量</li>
<li>配置主机名和IP的映射关系</li>
</ul>
<h3 id="（3）权限不足解决方案"><a href="#（3）权限不足解决方案" class="headerlink" title="（3）权限不足解决方案"></a>（3）权限不足解决方案</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.security.AccessControlException: Permission denied: user=Administrator, access=WRITE, inode=<span class="string">"/baizhi"</span>:root:supergroup:drwxr-xr-x</span><br></pre></td></tr></table></figure>



<h4 id="1）配置-hdfs-site-xml"><a href="#1）配置-hdfs-site-xml" class="headerlink" title="1）配置 hdfs-site.xml"></a>1）配置 hdfs-site.xml</h4><blockquote>
<p>将权限检查关闭</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="2）方案2"><a href="#2）方案2" class="headerlink" title="2）方案2"></a>2）方案2</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-DHADOOP_USER_NAME&#x3D;root</span><br></pre></td></tr></table></figure>

<h4 id="3）方案3"><a href="#3）方案3" class="headerlink" title="3）方案3"></a>3）方案3</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"root"</span>);</span><br></pre></td></tr></table></figure>

<h3 id="（3）相关操作"><a href="#（3）相关操作" class="headerlink" title="（3）相关操作"></a>（3）相关操作</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"><span class="keyword">import</span> org.junit.experimental.theories.suppliers.TestedOn;</span><br><span class="line"><span class="keyword">import</span> sun.awt.geom.AreaOp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Configuration configuration;</span><br><span class="line">    <span class="keyword">private</span> FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getClient</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//配置读写权限</span></span><br><span class="line">        System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"root"</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 准备配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 添加相应的配置文件*/</span></span><br><span class="line">        configuration.addResource(<span class="string">"core-site.xml"</span>);</span><br><span class="line">        configuration.addResource(<span class="string">"hdfs-site.xml"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 通过FileSystem.newInstance 获得客户端对象*/</span></span><br><span class="line">        fileSystem = FileSystem.newInstance(configuration);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testUpload01</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 源文件  |   目标文件</span></span><br><span class="line"><span class="comment">         * Path 对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        fileSystem.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"G:\\A.docx"</span>), <span class="keyword">new</span> Path(<span class="string">"/baizhi/2.docx"</span>));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testUpload02</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 准备 本地输入流</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="string">"G:\\A.docx"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 准备 hdfs 输出流</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        FSDataOutputStream outputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/baizhi/3.docx"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 使用工具类进行拷贝</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        IOUtils.copyBytes(inputStream, outputStream, <span class="number">1024</span>, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDownload01</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        fileSystem.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"G:\\3.txt"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDownload02</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        FileOutputStream outputStream = <span class="keyword">new</span> FileOutputStream(<span class="string">"G:\\4.txt"</span>);</span><br><span class="line"></span><br><span class="line">        FSDataInputStream inputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>));</span><br><span class="line">        IOUtils.copyBytes(inputStream, outputStream, <span class="number">1024</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test011</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        RemoteIterator&lt;LocatedFileStatus&gt; list = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (list.hasNext()) &#123;</span><br><span class="line"></span><br><span class="line">            LocatedFileStatus locatedFileStatus = list.next();</span><br><span class="line">            Path path = locatedFileStatus.getPath();</span><br><span class="line">            System.out.println(path.toString());</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test02</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">        fileSystem.delete(<span class="keyword">new</span> Path(<span class="string">"/baizhi"</span>),<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test03</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> exists = fileSystem.exists(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>));</span><br><span class="line">        <span class="keyword">if</span> (exists)&#123;</span><br><span class="line">            System.out.println(<span class="string">"文件存在"</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">            System.out.println(<span class="string">"文件不存在"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testy04</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">        fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/baizhi1243"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="3-4-HDFS-Architecture（架构-）"><a href="#3-4-HDFS-Architecture（架构-）" class="headerlink" title="3.4 HDFS Architecture（架构 ）"></a>3.4 HDFS Architecture（架构 ）</h2><p>HDFS为主从架构，HDFS中有一个主的NameNode，管理系统命名空间和管理客户端对文件的访问，其中还有DataNode负责和NameNode进行协调工作，DataNode负责数据的存储，在存储数据（文件）的过程中一个文件会被分成一个块或者多个块，在NameNode中存储了一些数据（存储的数据是块到DataNode的映射关系），datanode还根据NameNode的指令创建删除复制块。</p>
<p><img src="/2020/02/22/Hadoop/hdfsarchitecture.png" alt="HDFS Architecture"></p>
<p><code>namenode</code>:存储元数据（用户描述数据的数据），负责管理DataNode</p>
<p><code>datanode</code>：用于存储数据块的节点，负责响应客户端的对块的读写请求，向NameNode汇报自己的块信息</p>
<p><code>block块</code>：数据块，hdfs中对文件拆分的最小单元，切分尺度默认为128MB，每个块在默认情况下有三个副本</p>
<p><code>rack</code>：机架，使用机架配置文件对存储节点进行物理编排，用于优化存储和计算</p>
<h3 id="1）什么是Block块"><a href="#1）什么是Block块" class="headerlink" title="1）什么是Block块"></a>1）什么是Block块</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The default block size for new files, in bytes.</span><br><span class="line">      You can use the following suffix (case insensitive):</span><br><span class="line">      k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.),</span><br><span class="line">      Or provide complete size in bytes (such as 134217728 for 128 MB).</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p><img src="/2020/02/22/Hadoop/1568773592079.png" alt="1568773592079"></p>
<h3 id="（1）为什么块的大小为128MB？"><a href="#（1）为什么块的大小为128MB？" class="headerlink" title="（1）为什么块的大小为128MB？"></a>（1）为什么块的大小为128MB？</h3><p>在Hadoop1.x 块大小默认为64MB，在Hadoop2.x 默认为128MB</p>
<p>工业限制：一般来说机械硬盘的读取速度100MB左右</p>
<p>软件优化：通常认为最佳状态为寻址时间为传输时间的100分之一</p>
<h3 id="（2）Block块的大小能否随意设置？"><a href="#（2）Block块的大小能否随意设置？" class="headerlink" title="（2）Block块的大小能否随意设置？"></a>（2）Block块的大小能否随意设置？</h3><p>不能，如果BlockSize过大，可能导致多余存储空间浪费，导致存取时间过长 如果BlockSize过小，会导致寻址时间过长，同样造成效率低下。</p>
<h3 id="（3）HDFS为什么不适合存储小文件"><a href="#（3）HDFS为什么不适合存储小文件" class="headerlink" title="（3）HDFS为什么不适合存储小文件"></a>（3）HDFS为什么不适合存储小文件</h3><table>
<thead>
<tr>
<th>文件</th>
<th>namenode内存占用</th>
<th>datanode磁盘占用</th>
</tr>
</thead>
<tbody><tr>
<td>128MB 单文件</td>
<td>1个Blcok元数据的大小</td>
<td>128MB</td>
</tr>
<tr>
<td>128*1MB</td>
<td>128个Block元数据的大小</td>
<td>128MB</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>namenode内存会过于紧张</p>
<h3 id="2）Rack-Awareness-机架感知"><a href="#2）Rack-Awareness-机架感知" class="headerlink" title="2）Rack Awareness  机架感知"></a>2）Rack Awareness  机架感知</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于常见情况，当复制因子为3时，HDFS的放置策略是将一个副本放在本地机架中的一个节点上，另一个放在本地机架中的另一个节点上，将最后一个放在另一个机架中的另一个节点上。此策略可以减少机架间写入流量，从而提高写入性能。机架故障的可能性远小于节点故障的可能性;此策略不会影响数据可靠性和可用性保证。但是，它确实减少了读取数据时使用的聚合网络带宽，因为块只放在两个唯一的机架而不是三个。使用此策略时，文件的副本不会均匀分布在机架上。三分之一的副本位于一个节点上，三分之二的副本位于一个机架上，另外三个副本均匀分布在剩余的机架上。此策略可提高写入性能，而不会影响数据可靠性或读取性能。</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/22/Hadoop/1568776795084.png" alt="1568776795084"></p>
<p>查看默认机架</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hdfs  dfsadmin  -printTopology</span><br><span class="line">Rack: /default-rack</span><br><span class="line">   192.168.11.20:50010 (HadoopNode00)</span><br></pre></td></tr></table></figure>



<h3 id="3）NameNode-和-SecondaryNameNode-的-关系-（重点）"><a href="#3）NameNode-和-SecondaryNameNode-的-关系-（重点）" class="headerlink" title="3）NameNode 和 SecondaryNameNode 的 关系 （重点）"></a>3）NameNode 和 SecondaryNameNode 的 关系 （重点）</h3><p>fsimage文件：元数据信息的备份，会被加载到内存中</p>
<p>edits文件：Edits文件帮助记录增加和更新操作，提高效率</p>
<p>namenode在启动时会加载fsimage和edits的文件，所以在第一次启动的时候需要格式化namenode</p>
<p>当用户上传文件的时候或者进行其他操作的时候，操作记录会写入edits文件中，这样edits和fsimage文件加起来的元数据永远是最新的。</p>
<p>如果此时用户一直进行操作的话，edits文件会越来越大，这就导致了在下次启动的时候启动速度过慢。</p>
<p>为了解决这个问题，出现了SecondaryNameNode ，将当前的NameNode的edits和fsimage文件拷贝到自己的节点上，进行合并操作，在合并完成后，将新的fsimage文件传输到原来的namenode中，此时namanode再去加载最新的fsimage。</p>
<p>新的问题：在SecondaryNameNode 进行拷贝操作的时候，如果有客户端读写请求过来，势必要追加相应的操作记录到edits文件中，但是此时正在进行拷贝操作，改变则代表会造成数据紊乱，怎么办？解办法是：会有一个新的叫做edits-inprogress的文件被创建，新的操作将写入此文件中，等待SecondaryNameNode合并完成，将edits-inprogress文件改名成为当前的edits文件。</p>
<p><img src="/2020/02/22/Hadoop/1568778812355.png" alt="1568778812355"></p>
<h3 id="4）检查点"><a href="#4）检查点" class="headerlink" title="4）检查点"></a>4）检查点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">namenode使用fsimage和edits文件保存元数据，2nn会定期的下载主的（Active）namenode的fsimage文件和edits 文件，并在本地进行合并。</span><br><span class="line">合并的时机就称之为检查点</span><br><span class="line">检查点有两种触发机制：</span><br><span class="line">（1） 默认一个小时进行合并</span><br><span class="line">（2） 操作数量达到100W次进行合并</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    The number of seconds between two periodic checkpoints.</span><br><span class="line">    Support multiple time unit suffix(case insensitive), as described</span><br><span class="line">    in dfs.heartbeat.interval.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The Secondary NameNode or CheckpointNode will create a checkpoint</span><br><span class="line">  of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless</span><br><span class="line">  of whether 'dfs.namenode.checkpoint.period' has expired.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="5）Safemode"><a href="#5）Safemode" class="headerlink" title="5）Safemode"></a>5）Safemode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在启动时候加载fsimage和edits文件，等待其他的DataNode报告块信息，直至大部分块可用。在次期间，集群处于SafeMode，NameNode的安全模式本质上是HDFS集群的只读模式，它不允许对文件系统或块进行任何修改。</span><br><span class="line">通常，在DataNode报告大多数文件系统块可用之后，NameNode会自动离开Safemode。</span><br><span class="line">可以手动的进入或者退出SafeMode</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hdfs dfsadmin -safemode  enter</span><br><span class="line">Safe mode is ON</span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -put /root/1.txt  /</span><br><span class="line">put: Cannot create file/1.txt._COPYING_. Name node is in safe mode.</span><br><span class="line">[root@HadoopNode00 ~]# hdfs dfsadmin -safemode  leave</span><br><span class="line">Safe mode is OFF</span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -put /root/1.txt  /</span><br></pre></td></tr></table></figure>

<h3 id="6）DataNode工作机制"><a href="#6）DataNode工作机制" class="headerlink" title="6）DataNode工作机制"></a>6）DataNode工作机制</h3><p><img src="/2020/02/22/Hadoop/1568789485184.png" alt="1568789485184"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动的时候会注册DataNode</span><br><span class="line">周期向NameNode上报块信息，并且对其信息状态进行反馈，DataNode进行相应的操作</span><br><span class="line">心跳不能出现10分钟以上的断连，必须重启DataNode才能重现上线</span><br></pre></td></tr></table></figure>

<h1 id="四、MapReduce"><a href="#四、MapReduce" class="headerlink" title="四、MapReduce"></a>四、MapReduce</h1><h2 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h2><p>MapReduce是一种编程模型，用于大规模数据集（大于1TB）的并行运算。概念”Map（映射）”和”Reduce（归约）”，是它们的主要思想，都是从函数式编程语言里借来的，还有从矢量编程语言里借来的特性。它极大地方便了编程人员在不会分布式并行编程的情况下，将自己的程序运行在<a href="https://baike.baidu.com/item/分布式系统/4905336" target="_blank" rel="noopener">分布式系统</a>上。 当前的软件实现是指定一个Map（映射）函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce（归约）函数，用来保证所有映射的键值对中的每一个共享相同的键组。</p>
<p><code>MapReduce</code>是Hadoop框架的一个<code>并行计算框架</code>,将一个计算任务拆分成两个阶段：Map和Reduce</p>
<p>MapReduce计算框架充分利用了 存储节点（DataNode）所在物理主机的计算资源进行并行计算</p>
<p>默认情况下NodeManager会将本进程运行 的节点的计算资源抽像成8个计算单元，每个单元称之为一个<code>Contioner</code>，所有的NodeManager都由ResourceManager调度，ResourceManager负责计算资源的统筹分配。</p>
<blockquote>
<p>一是软件框架   二是并行处理   三 可靠容错  四 大规模计算   五 处理海量数据</p>
</blockquote>
<p><img src="/2020/02/22/Hadoop/1568860846937.png" alt="1568860846937"></p>
<p>MapReduce擅长做大数据处理，MapReduce的思想就是<code>分而治之</code></p>
<ul>
<li><p>Map负责<strong>”分“</strong>，即把庞大且复杂的任务分解成若干个”简单的任务“来处理，简单的任务包含三层</p>
<ul>
<li>是对数据或者计算模型相对于原任务要大大缩小</li>
<li>就近计算原则，就是任务会被分配到存放所需数据的节点上进行计算</li>
<li>这些小任务不止一个且并行计算，而且彼此间没有依赖关系</li>
</ul>
</li>
</ul>
<ul>
<li>Reducer负责对Map的计算结果<strong>进行汇总</strong></li>
</ul>
<h2 id="4-2-为什么使用MR？"><a href="#4-2-为什么使用MR？" class="headerlink" title="4.2 为什么使用MR？"></a>4.2 为什么使用MR？</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileWriter;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CleanApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        File file = <span class="keyword">new</span> File(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\access.tmp2019-05-19-10-28.log"</span>);</span><br><span class="line"></span><br><span class="line">        BufferedReader bufferedReader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> FileReader(file));</span><br><span class="line"></span><br><span class="line">        FileWriter fileWriter = <span class="keyword">new</span> FileWriter(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\clean.log"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">            String line = bufferedReader.readLine();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (line == <span class="keyword">null</span>) &#123;</span><br><span class="line">                bufferedReader.close();</span><br><span class="line">                fileWriter.close();</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">boolean</span> contains = line.contains(<span class="string">"thisisshortvideoproject'slog"</span>);</span><br><span class="line">            <span class="keyword">if</span> (contains) &#123;</span><br><span class="line"></span><br><span class="line">                String s = line.split(<span class="string">"thisisshortvideoproject'slog"</span>)[<span class="number">0</span>];</span><br><span class="line">                fileWriter.write(s.trim() + <span class="string">"\n"</span>);</span><br><span class="line">                fileWriter.flush();</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述代码是对日志进行简单的清晰，在数据量少的时候一点问题都没有，但是数据量一旦增加，就可能无法胜任需求，因为无法在合理的时间内完成计算，此时单机性能已经成为计算的瓶颈，但是手写分布式应用程序难度太大，有现成的框架可以使用，那就是MR!</p>
<h2 id="4-3-YARN-环境搭建"><a href="#4-3-YARN-环境搭建" class="headerlink" title="4.3 YARN  环境搭建"></a>4.3 YARN  环境搭建</h2><h3 id="（1）什么是-YARN-？"><a href="#（1）什么是-YARN-？" class="headerlink" title="（1）什么是 YARN ？"></a>（1）什么是 YARN ？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Yarn作为一个资源调度平台,有一个全局的管理者叫做ResourceManager，ResourceManager负责对集群的整体计算及资源做统筹规划，有各个节点的管理者叫做NodeManager，负责向ResourceManager报告其计算资源的使用情况，在NodeManger中有一个MRAppMaster管理这里当前运行的MRApp，其任务是协调来自ResourceManager的资源，并与NodeManager一起执行和监视任务。</span><br></pre></td></tr></table></figure>

<p><code>ResourceManager</code>：负责对集群的整体计算及资源做统筹规划</p>
<p><code>NodeManager</code>：管理主机上的计算组员，负责向RM 汇报自身的状态信息</p>
<p><code>MRAppMaster</code>：计算任务的Master，负责申请计算资源，协调计算任务</p>
<p><code>YARN Child</code>：负责做实际计算任务</p>
<p><code>Container：</code>计算资源的抽象单元</p>
<p><img src="/2020/02/22/Hadoop/yarn_architecture.gif" alt="MapReduce NextGen Architecture"></p>
<h3 id="（2）配置YARN"><a href="#（2）配置YARN" class="headerlink" title="（2）配置YARN"></a>（2）配置YARN</h3><p><code>etc/hadoop/yarn-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>The hostname of the RM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>HadoopNode00<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><code>etc/hadoop/mapred-site.xml</code></p>
<blockquote>
<p> etc/hadoop/  下其实是没有这个文件 的但是有yitmp结尾的文件，将其改名即可</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="（3）启动YARN"><a href="#（3）启动YARN" class="headerlink" title="（3）启动YARN"></a>（3）启动YARN</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# start-yarn.sh</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /home/hadoop/hadoop-2.6.0/logs/yarn-root-resourcemanager-HadoopNode00.out</span><br><span class="line">localhost: starting nodemanager, logging to /home/hadoop/hadoop-2.6.0/logs/yarn-root-nodemanager-HadoopNode00.out</span><br><span class="line">[root@HadoopNode00 ~]# jps</span><br><span class="line">60192 Jps</span><br><span class="line">60046 ResourceManager</span><br><span class="line">60142 NodeManager</span><br></pre></td></tr></table></figure>

<blockquote>
<p>web 界面： hostname:8088</p>
</blockquote>
<h2 id="4-4-MR-入门程序"><a href="#4-4-MR-入门程序" class="headerlink" title="4.4 MR 入门程序"></a>4.4 MR 入门程序</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">需求：</span><br><span class="line">wangkai gjf zkf suns gzy</span><br><span class="line">wangkai zkf suns gzy</span><br><span class="line">zkf suns gzy hxz leijun</span><br><span class="line"></span><br><span class="line">wangkai 2</span><br><span class="line">gjf 1</span><br><span class="line">zkf 3 </span><br><span class="line">suns 3</span><br><span class="line">gzy 3</span><br><span class="line">hxz 1</span><br><span class="line">leijun 1</span><br></pre></td></tr></table></figure>

<h3 id="（1）依赖"><a href="#（1）依赖" class="headerlink" title="（1）依赖"></a>（1）依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">      </span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> 	 	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-jobclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="（2）Mapper-逻辑"><a href="#（2）Mapper-逻辑" class="headerlink" title="（2）Mapper 逻辑"></a>（2）Mapper 逻辑</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* keyIn  LongWritable (Long) 输入文本字节偏移量</span></span><br><span class="line"><span class="comment">* valueIn Text (String)      输入文本行</span></span><br><span class="line"><span class="comment">*  keyOut Text(String)</span></span><br><span class="line"><span class="comment">*  valueOut IntWritable(Int)</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCMapper</span>  <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String[] names = value.toString().split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String name : names) &#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(name),<span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="（3）Reduce-逻辑"><a href="#（3）Reduce-逻辑" class="headerlink" title="（3）Reduce 逻辑"></a>（3）Reduce 逻辑</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *keyIn Text 与mapper的keyOut的数据类型相对应</span></span><br><span class="line"><span class="comment"> *valeuIn IntWritable   与mapper的ValueOut的数据类型相对应</span></span><br><span class="line"><span class="comment"> * KeyOut</span></span><br><span class="line"><span class="comment"> * valueOut</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="（4）Job封装"><a href="#（4）Job封装" class="headerlink" title="（4）Job封装"></a>（4）Job封装</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取Job对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置数据输入输出组件</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *设置数据输入输出路径</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/wordcount.txt"</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 注意： 此输出路径不能存在</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"/baizhi/out1"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置MAP 和 REDUCE 处理逻辑</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapperClass(WCMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(WCReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置 map任务和reduce任务的输出泛型</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//  提交</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.submit();</span></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="4-5-部署运行"><a href="#4-5-部署运行" class="headerlink" title="4.5 部署运行"></a>4.5 部署运行</h2><h3 id="（1）远程Jar-包部署"><a href="#（1）远程Jar-包部署" class="headerlink" title="（1）远程Jar 包部署"></a>（1）远程Jar 包部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 设置jar 类加载器 否则MapReduce框架找不到Map和Reuce</span><br><span class="line">job.setJarByClass(JobRunner.class);</span><br></pre></td></tr></table></figure>

<ul>
<li>打包</li>
<li>运行 hadoop jar 包的名字  主类名</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop jar Hadoop_Test-1.0-SNAPSHOT.jar com.baizhi.mr.test01.JobRunner</span><br></pre></td></tr></table></figure>

<h3 id="（2）本地仿真"><a href="#（2）本地仿真" class="headerlink" title="（2）本地仿真"></a>（2）本地仿真</h3><h4 id="填坑"><a href="#填坑" class="headerlink" title="填坑"></a>填坑</h4><p><img src="/2020/02/22/Hadoop/webwxgetmsgimg.jpg" alt="webwxgetmsgimg"></p>
<p><img src="/2020/02/22/Hadoop/1568944749720.png" alt="1568944749720"></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>log4j.properties</p>
</blockquote>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### 配置根 ###</span></span><br><span class="line"><span class="meta">log4j.rootLogger</span> = <span class="string">info,console</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 配置输出到控制台 ###</span></span><br><span class="line"><span class="meta">log4j.appender.console</span> = <span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.console.Target</span> = <span class="string">System.out</span></span><br><span class="line"><span class="meta">log4j.appender.console.layout</span> = <span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.console.layout.ConversionPattern</span> =  <span class="string">%p %d&#123;yyyy-MM-dd HH:mm:ss&#125; %c %m%n</span></span><br></pre></td></tr></table></figure>



<h3 id="（3）跨平台提交"><a href="#（3）跨平台提交" class="headerlink" title="（3）跨平台提交"></a>（3）跨平台提交</h3><ul>
<li>需要拷贝相关配置文件到resource目录<ul>
<li>core-site.xml</li>
<li>hdfs-site.xml</li>
<li>yarn-site.xml</li>
<li>mapred-site.xml</li>
</ul>
</li>
</ul>
<p>代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"root"</span>);</span><br><span class="line"></span><br><span class="line">     conf.addResource(<span class="string">"conf2/core-site.xml"</span>);</span><br><span class="line">     conf.addResource(<span class="string">"conf2/hdfs-site.xml"</span>);</span><br><span class="line">     conf.addResource(<span class="string">"conf2/mapred-site.xml"</span>);</span><br><span class="line">     conf.addResource(<span class="string">"conf2/yarn-site.xml"</span>);</span><br><span class="line">     conf.set(MRJobConfig.JAR, <span class="string">"G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar"</span>);</span><br></pre></td></tr></table></figure>

<p>配置跨平台提交</p>
<ul>
<li>配置mapred-site.xml</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.app-submission.cross-platform<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>代码的方式</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(<span class="string">"mapreduce.app-submission.cross-platform"</span>, <span class="string">"true"</span>);</span><br></pre></td></tr></table></figure>





<h2 id="4-6-自定义Bean对象"><a href="#4-6-自定义Bean对象" class="headerlink" title="4.6  自定义Bean对象"></a>4.6  自定义Bean对象</h2><h3 id="（1）什么是自定义Bean对象"><a href="#（1）什么是自定义Bean对象" class="headerlink" title="（1）什么是自定义Bean对象"></a>（1）什么是自定义Bean对象</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">开发不是一成不变的，Hadoop中提供了集中数据类型的序列化，但是在实际的开发中往往是不够用的，需要自定义序列化对象</span><br><span class="line">在Java中使用的序列化技术是内置的Serializable</span><br><span class="line">但是Hadoop并没有采取这种序列化方式，使用了自己实现的一套序列化机制，叫做Writable</span><br><span class="line"></span><br><span class="line">需要进行序列化后才能在网络中进行传输</span><br><span class="line">编码（序列化）----解码（反序列化）</span><br></pre></td></tr></table></figure>

<h3 id="（2）需求"><a href="#（2）需求" class="headerlink" title="（2）需求"></a>（2）需求</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">18611781163 700000 10000</span><br><span class="line">18611781161 123 123123  </span><br><span class="line">18611781163 700000 10000</span><br><span class="line">18236529965 123 1223123</span><br><span class="line">18236529964 123123 123</span><br><span class="line">18236529965 546 45645</span><br><span class="line">18611781163 300000 70000</span><br><span class="line">18236529965 123 234523</span><br><span class="line">18236529965 31243 436543</span><br></pre></td></tr></table></figure>

<p>这是一组运营商的流量信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">电话         上行    下行   总流量</span><br><span class="line">18611781163 700000 10000   ？</span><br></pre></td></tr></table></figure>

<h3 id="（3）定义Bean对象"><a href="#（3）定义Bean对象" class="headerlink" title="（3）定义Bean对象"></a>（3）定义Bean对象</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test02;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> sun.rmi.runtime.Log;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String phone;</span><br><span class="line">    <span class="keyword">private</span> Long upFlow;</span><br><span class="line">    <span class="keyword">private</span> Long downFlow;</span><br><span class="line">    <span class="keyword">private</span> Long sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(String phone, Long upFlow, Long downFlow, Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPhone</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPhone</span><span class="params">(String phone)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(Long upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(Long downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span> +</span><br><span class="line">                <span class="string">"phone='"</span> + phone + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">" upFlow="</span> + upFlow +</span><br><span class="line">                <span class="string">" downFlow="</span> + downFlow +</span><br><span class="line">                <span class="string">" sumFlow="</span> + sumFlow ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 序列化 编码</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.phone);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.upFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.downFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 反序列化  解码</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.phone = dataInput.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test02;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] data = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *  phone</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        context.write(<span class="keyword">new</span> Text(data[<span class="number">0</span>]), <span class="keyword">new</span> FlowBean(data[<span class="number">0</span>], Long.valueOf(data[<span class="number">1</span>]), Long.valueOf(data[<span class="number">2</span>]), (Long.valueOf(data[<span class="number">1</span>]) + Long.valueOf(data[<span class="number">2</span>]))));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test02;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 18611781163  FlowBean[]</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">NullWritable</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Long up = <span class="number">0L</span>;</span><br><span class="line">        Long down = <span class="number">0L</span>;</span><br><span class="line">        Long sum = <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FlowBean flowBean : values) &#123;</span><br><span class="line"></span><br><span class="line">            up += flowBean.getUpFlow();</span><br><span class="line">            down += flowBean.getDownFlow();</span><br><span class="line">            sum += flowBean.getSumFlow();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.write(NullWritable.get(), <span class="keyword">new</span> FlowBean(key.toString(), up, down, sum));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test02;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.MRJobConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"root"</span>);</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.addResource(<span class="string">"conf2/core-site.xml"</span>);</span><br><span class="line">        conf.addResource(<span class="string">"conf2/hdfs-site.xml"</span>);</span><br><span class="line">        conf.addResource(<span class="string">"conf2/mapred-site.xml"</span>);</span><br><span class="line">        conf.addResource(<span class="string">"conf2/yarn-site.xml"</span>);</span><br><span class="line">        conf.set(MRJobConfig.JAR, <span class="string">"G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar"</span>);</span><br><span class="line">        conf.set(<span class="string">"mapreduce.app-submission.cross-platform"</span>, <span class="string">"true"</span>);</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setJarByClass(FlowRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/flow.dat"</span>));</span><br><span class="line"></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"/baizhi/out333"</span>));</span><br><span class="line">        job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="4-7-MapReduce-计算流程（重点）"><a href="#4-7-MapReduce-计算流程（重点）" class="headerlink" title="4.7 MapReduce 计算流程（重点）"></a>4.7 MapReduce 计算流程（重点）</h2><p><img src="/2020/02/22/Hadoop/1768269-20190829210253564-211954667.png" alt="8img"></p>
<p>1  首先是通过程序员所编写的MR程序通过命令行本地提交或者IDE远程提交</p>
<p>2    一个MR程序就是一个Job，Job信息会给Resourcemanger，向Resourcemanger注册信息</p>
<p>3  在注册通过后，Job会拷贝相关的资源信息（从HDFS中）</p>
<p>4 紧接着会向Resourcemanger提交完整的Job信息（包括资源信息）</p>
<p>5a  Resourcemanger 会通过提交的Job信息，计算出Job所需的资源，为Job分配Container资源</p>
<p>5b 计算资源会分发给对应的NodeManger，NodeManager会创建一个MRAppMaster</p>
<p>6  MRAppMaster初始化Job</p>
<p>7 <strong>获取输入切片信息</strong></p>
<p>8 MRAppMaster向ResourceManager 请求资源</p>
<p>9a 启动计算资源（连接到对应的资源所在NodeManager）</p>
<p>9b 启动YARN Child</p>
<p>10 从文件系统中获取完整的Job信息</p>
<p>11 启动对应的Maptask或者ReduceTask 进程，执行计算。</p>
<p><img src="/2020/02/22/Hadoop/1568950626404.png" alt="1568950626404"></p>
<h2 id="4-8-Job-提交流程（重点）"><a href="#4-8-Job-提交流程（重点）" class="headerlink" title="4.8 Job 提交流程（重点）"></a>4.8 Job 提交流程（重点）</h2><p><img src="/2020/02/22/Hadoop/1568963134494.png" alt="1568963134494"></p>
<h3 id="（1）建立连接"><a href="#（1）建立连接" class="headerlink" title="（1）建立连接"></a>（1）建立连接</h3><p>判断是在本地运行还是集群运行，分别会创建不同的运行对象  YARN | Local</p>
<h3 id="（2）提交Job"><a href="#（2）提交Job" class="headerlink" title="（2）提交Job"></a>（2）提交Job</h3><h4 id="1）校验空间-checkSpecs"><a href="#1）校验空间-checkSpecs" class="headerlink" title="1）校验空间 checkSpecs()"></a>1）校验空间 checkSpecs()</h4><p><img src="/2020/02/22/Hadoop/1568963471055.png" alt="1568963471055"></p>
<p><img src="/2020/02/22/Hadoop/1568963766792.png" alt="1568963766792"></p>
<h4 id="2）缓存处理"><a href="#2）缓存处理" class="headerlink" title="2）缓存处理"></a>2）缓存处理</h4><p><img src="/2020/02/22/Hadoop/1568964000325.png" alt="1568964000325"></p>
<h4 id="3）创建资源路径-Staging路径"><a href="#3）创建资源路径-Staging路径" class="headerlink" title="3）创建资源路径 Staging路径"></a>3）创建资源路径 Staging路径</h4><p><img src="/2020/02/22/Hadoop/1568964277657.png" alt="1568964277657"></p>
<h4 id="4）获取Job-ID-，在Staging路径下创建Job路径"><a href="#4）获取Job-ID-，在Staging路径下创建Job路径" class="headerlink" title="4）获取Job ID ，在Staging路径下创建Job路径"></a>4）获取Job ID ，在Staging路径下创建Job路径</h4><p><img src="/2020/02/22/Hadoop/1568964431696.png" alt="1568964431696"></p>
<h4 id="5）拷贝相关资源到jobID路径"><a href="#5）拷贝相关资源到jobID路径" class="headerlink" title="5）拷贝相关资源到jobID路径"></a>5）拷贝相关资源到jobID路径</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">files</span><br><span class="line">libjars</span><br><span class="line">archives</span><br><span class="line">jobJar</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/22/Hadoop/1568966050505.png" alt="1568966050505"></p>
<h4 id="6）计算切片-生成切片规划文件"><a href="#6）计算切片-生成切片规划文件" class="headerlink" title="6）计算切片 生成切片规划文件"></a>6）计算切片 生成切片规划文件</h4><blockquote>
<p>切片是一个逻辑上的概念，不会文件进行实际物理拆分，默认切分为128MB（本地为32MB）</p>
</blockquote>
<p><img src="/2020/02/22/Hadoop/1568964863438.png" alt="1568964863438"></p>
<p><img src="/2020/02/22/Hadoop/1568964930859.png" alt="1568964930859"></p>
<h4 id="7）向Staging路径写XML-配置文件"><a href="#7）向Staging路径写XML-配置文件" class="headerlink" title="7）向Staging路径写XML 配置文件"></a>7）向Staging路径写XML 配置文件</h4><p><img src="/2020/02/22/Hadoop/1568965043669.png" alt="1568965043669"></p>
<p><img src="/2020/02/22/Hadoop/1568965159913.png" alt="1568965159913"></p>
<h2 id="4-9-MapReduce-组件解析"><a href="#4-9-MapReduce-组件解析" class="headerlink" title="4 .9 MapReduce 组件解析"></a>4 .9 MapReduce 组件解析</h2><h3 id="（1）概述"><a href="#（1）概述" class="headerlink" title="（1）概述"></a>（1）概述</h3><p>通过WC案例的编写，不难发现，其实我们是按照一定的规则进行程序的输入和输出，将作业放在本地运行或者提交到Hadoop集群中运行。</p>
<p>Hadoop是将数据切分成了若干个输入切片（Input Split），并将每个切片交由一个MapTask的进程处理，MapTask不断从对应的Split中解析出来一个一个的 key、value，并交由map()函数进行处理。处理完成之后根据ReduceTask的个数将结果集分成若干个分片（partition）写到磁盘中。</p>
<p>然后，每个ReduceTask会从MapTask所在的节点上的磁盘读取属于的那个分区（partition），然后使用基于排序方法将key 相同的数据聚合在一起，调用Reduce函数，处理完成后输出到磁盘。</p>
<p>从上面的描述中，可以看出，还有一些组件是没有在（目前的）编程中没有体现到：</p>
<p>（1）指定文本格式。将输入数据切分成若干个切片，且将每个Split（切片）解析成满足map函数要求的keyvalue对。</p>
<p>（2）确定map（）函数产生的keyvalue对象发送给那个Reduce 函数处理</p>
<p>（3）指定输出文件格式，即每个keyvalue已何种形式保存成输出文件。</p>
<p>所以在MR中，这个三个组件分别是InputFormat 、Partitioner、OutputFormat ，他们均需要用户根据自己的需求进行配置，但是对于WC 来说，都是默认的。</p>
<p>但是最终。Hadoop还是提供五个可以编程的组件：分别 Mapper Reducer InputFormat   Partitioner OutputFormat。</p>
<p>按照顺序来：InputFormat     Mapper  Partitioner  Reducer  OutputFormat。</p>
<p>还有不是必备的组件：Canbiner  ，通常是用于优化MR程序的性能，但是不能随意添加。</p>
<h3 id="（2）InputFormat组件"><a href="#（2）InputFormat组件" class="headerlink" title="（2）InputFormat组件"></a>（2）InputFormat组件</h3><p>InputFormat主要用于描述输入数据的格式，它提供了如下的两个功能：</p>
<ul>
<li><p>数据切分：按照某个策略将输入数据切分成若干输入切片，确认MapTask个数和对应的Split</p>
</li>
<li><p>为Mapper提供输入数据：给定某个InputSplit，将其解析成一个一个的key、value</p>
</li>
</ul>
<h4 id="1）什么是切片，如何分割？"><a href="#1）什么是切片，如何分割？" class="headerlink" title="1）什么是切片，如何分割？"></a>1）什么是切片，如何分割？</h4><p><img src="/2020/02/22/Hadoop/1568970205814.png" alt="1568970205814"></p>
<p><code>切片</code>：逻辑上对数据文件进行划分</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">package org.apache.hadoop.mapreduce.lib.input;</span><br><span class="line">-|FileInputFormat</span><br><span class="line">	-|getSplits</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/22/Hadoop/1568971578955.png" alt="1568971578955"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    	<span class="comment">// 秒表 不用关注</span></span><br><span class="line">        Stopwatch sw = (<span class="keyword">new</span> Stopwatch()).start();</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    	<span class="comment">// 获取最小大小</span></span><br><span class="line">        <span class="keyword">long</span> minSize = Math.max(<span class="keyword">this</span>.getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">    	<span class="comment">// 获取最大大小</span></span><br><span class="line">        <span class="keyword">long</span> maxSize = getMaxSplitSize(job);</span><br><span class="line">    </span><br><span class="line">   		 <span class="comment">// 准备存放InputSplit 的集合</span></span><br><span class="line">        List&lt;InputSplit&gt; splits = <span class="keyword">new</span> ArrayList();</span><br><span class="line">    	 <span class="comment">// 准备存放FileStatus 的集合</span></span><br><span class="line">        List&lt;FileStatus&gt; files = <span class="keyword">this</span>.listStatus(job);</span><br><span class="line">        Iterator i$ = files.iterator();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">                <span class="keyword">while</span>(i$.hasNext()) &#123;</span><br><span class="line">                    FileStatus file = (FileStatus)i$.next();</span><br><span class="line">                    <span class="comment">//获得当前文件路径</span></span><br><span class="line">                    Path path = file.getPath();</span><br><span class="line">                    <span class="comment">// 获取到当前的长度</span></span><br><span class="line">                    <span class="keyword">long</span> length = file.getLen();</span><br><span class="line">                    <span class="keyword">if</span> (length != <span class="number">0L</span>) &#123;</span><br><span class="line">                        BlockLocation[] blkLocations;</span><br><span class="line">                        <span class="comment">// 判断是否是本地文件还是hdfs文件</span></span><br><span class="line">                        <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span><br><span class="line">                            blkLocations = ((LocatedFileStatus)file).getBlockLocations();</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            FileSystem fs = path.getFileSystem(job.getConfiguration());</span><br><span class="line">                            blkLocations = fs.getFileBlockLocations(file, <span class="number">0L</span>, length);</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 判断是否可以进行切分</span></span><br><span class="line">                        <span class="comment">// hadoop默认数据都可以进行切割</span></span><br><span class="line">                        <span class="keyword">if</span> (<span class="keyword">this</span>.isSplitable(job, path)) &#123;</span><br><span class="line">                            <span class="comment">// 多的块的大小</span></span><br><span class="line">                            <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">                            <span class="comment">// 计算切片大小</span></span><br><span class="line">                            <span class="keyword">long</span> splitSize = <span class="keyword">this</span>.computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line">							<span class="comment">// 准备描述剩余数据的字段</span></span><br><span class="line">                            <span class="keyword">long</span> bytesRemaining;</span><br><span class="line">                            <span class="keyword">int</span> blkIndex;</span><br><span class="line">                            <span class="keyword">for</span>(bytesRemaining = length; (<span class="keyword">double</span>)bytesRemaining / (<span class="keyword">double</span>)splitSize &gt; <span class="number">1.1</span>D; bytesRemaining -= splitSize) &#123;</span><br><span class="line">                                blkIndex = <span class="keyword">this</span>.getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">                                splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, splitSize, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="keyword">if</span> (bytesRemaining != <span class="number">0L</span>) &#123;</span><br><span class="line">                                blkIndex = <span class="keyword">this</span>.getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">                                splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, bytesRemaining, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            splits.add(<span class="keyword">this</span>.makeSplit(path, <span class="number">0L</span>, length, blkLocations[<span class="number">0</span>].getHosts(), blkLocations[<span class="number">0</span>].getCachedHosts()));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        splits.add(<span class="keyword">this</span>.makeSplit(path, <span class="number">0L</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                job.getConfiguration().setLong(<span class="string">"mapreduce.input.fileinputformat.numinputfiles"</span>, (<span class="keyword">long</span>)files.size());</span><br><span class="line">                sw.stop();</span><br><span class="line">                <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">                    LOG.debug(<span class="string">"Total # of splits generated by getSplits: "</span> + splits.size() + <span class="string">", TimeTaken: "</span> + sw.elapsedMillis());</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> splits;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>





<h4 id="2）如何为Mapper提供数据？"><a href="#2）如何为Mapper提供数据？" class="headerlink" title="2）如何为Mapper提供数据？"></a>2）如何为Mapper提供数据？</h4><p>TextInpuFormat使用的是org.apache.hadoop.mapreduce.lib.input.LineRecordReader . 这个类中，首先是initialize（）方法，该方法主要是获取切片信息初始化位置和结束位置，以及输入流；</p>
<p> Mapper的key、value是通过nextKeyValue（）判断是否还有下一个，在这个方法中可以被设置成了文件的偏移量，value通过LineReader.readLine()方法将每一行的值拿到</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit genericSplit, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        FileSplit split = (FileSplit)genericSplit;</span><br><span class="line">        Configuration job = context.getConfiguration();</span><br><span class="line">        <span class="keyword">this</span>.maxLineLength = job.getInt(<span class="string">"mapreduce.input.linerecordreader.line.maxlength"</span>, <span class="number">2147483647</span>);</span><br><span class="line">        <span class="keyword">this</span>.start = split.getStart();</span><br><span class="line">        <span class="keyword">this</span>.end = <span class="keyword">this</span>.start + split.getLength();</span><br><span class="line">        Path file = split.getPath();</span><br><span class="line">        FileSystem fs = file.getFileSystem(job);</span><br><span class="line">        <span class="keyword">this</span>.fileIn = fs.open(file);</span><br><span class="line">        CompressionCodec codec = (<span class="keyword">new</span> CompressionCodecFactory(job)).getCodec(file);</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> != codec) &#123;</span><br><span class="line">            <span class="keyword">this</span>.isCompressedInput = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">this</span>.decompressor = CodecPool.getDecompressor(codec);</span><br><span class="line">            <span class="keyword">if</span> (codec <span class="keyword">instanceof</span> SplittableCompressionCodec) &#123;</span><br><span class="line">                SplitCompressionInputStream cIn = ((SplittableCompressionCodec)codec).createInputStream(<span class="keyword">this</span>.fileIn, <span class="keyword">this</span>.decompressor, <span class="keyword">this</span>.start, <span class="keyword">this</span>.end, READ_MODE.BYBLOCK);</span><br><span class="line">                <span class="keyword">this</span>.in = <span class="keyword">new</span> CompressedSplitLineReader(cIn, job, <span class="keyword">this</span>.recordDelimiterBytes);</span><br><span class="line">                <span class="keyword">this</span>.start = cIn.getAdjustedStart();</span><br><span class="line">                <span class="keyword">this</span>.end = cIn.getAdjustedEnd();</span><br><span class="line">                <span class="keyword">this</span>.filePosition = cIn;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">this</span>.in = <span class="keyword">new</span> SplitLineReader(codec.createInputStream(<span class="keyword">this</span>.fileIn, <span class="keyword">this</span>.decompressor), job, <span class="keyword">this</span>.recordDelimiterBytes);</span><br><span class="line">                <span class="keyword">this</span>.filePosition = <span class="keyword">this</span>.fileIn;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">this</span>.fileIn.seek(<span class="keyword">this</span>.start);</span><br><span class="line">            <span class="keyword">this</span>.in = <span class="keyword">new</span> SplitLineReader(<span class="keyword">this</span>.fileIn, job, <span class="keyword">this</span>.recordDelimiterBytes);</span><br><span class="line">            <span class="keyword">this</span>.filePosition = <span class="keyword">this</span>.fileIn;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.start != <span class="number">0L</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.start += (<span class="keyword">long</span>)<span class="keyword">this</span>.in.readLine(<span class="keyword">new</span> Text(), <span class="number">0</span>, <span class="keyword">this</span>.maxBytesToConsume(<span class="keyword">this</span>.start));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.pos = <span class="keyword">this</span>.start;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (<span class="keyword">this</span>.key == <span class="keyword">null</span>) &#123;</span><br><span class="line">           <span class="keyword">this</span>.key = <span class="keyword">new</span> LongWritable();</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">this</span>.key.set(<span class="keyword">this</span>.pos);</span><br><span class="line">       <span class="keyword">if</span> (<span class="keyword">this</span>.value == <span class="keyword">null</span>) &#123;</span><br><span class="line">           <span class="keyword">this</span>.value = <span class="keyword">new</span> Text();</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">int</span> newSize = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">while</span>(<span class="keyword">this</span>.getFilePosition() &lt;= <span class="keyword">this</span>.end || <span class="keyword">this</span>.in.needAdditionalRecordAfterSplit()) &#123;</span><br><span class="line">           <span class="keyword">if</span> (<span class="keyword">this</span>.pos == <span class="number">0L</span>) &#123;</span><br><span class="line">               newSize = <span class="keyword">this</span>.skipUtfByteOrderMark();</span><br><span class="line">           &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">               newSize = <span class="keyword">this</span>.in.readLine(<span class="keyword">this</span>.value, <span class="keyword">this</span>.maxLineLength, <span class="keyword">this</span>.maxBytesToConsume(<span class="keyword">this</span>.pos));</span><br><span class="line">               <span class="keyword">this</span>.pos += (<span class="keyword">long</span>)newSize;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           <span class="keyword">if</span> (newSize == <span class="number">0</span> || newSize &lt; <span class="keyword">this</span>.maxLineLength) &#123;</span><br><span class="line">               <span class="keyword">break</span>;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           LOG.info(<span class="string">"Skipped line of size "</span> + newSize + <span class="string">" at pos "</span> + (<span class="keyword">this</span>.pos - (<span class="keyword">long</span>)newSize));</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> (newSize == <span class="number">0</span>) &#123;</span><br><span class="line">           <span class="keyword">this</span>.key = <span class="keyword">null</span>;</span><br><span class="line">           <span class="keyword">this</span>.value = <span class="keyword">null</span>;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">readLine</span><span class="params">(Text str, <span class="keyword">int</span> maxLineLength, <span class="keyword">int</span> maxBytesToConsume)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.recordDelimiterBytes != <span class="keyword">null</span> ? <span class="keyword">this</span>.readCustomLine(str, maxLineLength, maxBytesToConsume) : <span class="keyword">this</span>.readDefaultLine(str, maxLineLength, maxBytesToConsume);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">readDefaultLine</span><span class="params">(Text str, <span class="keyword">int</span> maxLineLength, <span class="keyword">int</span> maxBytesToConsume)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        str.clear();</span><br><span class="line">        <span class="keyword">int</span> txtLength = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> newlineLength = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">boolean</span> prevCharCR = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">long</span> bytesConsumed = <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> startPosn = <span class="keyword">this</span>.bufferPosn;</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.bufferPosn &gt;= <span class="keyword">this</span>.bufferLength) &#123;</span><br><span class="line">                startPosn = <span class="keyword">this</span>.bufferPosn = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">if</span> (prevCharCR) &#123;</span><br><span class="line">                    ++bytesConsumed;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">this</span>.bufferLength = <span class="keyword">this</span>.fillBuffer(<span class="keyword">this</span>.in, <span class="keyword">this</span>.buffer, prevCharCR);</span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>.bufferLength &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span>(<span class="keyword">this</span>.bufferPosn &lt; <span class="keyword">this</span>.bufferLength) &#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>.buffer[<span class="keyword">this</span>.bufferPosn] == <span class="number">10</span>) &#123;</span><br><span class="line">                    newlineLength = prevCharCR ? <span class="number">2</span> : <span class="number">1</span>;</span><br><span class="line">                    ++<span class="keyword">this</span>.bufferPosn;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (prevCharCR) &#123;</span><br><span class="line">                    newlineLength = <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                prevCharCR = <span class="keyword">this</span>.buffer[<span class="keyword">this</span>.bufferPosn] == <span class="number">13</span>;</span><br><span class="line">                ++<span class="keyword">this</span>.bufferPosn;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> readLength = <span class="keyword">this</span>.bufferPosn - startPosn;</span><br><span class="line">            <span class="keyword">if</span> (prevCharCR &amp;&amp; newlineLength == <span class="number">0</span>) &#123;</span><br><span class="line">                --readLength;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            bytesConsumed += (<span class="keyword">long</span>)readLength;</span><br><span class="line">            <span class="keyword">int</span> appendLength = readLength - newlineLength;</span><br><span class="line">            <span class="keyword">if</span> (appendLength &gt; maxLineLength - txtLength) &#123;</span><br><span class="line">                appendLength = maxLineLength - txtLength;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (appendLength &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                str.append(<span class="keyword">this</span>.buffer, startPosn, appendLength);</span><br><span class="line">                txtLength += appendLength;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">while</span>(newlineLength == <span class="number">0</span> &amp;&amp; bytesConsumed &lt; (<span class="keyword">long</span>)maxBytesToConsume);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (bytesConsumed &gt; <span class="number">2147483647L</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Too many bytes before newline: "</span> + bytesConsumed);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> (<span class="keyword">int</span>)bytesConsumed;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>



<h3 id="（3）切片MapTask的关系"><a href="#（3）切片MapTask的关系" class="headerlink" title="（3）切片MapTask的关系"></a>（3）切片MapTask的关系</h3><p>MapTask 的并发数量与切片相关（决定），ReduceTask数量是可以手动设置的，默认为1</p>
<h3 id="（4）常用的InputFormat"><a href="#（4）常用的InputFormat" class="headerlink" title="（4）常用的InputFormat"></a>（4）常用的InputFormat</h3><h4 id="1）分类"><a href="#1）分类" class="headerlink" title="1）分类"></a>1）分类</h4><ul>
<li><p>FileInputFormat</p>
<ul>
<li><p>TextInputFormat</p>
<ul>
<li>key LongWriteable 行的字节偏移量</li>
<li>value Text 文本</li>
</ul>
<blockquote>
<p>切片：以文件为切分单位，有多少个文件就至少有多少个切片</p>
</blockquote>
</li>
<li><p>NLineInputFormat</p>
<ul>
<li>key LongWriteable 行的字节偏移量</li>
<li>value Text 文本</li>
</ul>
<blockquote>
<p>切片：n行为一个切片，默认1行为一个切片，可以设置</p>
<p>conf.set(“mapreduce.input.lineinputformat.linespermap”,”10”)</p>
<p>NLineInputFormat.setNumLinesPerSplit();</p>
</blockquote>
</li>
<li><p>CombineTextInputFormat</p>
</li>
<li><p>key LongWriteable 行的字节偏移量</p>
<ul>
<li>value Text 文本</li>
</ul>
<blockquote>
<p>切片：按照SplitSize切分，一个切片可能对应多个Block块</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CombineTextInputFormat.setMaxInputSplitSize();</span><br></pre></td></tr></table></figure>
<p>CombineTextInputFormat.setMinInputSplitSize();</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
</blockquote>
</li>
<li><p>SequenceFileInputFormat</p>
<ul>
<li>key 文件名</li>
<li>value 文件数据</li>
</ul>
</li>
</ul>
</li>
<li><p>DBInputFormat（数据库）</p>
</li>
<li><p>TableInputFormat（HBase）</p>
</li>
</ul>
<h4 id="2）NLineInputFormat"><a href="#2）NLineInputFormat" class="headerlink" title="2）NLineInputFormat"></a>2）NLineInputFormat</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.MRJobConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">/*System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");*/</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取Job对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// // 设置jar 类加载器 否则MapReduce框架找不到Map和Reuce</span></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      <span class="comment">/*  CombineFileInputFormat.setMaxInputSplitSize();</span></span><br><span class="line"><span class="comment">        CombineFileInputFormat.setMinInputSplitSize();*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置数据输入输出组件</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *设置数据输入输出路径</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        NLineInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\data02"</span>));</span><br><span class="line">        NLineInputFormat.setNumLinesPerSplit(job,<span class="number">3</span>);</span><br><span class="line">        <span class="comment">//TextInputFormat.setInputPaths(job, new Path("/wordcount1.txt"));</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 注意： 此输出路径不能存在</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="comment">//TextOutputFormat.setOutputPath(job, new Path("/baizhi/out8121231233"));</span></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out12"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置MAP 和 REDUCE 处理逻辑</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapperClass(WCMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(WCReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置 map任务和reduce任务的输出泛型</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//  提交</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.submit();</span></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* keyIn  LongWritable (Long) 输入文本字节偏移量</span></span><br><span class="line"><span class="comment">* valueIn Text (String)      输入文本行</span></span><br><span class="line"><span class="comment">*  keyOut Text(String)</span></span><br><span class="line"><span class="comment">*  valueOut IntWritable(Int)</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCMapper</span>  <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        String[] names = value.toString().split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String name : names) &#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(name),<span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *keyIn Text 与mapper的keyOut的数据类型相对应</span></span><br><span class="line"><span class="comment"> *valeuIn IntWritable   与mapper的ValueOut的数据类型相对应</span></span><br><span class="line"><span class="comment"> * KeyOut</span></span><br><span class="line"><span class="comment"> * valueOut</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="3）CombineTextInputFormat"><a href="#3）CombineTextInputFormat" class="headerlink" title="3）CombineTextInputFormat"></a>3）CombineTextInputFormat</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.MRJobConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">/*System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");*/</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取Job对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// // 设置jar 类加载器 否则MapReduce框架找不到Map和Reuce</span></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置数据输入输出组件</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *设置数据输入输出路径</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        CombineTextInputFormat.setMinInputSplitSize(job, <span class="number">1048576</span>);</span><br><span class="line">        CombineTextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\data"</span>));</span><br><span class="line">        <span class="comment">//NLineInputFormat.setInputPaths(job, new Path("G:\\Note\\Day02-Hadoop\\数据文件\\data02"));</span></span><br><span class="line">        <span class="comment">// NLineInputFormat.setNumLinesPerSplit(job,3);</span></span><br><span class="line">        <span class="comment">//TextInputFormat.setInputPaths(job, new Path("/wordcount1.txt"));</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 注意： 此输出路径不能存在</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="comment">//TextOutputFormat.setOutputPath(job, new Path("/baizhi/out8121231233"));</span></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out111122"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置MAP 和 REDUCE 处理逻辑</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapperClass(WCMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(WCReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置 map任务和reduce任务的输出泛型</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//  提交</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.submit();</span></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="4）DBInputFormat"><a href="#4）DBInputFormat" class="headerlink" title="4）DBInputFormat"></a>4）DBInputFormat</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.DBInputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.db.DBConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.db.DBInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        DBConfiguration.configureDB(conf, <span class="string">"com.mysql.jdbc.Driver"</span>, <span class="string">"jdbc:mysql://hadoopnode00:3306/hadoop"</span>, <span class="string">"root"</span>, <span class="string">"1234"</span>);</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(DBInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        DBInputFormat.setInput(job, User.class, "select id,name from user", "select count(1) from user");</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\src\\main\\java\\com\\baizhi\\DBInputFormat\\out1"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setMapperClass(DBMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(DBReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.DBInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DBMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">User</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, User value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        context.write(key, <span class="keyword">new</span> Text(value.toString()));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.DBInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DBReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(LongWritable key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            context.write(NullWritable.get(), value);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.DBInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.lib.db.DBWritable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"><span class="keyword">import</span> java.sql.SQLException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">implements</span> <span class="title">Writable</span>, <span class="title">DBWritable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> id;</span><br><span class="line"></span><br><span class="line">    String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">(<span class="keyword">int</span> id, String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(<span class="keyword">int</span> id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"User&#123;"</span> +</span><br><span class="line">                <span class="string">"id="</span> + id +</span><br><span class="line">                <span class="string">", name='"</span> + name + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        dataOutput.writeInt(<span class="keyword">this</span>.id);</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.name);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.id = dataInput.readInt();</span><br><span class="line">        <span class="keyword">this</span>.name = dataInput.readUTF();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(PreparedStatement preparedStatement)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">        preparedStatement.setInt(<span class="number">1</span>, <span class="keyword">this</span>.id);</span><br><span class="line">        preparedStatement.setString(<span class="number">2</span>, <span class="keyword">this</span>.name);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(ResultSet resultSet)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.id = resultSet.getInt(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">this</span>.name = resultSet.getString(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<ul>
<li><p>本地运行</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>JAR 包部署</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">需要在hadoopNode00中添加MySQL的环境</span><br><span class="line">将mysql的jar包放入&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.6.0&#x2F;share&#x2F;hadoop&#x2F;yarn&#x2F; 中即可</span><br></pre></td></tr></table></figure>



<ul>
<li>远程提交</li>
</ul>
<blockquote>
<p>加上相应的配置属性即可</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;);</span><br><span class="line">     conf.addResource(&quot;conf2&#x2F;core-site.xml&quot;);</span><br><span class="line">     conf.addResource(&quot;conf2&#x2F;hdfs-site.xml&quot;);</span><br><span class="line">     conf.addResource(&quot;conf2&#x2F;mapred-site.xml&quot;);</span><br><span class="line">     conf.addResource(&quot;conf2&#x2F;yarn-site.xml&quot;);</span><br><span class="line">     conf.set(MRJobConfig.JAR, &quot;G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar&quot;);</span><br><span class="line">     conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);</span><br></pre></td></tr></table></figure>



<h4 id="5）-自定义InputFormat"><a href="#5）-自定义InputFormat" class="headerlink" title="5） 自定义InputFormat"></a>5） 自定义InputFormat</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">解决小文件存储问题，将多个小文件存放在一个SequenceFile（SequenceFile文件是Hadoop用来存储二进制文件形式的key-value的文件格式），SequenceFile，存储的形式为文件的路径名称为key，文件的内容为value</span><br></pre></td></tr></table></figure>



<p><img src="/2020/02/22/Hadoop/1569136700216.png" alt="1569136700216"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.OutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.baizhi.mr.test01.WCMapper;</span><br><span class="line"><span class="keyword">import</span> com.baizhi.mr.test01.WCReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">/*System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");*/</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取Job对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// // 设置jar 类加载器 否则MapReduce框架找不到Map和Reuce</span></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置数据输入输出组件</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setInputFormatClass(OwnInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(SequenceFileOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *设置数据输入输出路径</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//CombineTextInputFormat.setMinInputSplitSize(job, 1048576);</span></span><br><span class="line">        OwnInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\data"</span>));</span><br><span class="line">        <span class="comment">//NLineInputFormat.setInputPaths(job, new Path("G:\\Note\\Day02-Hadoop\\数据文件\\data02"));</span></span><br><span class="line">        <span class="comment">// NLineInputFormat.setNumLinesPerSplit(job,3);</span></span><br><span class="line">        <span class="comment">//TextInputFormat.setInputPaths(job, new Path("/wordcount1.txt"));</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 注意： 此输出路径不能存在</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="comment">//TextOutputFormat.setOutputPath(job, new Path("/baizhi/out8121231233"));</span></span><br><span class="line">        SequenceFileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out12313"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置MAP 和 REDUCE 处理逻辑</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapperClass(FileMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FileReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置 map任务和reduce任务的输出泛型</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//  提交</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.submit();</span></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.OutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, BytesWritable value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        context.write(key, value);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.OutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;BytesWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (BytesWritable value : values) &#123;</span><br><span class="line"></span><br><span class="line">            context.write(key, value);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.OutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.ByteWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.JobContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnInputFormat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordReader&lt;Text, BytesWritable&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        OwnRecordReader ownRecordReader = <span class="keyword">new</span> OwnRecordReader();</span><br><span class="line">        ownRecordReader.initialize(inputSplit, taskAttemptContext);</span><br><span class="line">        <span class="keyword">return</span> ownRecordReader;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isSplitable</span><span class="params">(JobContext context, Path filename)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.OutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    FileSplit fileSplit;</span><br><span class="line">    Configuration conf;</span><br><span class="line">    BytesWritable value = <span class="keyword">new</span> BytesWritable();</span><br><span class="line">    Text key = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> isProgress = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        fileSplit = (FileSplit) inputSplit;</span><br><span class="line"></span><br><span class="line">        conf = taskAttemptContext.getConfiguration();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (isProgress) &#123;</span><br><span class="line">            <span class="keyword">byte</span>[] bytes = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>) fileSplit.getLength()];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment">//获取fs 对象</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             * 当前文件的路径</span></span><br><span class="line"><span class="comment">             * */</span></span><br><span class="line">            Path path = fileSplit.getPath();</span><br><span class="line"></span><br><span class="line">            FileSystem fileSystem = path.getFileSystem(conf);</span><br><span class="line"></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             * 获取到文件的数据流</span></span><br><span class="line"><span class="comment">             * */</span></span><br><span class="line">            FSDataInputStream inputStream = fileSystem.open(path);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            IOUtils.readFully(inputStream, bytes, <span class="number">0</span>, bytes.length);</span><br><span class="line"></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             * 封装value</span></span><br><span class="line"><span class="comment">             * */</span></span><br><span class="line">            value.set(bytes, <span class="number">0</span>, bytes.length);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            key.set(path.toString());</span><br><span class="line"></span><br><span class="line">            IOUtils.closeStream(inputStream);</span><br><span class="line"></span><br><span class="line">            isProgress = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.key;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> BytesWritable <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="（5）Partitioner-组件"><a href="#（5）Partitioner-组件" class="headerlink" title="（5）Partitioner 组件"></a>（5）Partitioner 组件</h3><p><img src="/2020/02/22/Hadoop/1569138853383.png" alt="1569138853383"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">将不同地区的数据输出到不同的文件中</span><br><span class="line">18611781163 700000 10000 hn</span><br><span class="line">18611781161 123 123123 bj  </span><br><span class="line">18611781163 700000 10000 hn</span><br><span class="line">18236529965 123 1223123 tj</span><br><span class="line">18236529964 123123 123 hb</span><br><span class="line">18236529965 546 45645 tj</span><br><span class="line">18611781163 300000 70000 hn</span><br><span class="line">18236529965 123 234523 tj</span><br><span class="line">18236529965 31243 436543 tj</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/22/Hadoop/1569140142429.png" alt="1569140142429"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnPartitioner</span>&lt;<span class="title">KEY</span>, <span class="title">VALUE</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">KEY</span>, <span class="title">VALUE</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> HashMap&lt;String, Integer&gt; areaMap = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        areaMap.put(<span class="string">"hn"</span>, <span class="number">0</span>);</span><br><span class="line">        areaMap.put(<span class="string">"henna"</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        areaMap.put(<span class="string">"bj"</span>, <span class="number">1</span>);</span><br><span class="line">        areaMap.put(<span class="string">"tj"</span>, <span class="number">2</span>);</span><br><span class="line">        areaMap.put(<span class="string">"hb"</span>, <span class="number">3</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(KEY key, VALUE value, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> areaMap.get(key.toString()) == <span class="keyword">null</span> ? <span class="number">5</span> : areaMap.get(key.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String phone;</span><br><span class="line">    <span class="keyword">private</span> Long upFlow;</span><br><span class="line">    <span class="keyword">private</span> Long downFlow;</span><br><span class="line">    <span class="keyword">private</span> Long sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(String phone, Long upFlow, Long downFlow, Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPhone</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPhone</span><span class="params">(String phone)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(Long upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(Long downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span> +</span><br><span class="line">                <span class="string">"phone='"</span> + phone + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">" upFlow="</span> + upFlow +</span><br><span class="line">                <span class="string">" downFlow="</span> + downFlow +</span><br><span class="line">                <span class="string">" sumFlow="</span> + sumFlow ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 序列化 编码</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.phone);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.upFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.downFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 反序列化  解码</span></span><br><span class="line"><span class="comment">     *f */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.phone = dataInput.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] data = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *  phone</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        context.write(<span class="keyword">new</span> Text(data[<span class="number">3</span>]), <span class="keyword">new</span> FlowBean(data[<span class="number">0</span>], Long.valueOf(data[<span class="number">1</span>]), Long.valueOf(data[<span class="number">2</span>]), (Long.valueOf(data[<span class="number">1</span>]) + Long.valueOf(data[<span class="number">2</span>]))));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 18611781163  FlowBean[]</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Long up = <span class="number">0L</span>;</span><br><span class="line">        Long down = <span class="number">0L</span>;</span><br><span class="line">        Long sum = <span class="number">0L</span>;</span><br><span class="line">        String phone = <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FlowBean flowBean : values) &#123;</span><br><span class="line"></span><br><span class="line">            up += flowBean.getUpFlow();</span><br><span class="line">            down += flowBean.getDownFlow();</span><br><span class="line">            sum += flowBean.getSumFlow();</span><br><span class="line">            phone = flowBean.getPhone();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.write(key, <span class="keyword">new</span> FlowBean(phone, up, down, sum));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.MRJobConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">       <span class="comment">/* conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setJarByClass(FlowRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setPartitionerClass(OwnPartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\flow02.dat"</span>));</span><br><span class="line"></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out131"</span>));</span><br><span class="line">        job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="（6）OutputFormat"><a href="#（6）OutputFormat" class="headerlink" title="（6）OutputFormat"></a>（6）OutputFormat</h3><p>自定义输出</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.outformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(NullWritable.get(), value);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.outformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(NullWritable key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            context.write(NullWritable.get(), value);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.outformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.baizhi.mr.test01.WCMapper;</span><br><span class="line"><span class="keyword">import</span> com.baizhi.mr.test01.WCReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">/*System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");*/</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取Job对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// // 设置jar 类加载器 否则MapReduce框架找不到Map和Reuce</span></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置数据输入输出组件</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(OwnOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *设置数据输入输出路径</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//CombineTextInputFormat.setMinInputSplitSize(job, 1048576);</span></span><br><span class="line">        <span class="comment">//CombineTextInputFormat.setInputPaths(job, new Path(" v  "));</span></span><br><span class="line">        <span class="comment">//NLineInputFormat.setInputPaths(job, new Path("G:\\Note\\Day02-Hadoop\\数据文件\\data02"));</span></span><br><span class="line">        <span class="comment">// NLineInputFormat.setNumLinesPerSplit(job,3);</span></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\flow.dat"</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 注意： 此输出路径不能存在</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="comment">//TextOutputFormat.setOutputPath(job, new Path("/baizhi/out8121231233"));</span></span><br><span class="line">        OwnOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\outbaizhi"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置MAP 和 REDUCE 处理逻辑</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapperClass(FileMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FileReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置 map任务和reduce任务的输出泛型</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//  提交</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.submit();</span></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.outformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordWriter&lt;NullWritable, Text&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> OwnRecordWriter(taskAttemptContext);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.outformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line">    FSDataOutputStream outputStream;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OwnRecordWriter</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        fileSystem = FileSystem.get(taskAttemptContext.getConfiguration());</span><br><span class="line"></span><br><span class="line">        outputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\testoutputforamt.txt"</span>));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(NullWritable nullWritable, Text text)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        outputStream.write(text.getBytes());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        IOUtils.closeStream(outputStream);</span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="（7）Combiner-组件"><a href="#（7）Combiner-组件" class="headerlink" title="（7）Combiner 组件"></a>（7）Combiner 组件</h3><ul>
<li>Conbiner是MR程序中Mapper和Reducer之外的一种组件</li>
<li>Combiner的组件的父类就是Reducer</li>
<li>Combiner和Reucer的区别在于运行的位置</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Combiner是在每一个MapTask的节点上运行  （局部汇总）</span><br><span class="line">Reducer是接收全局的所有的Mapper结果再进行处理 （全局汇总）</span><br></pre></td></tr></table></figure>

<ul>
<li>Combiner的意义就是对于每一个MapTask的输出进行局部汇总，减少网络传输量</li>
<li>Combiner能够运用的前提是不能影响最终业务结果（累加操作不会影响）而且 Combiner的输出KV 应该能跟Reducer的KV相对应</li>
</ul>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>Combiner并不是适用于所有的场景</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. 并不是所有场景都可以使用Combiner，必须满足结果可以累加</span><br><span class="line">2. 适合求和，但不适合求平均数   Avg（0,20,10,25,15）&#x3D;14 | avg（0,20，10）&#x3D;10  avg（25,15）&#x3D;20   avg（10,20）&#x3D;15，通过上述案例可以发现显然这里不适合使用Combiner</span><br></pre></td></tr></table></figure>

<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><ul>
<li>新建CombinerClass继承Reducer ，job.setCombinerClass();</li>
<li>直接使用ReducerClass 作为CombinerClass</li>
</ul>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 核心代码  注意必须满足累加</span></span><br><span class="line">job.setCombinerClass(WCReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>没有使用Combiner</li>
</ul>
<p><img src="/2020/02/22/Hadoop/1569206721162.png" alt="1569206721162"></p>
<ul>
<li>使用Combiner</li>
</ul>
<p><img src="/2020/02/22/Hadoop/1569206773669.png" alt="1569206773669"></p>
<h2 id="4-10-MR-过程"><a href="#4-10-MR-过程" class="headerlink" title="4.10 MR 过程"></a>4.10 MR 过程</h2><p><img src="/2020/02/22/Hadoop/1569208873544.png" alt="1569208873544"></p>
<p><img src="/2020/02/22/Hadoop/1558528156916.png" alt="1558528156916"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">MR框架是使用InputFormat为map所需的数据进行预处理，并为其提供数据。两个功能：切片，封装keyvalue</span><br><span class="line">因为InputSplit为逻辑切分而非物理拆分，所以说还需要RecoderReader根据InputSplit中的信息里处理InputSplit中的具体信息，加载数据并转换为合适的Map任务的keyvalue，输入给Map任务</span><br><span class="line"></span><br><span class="line">Map是自定义的逻辑，根据InputFormat给定的相应数据结合场景进行相应的处理</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">为了让Reducer可以并处理Map的处理结果，需要对map的输出结果进行一定的分区（Partition）、排序（Sort）、合并（Combine）、归并（Merge）等操作，得到keyvalue形式的中间结果，再交给Reducer处理，这个过程就是Shuffle，从无序的keyvalue到有序有分区的keyvalue，这个过程称之为Shuffle很形象。</span><br><span class="line"></span><br><span class="line">Reducer是自定义的逻辑，根据从不同的MapTask 节点拿过来的给定的相应数据结合场景进行相应的处理</span><br><span class="line"></span><br><span class="line">OutputFormat进行输出，输出至分布式文件系统</span><br></pre></td></tr></table></figure>

<h2 id="4-11-Shuffle"><a href="#4-11-Shuffle" class="headerlink" title="4.11 Shuffle"></a>4.11 Shuffle</h2><p>Shuffle过程是MapReducer的核心，描述这数据从map task输出到reduce task的过程。</p>
<p>Hadoop的集群环境，大部分的map task和reduce task 是执行在不同的节点上的，那么reduce就要取得map的输出结果，一搬就需要在不同的节点上去拉取；那么集群中运行的很多个Job，task的执行会对集群中网络资源消耗严重，虽说这种消耗是正常的，不可便面的，但是可以采取措施减少不必要的网络消耗，另一方面，每个节点内部，向对比于内存，磁盘IO对Job的完成时间影响较大。</p>
<p>所以说：从以上进行分析，shuffle的过程基本要求：</p>
<ul>
<li>完整的从map task 端拉取数据到reduce task端</li>
<li>在拉取数据的过程中，尽可能减少网络消耗 </li>
<li>尽可能的减少磁盘IO 对task执行效率的影响</li>
</ul>
<p>shuffle过程</p>
<h3 id="（1）Map端的shuffle"><a href="#（1）Map端的shuffle" class="headerlink" title="（1）Map端的shuffle"></a>（1）Map端的shuffle</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.MapTask</span><br><span class="line">-|MapOutputBuffer</span><br><span class="line"> -|init()</span><br></pre></td></tr></table></figure>



<p>Map的输出结果首先被缓存到内存，当缓存区（环状缓冲区）达到80% （默认大小为100MB），就会启动溢写操作，当前启动溢写操作时，首先把缓存中的数据进行分区，对每个分区的数据进行排序和合并。之后再写入到磁盘中，每次溢写 都会生成新的磁盘文件，随着Job执行，被溢写出到磁盘的文件会越来越多，在Map任务全部结束之前，这些溢写文件会被归并成一个大的磁盘文件，然后通知相应的Reduce任务来领取属于自己的数据。   </p>
<p><img src="/2020/02/22/Hadoop/1569221806545.png" alt="1569221806545"></p>
<ul>
<li>map输入结果写入缓冲区</li>
<li>缓冲区达到阈值（触发溢写的百分比），溢写到磁盘中</li>
<li>分区内排序合并最后归并成大文件（key，value[]）</li>
</ul>
<h3 id="（2）Reduce-端的Shuffle"><a href="#（2）Reduce-端的Shuffle" class="headerlink" title="（2）Reduce 端的Shuffle"></a>（2）Reduce 端的Shuffle</h3><p>Reduce任务从Map端的不用的Map机器领回属于自己的处理那部分数据，然后对数据进行处理</p>
<p><img src="/2020/02/22/Hadoop/1569222214803.png" alt="1569222214803"></p>
<ul>
<li>领取数据</li>
<li>归并数据</li>
<li>数据给reduce任务</li>
</ul>
<h2 id="4-11-编程案例"><a href="#4-11-编程案例" class="headerlink" title="4.11 编程案例"></a>4.11 编程案例</h2><h3 id="（1）WordCount"><a href="#（1）WordCount" class="headerlink" title="（1）WordCount"></a>（1）WordCount</h3><p>略</p>
<h3 id="（2）PV-UV-的统计"><a href="#（2）PV-UV-的统计" class="headerlink" title="（2）PV UV 的统计"></a>（2）PV UV 的统计</h3><p>pv  网站的总访问数量  算总数</p>
<p>uv 独立活跃用户（日活，月活）  去重（UUID 能代表用户唯一为key）</p>
<h3 id="（3）流量统计之对象输出"><a href="#（3）流量统计之对象输出" class="headerlink" title="（3）流量统计之对象输出"></a>（3）流量统计之对象输出</h3><p>略</p>
<h3 id="（4）流量统计之对象排序输出"><a href="#（4）流量统计之对象排序输出" class="headerlink" title="（4）流量统计之对象排序输出"></a>（4）流量统计之对象排序输出</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String phone;</span><br><span class="line">    <span class="keyword">private</span> Long upFlow;</span><br><span class="line">    <span class="keyword">private</span> Long downFlow;</span><br><span class="line">    <span class="keyword">private</span> Long sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(String phone, Long upFlow, Long downFlow, Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPhone</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPhone</span><span class="params">(String phone)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(Long upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(Long downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span> +</span><br><span class="line">                <span class="string">"phone='"</span> + phone + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">" upFlow="</span> + upFlow +</span><br><span class="line">                <span class="string">" downFlow="</span> + downFlow +</span><br><span class="line">                <span class="string">" sumFlow="</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 序列化 编码</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.phone);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.upFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.downFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 反序列化  解码</span></span><br><span class="line"><span class="comment">     *f */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.phone = dataInput.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.sumFlow ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] data = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        context.write(<span class="keyword">new</span> FlowBean(data[<span class="number">0</span>], Long.valueOf(data[<span class="number">1</span>]), Long.valueOf(data[<span class="number">2</span>]), (Long.valueOf(data[<span class="number">2</span>]) + Long.valueOf(data[<span class="number">1</span>]))), NullWritable.get());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 18611781163  FlowBean[]</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">NullWritable</span>, <span class="title">FlowBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">       context.write(key,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序输出;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">       <span class="comment">/* conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\flow.dat"</span>));</span><br><span class="line"></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out4"</span>));</span><br><span class="line">        job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="（5）流量统计之对象排序分区输出"><a href="#（5）流量统计之对象排序分区输出" class="headerlink" title="（5）流量统计之对象排序分区输出"></a>（5）流量统计之对象排序分区输出</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序分区输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String phone;</span><br><span class="line">    <span class="keyword">private</span> Long upFlow;</span><br><span class="line">    <span class="keyword">private</span> Long downFlow;</span><br><span class="line">    <span class="keyword">private</span> Long sumFlow;</span><br><span class="line">    <span class="keyword">private</span> String area;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(String phone, Long upFlow, Long downFlow, Long sumFlow, String area)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">        <span class="keyword">this</span>.area = area;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPhone</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPhone</span><span class="params">(String phone)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(Long upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(Long downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getArea</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> area;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setArea</span><span class="params">(String area)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.area = area;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"FlowBean&#123;"</span> +</span><br><span class="line">                <span class="string">"phone='"</span> + phone + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", upFlow="</span> + upFlow +</span><br><span class="line">                <span class="string">", downFlow="</span> + downFlow +</span><br><span class="line">                <span class="string">", sumFlow="</span> + sumFlow +</span><br><span class="line">                <span class="string">", area='"</span> + area + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 序列化 编码</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.phone);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.upFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.downFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.sumFlow);</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.area);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 反序列化  解码</span></span><br><span class="line"><span class="comment">     *f */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.phone = dataInput.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.area = dataInput.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.sumFlow ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序分区输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] data = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *  phone</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        context.write(<span class="keyword">new</span> FlowBean(data[<span class="number">0</span>], Long.valueOf(data[<span class="number">1</span>]), Long.valueOf(data[<span class="number">2</span>]), (Long.valueOf(data[<span class="number">1</span>]) + Long.valueOf(data[<span class="number">2</span>])),data[<span class="number">3</span>]),NullWritable.get());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序分区输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 18611781163  FlowBean[]</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">NullWritable</span>, <span class="title">FlowBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        context.write(key, NullWritable.get());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序分区输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnPartitioner</span>&lt;<span class="title">KEY</span>, <span class="title">VALUE</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">KEY</span>, <span class="title">VALUE</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> HashMap&lt;String, Integer&gt; areaMap = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        areaMap.put(<span class="string">"hn"</span>, <span class="number">0</span>);</span><br><span class="line">        areaMap.put(<span class="string">"henna"</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        areaMap.put(<span class="string">"zz"</span>, <span class="number">1</span>);</span><br><span class="line">        areaMap.put(<span class="string">"kf"</span>, <span class="number">2</span>);</span><br><span class="line">        areaMap.put(<span class="string">"bj"</span>, <span class="number">3</span>);</span><br><span class="line">        areaMap.put(<span class="string">"xy"</span>, <span class="number">4</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(KEY key, VALUE value, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        FlowBean flowBean = (FlowBean) key;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> areaMap.get(flowBean.getArea()) == <span class="keyword">null</span> ? <span class="number">5</span> : areaMap.get(flowBean.getArea());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="（6）学生成绩之合并文件（表连接）"><a href="#（6）学生成绩之合并文件（表连接）" class="headerlink" title="（6）学生成绩之合并文件（表连接）"></a>（6）学生成绩之合并文件（表连接）</h3><p>需求：</p>
<p>student_info.txt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gjf 00001</span><br><span class="line">gzy 00002</span><br><span class="line">jzz 00003</span><br><span class="line">zkf 00004</span><br></pre></td></tr></table></figure>

<p>student_info_class.txt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">00001 yuwen</span><br><span class="line">00001 shuxue</span><br><span class="line">00002 yinyue</span><br><span class="line">00002 yuwen</span><br><span class="line">00003 tiyu</span><br><span class="line">00003 shengwu</span><br><span class="line">00004 tiyu</span><br><span class="line">00004 wuli</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">00001 gjf yuwen shuxue</span><br><span class="line">00002 gzy yinyue yuwen</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.合并文件表连接;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">       <span class="comment">/* conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">//job.setPartitionerClass(OwnPartitioner.class);</span></span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\classinfo"</span>));</span><br><span class="line"></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out7"</span>));</span><br><span class="line">        job.setMapperClass(StuMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(StuReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.合并文件表连接;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.yarn.webapp.hamlet.Hamlet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StuMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String STU_INFO = <span class="string">"student_info.txt"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String STU_INFO_CLASS = <span class="string">"student_info_class.txt"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String STU_INFO_FLAG = <span class="string">"a"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String STU_INFO_CLASS_FLAG = <span class="string">"b"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br><span class="line"></span><br><span class="line">        String filname = inputSplit.getPath().getName();</span><br><span class="line"></span><br><span class="line">        String[] data = value.toString().split(<span class="string">" "</span>);</span><br><span class="line">        String userid = <span class="string">""</span>;</span><br><span class="line">        String flag = <span class="string">""</span>;</span><br><span class="line">        String valueName = <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (filname.contains(STU_INFO)) &#123;</span><br><span class="line"></span><br><span class="line">            userid = data[<span class="number">1</span>];</span><br><span class="line">            flag = STU_INFO_FLAG;</span><br><span class="line"></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             * 名字</span></span><br><span class="line"><span class="comment">             * */</span></span><br><span class="line">            valueName = data[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (filname.contains(STU_INFO_CLASS)) &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            userid = data[<span class="number">0</span>];</span><br><span class="line">            flag = STU_INFO_CLASS_FLAG;</span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">            学科</span></span><br><span class="line"><span class="comment">            * */</span></span><br><span class="line">            valueName = data[<span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.write(<span class="keyword">new</span> Text(userid), <span class="keyword">new</span> Text(flag + <span class="string">" "</span> + valueName));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.合并文件表连接;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *userid  | 标示+学科或者名字</span></span><br><span class="line"><span class="comment"> * 0001 |a gjf</span></span><br><span class="line"><span class="comment"> * 0001 |b yuwen</span></span><br><span class="line"><span class="comment"> * 0001 |b shuxue</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StuReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        String classList = <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line">        String name = <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line"></span><br><span class="line">            String[] data = value.toString().split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (data[<span class="number">0</span>].equals(<span class="string">"a"</span>)) &#123;</span><br><span class="line">                name = data[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (data[<span class="number">0</span>].equals(<span class="string">"b"</span>)) &#123;</span><br><span class="line"></span><br><span class="line">                classList += <span class="string">" "</span> + data[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        context.write(<span class="keyword">new</span> Text(key.toString() + <span class="string">" "</span> + name + classList), NullWritable.get());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><img src="/2020/02/22/Hadoop/1569228212103.png" alt="1569228212103"></p>

    </div>

    
    
    
      <div>
  
    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
  
</div>

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2020/02/22/postgreSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="next" title="postgreSQL学习笔记">
      postgreSQL学习笔记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Inhaltsverzeichnis
        </li>
        <li class="sidebar-nav-overview">
          Übersicht
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#一、概述"><span class="nav-number">1.</span> <span class="nav-text">一、概述</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-大数据概念"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 大数据概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-大数据面临的问题"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 大数据面临的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-大数据的特点"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 大数据的特点</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1）数据量大"><span class="nav-number">1.3.1.</span> <span class="nav-text">1）数据量大</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2）数据时效性"><span class="nav-number">1.3.2.</span> <span class="nav-text">2）数据时效性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3）数据多样性"><span class="nav-number">1.3.3.</span> <span class="nav-text">3）数据多样性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#（1）数据存储类型多样性"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">（1）数据存储类型多样性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#（2）数据分析类型多样性"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">（2）数据分析类型多样性</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4）数据价值"><span class="nav-number">1.3.4.</span> <span class="nav-text">4）数据价值</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-应用场景"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 应用场景</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1）个人推荐"><span class="nav-number">1.4.1.</span> <span class="nav-text">1）个人推荐</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2）风控"><span class="nav-number">1.4.2.</span> <span class="nav-text">2）风控</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3）成本预测"><span class="nav-number">1.4.3.</span> <span class="nav-text">3）成本预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4）气候预测"><span class="nav-number">1.4.4.</span> <span class="nav-text">4）气候预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5）人工智能"><span class="nav-number">1.4.5.</span> <span class="nav-text">5）人工智能</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-工作方向"><span class="nav-number">1.5.</span> <span class="nav-text">1.5 工作方向</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-6分布式"><span class="nav-number">1.6.</span> <span class="nav-text">1.6分布式</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#二、Hadoop"><span class="nav-number">2.</span> <span class="nav-text">二、Hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-Hadoop生态系统"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 Hadoop生态系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-大数据分析方案"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 大数据分析方案</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#三、HDFS"><span class="nav-number">3.</span> <span class="nav-text">三、HDFS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-安装（伪集群）"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 安装（伪集群）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1）准备虚拟机"><span class="nav-number">3.1.1.</span> <span class="nav-text">1）准备虚拟机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2）安装JDK-8"><span class="nav-number">3.1.2.</span> <span class="nav-text">2）安装JDK 8</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3）配置Java环境变量"><span class="nav-number">3.1.3.</span> <span class="nav-text">3）配置Java环境变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4）配置主机名与IP的映射关系"><span class="nav-number">3.1.4.</span> <span class="nav-text">4）配置主机名与IP的映射关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5）关闭防火墙"><span class="nav-number">3.1.5.</span> <span class="nav-text">5）关闭防火墙</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6）ssh免密登陆"><span class="nav-number">3.1.6.</span> <span class="nav-text">6）ssh免密登陆</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7）解压Hadoop"><span class="nav-number">3.1.7.</span> <span class="nav-text">7）解压Hadoop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8）配置Hadoop环境变量"><span class="nav-number">3.1.8.</span> <span class="nav-text">8）配置Hadoop环境变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9）配置-etc-hadoop-core-site-xml"><span class="nav-number">3.1.9.</span> <span class="nav-text">9）配置 etc&#x2F;hadoop&#x2F;core-site.xml</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10）配置-etc-hadoop-hdfs-site-xml"><span class="nav-number">3.1.10.</span> <span class="nav-text">10）配置 etc&#x2F;hadoop&#x2F;hdfs-site.xml</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11）格式化namenode"><span class="nav-number">3.1.11.</span> <span class="nav-text">11）格式化namenode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12）启动hdfs"><span class="nav-number">3.1.12.</span> <span class="nav-text">12）启动hdfs</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-HDFS-Shell-相关操作"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 HDFS Shell 相关操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1）hdfs-shell"><span class="nav-number">3.2.1.</span> <span class="nav-text">1）hdfs shell</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2）上传文件"><span class="nav-number">3.2.2.</span> <span class="nav-text">2）上传文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-）-ls文件"><span class="nav-number">3.2.3.</span> <span class="nav-text">3 ） ls文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4）下载文件"><span class="nav-number">3.2.4.</span> <span class="nav-text">4）下载文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5）删除文件"><span class="nav-number">3.2.5.</span> <span class="nav-text">5）删除文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6）查看文件"><span class="nav-number">3.2.6.</span> <span class="nav-text">6）查看文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7）创建文件夹"><span class="nav-number">3.2.7.</span> <span class="nav-text">7）创建文件夹</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8）复制文件"><span class="nav-number">3.2.8.</span> <span class="nav-text">8）复制文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9）开启回收站机制"><span class="nav-number">3.2.9.</span> <span class="nav-text">9）开启回收站机制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-Java-API-操作HDFS"><span class="nav-number">3.3.</span> <span class="nav-text">3.3  Java API 操作HDFS</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）-依赖"><span class="nav-number">3.3.1.</span> <span class="nav-text">（1） 依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）Windows-配置Hadoop环境"><span class="nav-number">3.3.2.</span> <span class="nav-text">（2）Windows 配置Hadoop环境</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（3）权限不足解决方案"><span class="nav-number">3.3.3.</span> <span class="nav-text">（3）权限不足解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1）配置-hdfs-site-xml"><span class="nav-number">3.3.3.1.</span> <span class="nav-text">1）配置 hdfs-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2）方案2"><span class="nav-number">3.3.3.2.</span> <span class="nav-text">2）方案2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3）方案3"><span class="nav-number">3.3.3.3.</span> <span class="nav-text">3）方案3</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（3）相关操作"><span class="nav-number">3.3.4.</span> <span class="nav-text">（3）相关操作</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-4-HDFS-Architecture（架构-）"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 HDFS Architecture（架构 ）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1）什么是Block块"><span class="nav-number">3.4.1.</span> <span class="nav-text">1）什么是Block块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）为什么块的大小为128MB？"><span class="nav-number">3.4.2.</span> <span class="nav-text">（1）为什么块的大小为128MB？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）Block块的大小能否随意设置？"><span class="nav-number">3.4.3.</span> <span class="nav-text">（2）Block块的大小能否随意设置？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（3）HDFS为什么不适合存储小文件"><span class="nav-number">3.4.4.</span> <span class="nav-text">（3）HDFS为什么不适合存储小文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2）Rack-Awareness-机架感知"><span class="nav-number">3.4.5.</span> <span class="nav-text">2）Rack Awareness  机架感知</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3）NameNode-和-SecondaryNameNode-的-关系-（重点）"><span class="nav-number">3.4.6.</span> <span class="nav-text">3）NameNode 和 SecondaryNameNode 的 关系 （重点）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4）检查点"><span class="nav-number">3.4.7.</span> <span class="nav-text">4）检查点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5）Safemode"><span class="nav-number">3.4.8.</span> <span class="nav-text">5）Safemode</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6）DataNode工作机制"><span class="nav-number">3.4.9.</span> <span class="nav-text">6）DataNode工作机制</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#四、MapReduce"><span class="nav-number">4.</span> <span class="nav-text">四、MapReduce</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-概述"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-为什么使用MR？"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 为什么使用MR？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-YARN-环境搭建"><span class="nav-number">4.3.</span> <span class="nav-text">4.3 YARN  环境搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）什么是-YARN-？"><span class="nav-number">4.3.1.</span> <span class="nav-text">（1）什么是 YARN ？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）配置YARN"><span class="nav-number">4.3.2.</span> <span class="nav-text">（2）配置YARN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（3）启动YARN"><span class="nav-number">4.3.3.</span> <span class="nav-text">（3）启动YARN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-MR-入门程序"><span class="nav-number">4.4.</span> <span class="nav-text">4.4 MR 入门程序</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）依赖"><span class="nav-number">4.4.1.</span> <span class="nav-text">（1）依赖</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）Mapper-逻辑"><span class="nav-number">4.4.2.</span> <span class="nav-text">（2）Mapper 逻辑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（3）Reduce-逻辑"><span class="nav-number">4.4.3.</span> <span class="nav-text">（3）Reduce 逻辑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（4）Job封装"><span class="nav-number">4.4.4.</span> <span class="nav-text">（4）Job封装</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-5-部署运行"><span class="nav-number">4.5.</span> <span class="nav-text">4.5 部署运行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）远程Jar-包部署"><span class="nav-number">4.5.1.</span> <span class="nav-text">（1）远程Jar 包部署</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）本地仿真"><span class="nav-number">4.5.2.</span> <span class="nav-text">（2）本地仿真</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#填坑"><span class="nav-number">4.5.2.1.</span> <span class="nav-text">填坑</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（3）跨平台提交"><span class="nav-number">4.5.3.</span> <span class="nav-text">（3）跨平台提交</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-6-自定义Bean对象"><span class="nav-number">4.6.</span> <span class="nav-text">4.6  自定义Bean对象</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）什么是自定义Bean对象"><span class="nav-number">4.6.1.</span> <span class="nav-text">（1）什么是自定义Bean对象</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）需求"><span class="nav-number">4.6.2.</span> <span class="nav-text">（2）需求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（3）定义Bean对象"><span class="nav-number">4.6.3.</span> <span class="nav-text">（3）定义Bean对象</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-7-MapReduce-计算流程（重点）"><span class="nav-number">4.7.</span> <span class="nav-text">4.7 MapReduce 计算流程（重点）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-8-Job-提交流程（重点）"><span class="nav-number">4.8.</span> <span class="nav-text">4.8 Job 提交流程（重点）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）建立连接"><span class="nav-number">4.8.1.</span> <span class="nav-text">（1）建立连接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）提交Job"><span class="nav-number">4.8.2.</span> <span class="nav-text">（2）提交Job</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1）校验空间-checkSpecs"><span class="nav-number">4.8.2.1.</span> <span class="nav-text">1）校验空间 checkSpecs()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2）缓存处理"><span class="nav-number">4.8.2.2.</span> <span class="nav-text">2）缓存处理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3）创建资源路径-Staging路径"><span class="nav-number">4.8.2.3.</span> <span class="nav-text">3）创建资源路径 Staging路径</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4）获取Job-ID-，在Staging路径下创建Job路径"><span class="nav-number">4.8.2.4.</span> <span class="nav-text">4）获取Job ID ，在Staging路径下创建Job路径</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5）拷贝相关资源到jobID路径"><span class="nav-number">4.8.2.5.</span> <span class="nav-text">5）拷贝相关资源到jobID路径</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6）计算切片-生成切片规划文件"><span class="nav-number">4.8.2.6.</span> <span class="nav-text">6）计算切片 生成切片规划文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7）向Staging路径写XML-配置文件"><span class="nav-number">4.8.2.7.</span> <span class="nav-text">7）向Staging路径写XML 配置文件</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-9-MapReduce-组件解析"><span class="nav-number">4.9.</span> <span class="nav-text">4 .9 MapReduce 组件解析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）概述"><span class="nav-number">4.9.1.</span> <span class="nav-text">（1）概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）InputFormat组件"><span class="nav-number">4.9.2.</span> <span class="nav-text">（2）InputFormat组件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1）什么是切片，如何分割？"><span class="nav-number">4.9.2.1.</span> <span class="nav-text">1）什么是切片，如何分割？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2）如何为Mapper提供数据？"><span class="nav-number">4.9.2.2.</span> <span class="nav-text">2）如何为Mapper提供数据？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（3）切片MapTask的关系"><span class="nav-number">4.9.3.</span> <span class="nav-text">（3）切片MapTask的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（4）常用的InputFormat"><span class="nav-number">4.9.4.</span> <span class="nav-text">（4）常用的InputFormat</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1）分类"><span class="nav-number">4.9.4.1.</span> <span class="nav-text">1）分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2）NLineInputFormat"><span class="nav-number">4.9.4.2.</span> <span class="nav-text">2）NLineInputFormat</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3）CombineTextInputFormat"><span class="nav-number">4.9.4.3.</span> <span class="nav-text">3）CombineTextInputFormat</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4）DBInputFormat"><span class="nav-number">4.9.4.4.</span> <span class="nav-text">4）DBInputFormat</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5）-自定义InputFormat"><span class="nav-number">4.9.4.5.</span> <span class="nav-text">5） 自定义InputFormat</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（5）Partitioner-组件"><span class="nav-number">4.9.5.</span> <span class="nav-text">（5）Partitioner 组件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（6）OutputFormat"><span class="nav-number">4.9.6.</span> <span class="nav-text">（6）OutputFormat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（7）Combiner-组件"><span class="nav-number">4.9.7.</span> <span class="nav-text">（7）Combiner 组件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#应用场景"><span class="nav-number">4.9.7.1.</span> <span class="nav-text">应用场景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用"><span class="nav-number">4.9.7.2.</span> <span class="nav-text">使用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#案例"><span class="nav-number">4.9.7.3.</span> <span class="nav-text">案例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-10-MR-过程"><span class="nav-number">4.10.</span> <span class="nav-text">4.10 MR 过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-11-Shuffle"><span class="nav-number">4.11.</span> <span class="nav-text">4.11 Shuffle</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）Map端的shuffle"><span class="nav-number">4.11.1.</span> <span class="nav-text">（1）Map端的shuffle</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）Reduce-端的Shuffle"><span class="nav-number">4.11.2.</span> <span class="nav-text">（2）Reduce 端的Shuffle</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-11-编程案例"><span class="nav-number">4.12.</span> <span class="nav-text">4.11 编程案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（1）WordCount"><span class="nav-number">4.12.1.</span> <span class="nav-text">（1）WordCount</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（2）PV-UV-的统计"><span class="nav-number">4.12.2.</span> <span class="nav-text">（2）PV UV 的统计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（3）流量统计之对象输出"><span class="nav-number">4.12.3.</span> <span class="nav-text">（3）流量统计之对象输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（4）流量统计之对象排序输出"><span class="nav-number">4.12.4.</span> <span class="nav-text">（4）流量统计之对象排序输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（5）流量统计之对象排序分区输出"><span class="nav-number">4.12.5.</span> <span class="nav-text">（5）流量统计之对象排序分区输出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#（6）学生成绩之合并文件（表连接）"><span class="nav-number">4.12.6.</span> <span class="nav-text">（6）学生成绩之合并文件（表连接）</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="骚白"
      src="/images/timg2.jpg">
  <p class="site-author-name" itemprop="name">骚白</p>
  <div class="site-description" itemprop="description">生活不是等待暴风雨过去，而是学会在暴风雨中翱翔</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">Artikel</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">骚白</span>
</div>
  <div class="powered-by">Erstellt mit  <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Design – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  



</body>
</html>
