<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"gjq-666.github.io","root":"/","scheme":"Muse","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="welcome gjq-666">
<meta property="og:url" content="https://gjq-666.github.io/index.html">
<meta property="og:site_name" content="welcome gjq-666">
<meta property="article:author" content="骚白">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://gjq-666.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>welcome gjq-666</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Cambiar a barra de navegación">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">welcome gjq-666</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Inicio</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archivo</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://gjq-666.github.io/2020/02/26/Kafka-md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/./source/image/xixi.gif">
      <meta itemprop="name" content="骚白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome gjq-666">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/26/Kafka-md/" class="post-title-link" itemprop="url">Kafka.md</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Publicado el</span>
              

              <time title="Creado por: 2020-02-26 14:28:42 / Modificado por: 17:00:01" itemprop="dateCreated datePublished" datetime="2020-02-26T14:28:42+08:00">2020-02-26</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Apache-Kafka"><a href="#Apache-Kafka" class="headerlink" title="Apache Kafka"></a>Apache Kafka</h1><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><p>Apache Kafka是一个分布式的流数据平台，代表三层含义：</p>
<ul>
<li><strong>Publish/Subscribe</strong>: 消息队列系统 MQ（Message Queue）</li>
<li><strong>Process</strong>: 流数据的实时处理（Stream Process）</li>
<li><strong>Store</strong>: 流数据会以一种安全、容错冗余存储机制存放到分布式集群中</li>
</ul>
<h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="/2020/02/26/Kafka-md/1570779395086.png" alt="1570779395086"></p>
<h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><ul>
<li>构建实时的流数据管道，在系统和应用之间进行可靠的流数据传输</li>
<li>构建实时的流数据处理应用，对流数据进行转换和加工处理</li>
</ul>
<h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><ul>
<li><code>Cluster</code>： kafka支持一到多个服务构成的分布式集群，每一个服务实例成为<code>Broker</code></li>
<li><code>Topic</code>:  某一个分类的消息的集合，如：订单的topic、商品的topic等</li>
<li><code>Partition</code>: 一个Topic有若干个分区（Partition）构成，分区的数量在创建Topic时手动指定</li>
<li><code>Replication</code>:  分区副本，是Partition的冗余备份分区，当Partition不可用时，ZooKeeper会自动将Replication（Follower）分区升级为Partition（Leader）分区</li>
<li><code>Offset</code>:  分区中的Record的位置标示，每一个消费者都会记录自己的消费位置（offset）</li>
</ul>
<h3 id="Topic和Log"><a href="#Topic和Log" class="headerlink" title="Topic和Log"></a>Topic和Log</h3><blockquote>
<p>Each partition is an ordered, immutable sequence of records that is continually appended to—a structured commit log</p>
<p>Kafka的每一个分区（Partition），都是一个有序、不可变的持续追加的记录序列，Kafka会以一种结构化的提交日志保存分区中的数据。</p>
</blockquote>
<p><img src="/2020/02/26/Kafka-md/log_anatomy.png" alt="log_anatomy"></p>
<blockquote>
<p>注意：在分区中写入数据时，会在队列的末尾进行追加，每一个消费者都维护的有一个自己的消费位置（offset）</p>
</blockquote>
<p><img src="/2020/02/26/Kafka-md/log_consumer-1570781067075.png" alt="img"></p>
<h2 id="二、环境搭建"><a href="#二、环境搭建" class="headerlink" title="二、环境搭建"></a>二、环境搭建</h2><blockquote>
<p>完全分布式的Kafka集群</p>
</blockquote>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ul>
<li><p>分布式集群中时钟同步</p>
</li>
<li><p>JDK1.8以上</p>
</li>
<li><p>ZooKeeper集群服务运行正常</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode0* zookeeper-3.4.6]# vi conf/zoo.cfg</span><br><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/home/zk/data</span><br><span class="line">clientPort=2181</span><br><span class="line">server.1=hadoopnode01:2887:3887</span><br><span class="line">server.2=hadoopnode02:2888:3888</span><br><span class="line">server.3=hadoopnode03:2889:3889</span><br><span class="line"></span><br><span class="line">[root@HadoopNode0* zookeeper-3.4.6]# cd /home/zk</span><br><span class="line">[root@HadoopNode0* zk]# mkdir data</span><br><span class="line">[root@HadoopNode0* zk]# vi data/myid</span><br><span class="line"><span class="meta">#</span><span class="bash"> Node01</span></span><br><span class="line">1</span><br><span class="line"><span class="meta">#</span><span class="bash"> Node02</span></span><br><span class="line">2</span><br><span class="line"><span class="meta">#</span><span class="bash"> Node03</span></span><br><span class="line">3</span><br><span class="line">[root@HadoopNode0* zookeeper-3.4.6]# bin/zkServer.sh start conf/zoo.cfg</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如何判断zookeeper集群服务正常</span></span><br><span class="line">[root@HadoopNode03 zookeeper-3.4.6]# bin/zkServer.sh status conf/zoo.cfg</span><br><span class="line">JMX enabled by default</span><br><span class="line">Using config: conf/zoo.cfg</span><br><span class="line">Mode: leader  (一主两从)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h3><ul>
<li><p>将安装包上传并复制到其它节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 上传到某一个服务器，并拷贝给其它服务器</span></span><br><span class="line"></span><br><span class="line">[root@HadoopNode01 ~]# scp kafka_2.11-2.2.0.tgz root@hadoopnode02:~</span><br><span class="line">kafka_2.11-2.2.0.tgz                                                                         100%   61MB  61.0MB/s   00:00</span><br><span class="line">[root@HadoopNode01 ~]# scp kafka_2.11-2.2.0.tgz root@hadoopnode03:~</span><br><span class="line">kafka_2.11-2.2.0.tgz</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装Kafka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode0* ~]# tar -zxf kafka_2.11-2.2.0.tgz -C /usr</span><br><span class="line">[root@HadoopNode01 kafka_2.11-2.2.0]# ll</span><br><span class="line">total 52</span><br><span class="line">drwxr-xr-x. 3 root root  4096 Mar 10  2019 bin  # 指令</span><br><span class="line">drwxr-xr-x. 2 root root  4096 Mar 10  2019 config  # 配置文件</span><br><span class="line">drwxr-xr-x. 2 root root  4096 Oct  9 08:56 libs  # 依赖jar包</span><br><span class="line">-rw-r--r--. 1 root root 32216 Mar 10  2019 LICENSE </span><br><span class="line">-rw-r--r--. 1 root root   336 Mar 10  2019 NOTICE </span><br><span class="line">drwxr-xr-x. 2 root root  4096 Mar 10  2019 site-docs  # 使用文档</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h3><ul>
<li><p>修改kafka核心配置文件<code>server.properties</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode01 kafka_2.11-2.2.0]# vi config/server.properties</span><br><span class="line">broker.id=0 | 1 | 2</span><br><span class="line">listeners=PLAINTEXT://HadoopNode0[1 | 2 | 3]:9092</span><br><span class="line">log.dirs=/usr/kafka_2.11-2.2.0/data</span><br><span class="line">zookeeper.connect=HadoopNode01:2181,HadoopNode02:2181,HadoopNode03:2181</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><ul>
<li><p>启动</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">[root@HadoopNode0*</span> <span class="string">kafka_2.11-2.2.0]# bin/kafka-server-start.sh -daemon config/server.properties</span></span><br><span class="line"><span class="meta">[root@HadoopNode0*</span> <span class="string">kafka_2.11-2.2.0]# jps</span></span><br><span class="line"><span class="attr">10386</span> <span class="string">Kafka</span></span><br><span class="line"><span class="attr">10517</span> <span class="string">Jps</span></span><br><span class="line"><span class="attr">3276</span> <span class="string">QuorumPeerMain</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>关闭</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode0* kafka_2.11-2.2.0]# bin/kafka-server-stop.sh config/server.properties</span><br></pre></td></tr></table></figure>



</li>
</ul>
<h2 id="三、基础使用"><a href="#三、基础使用" class="headerlink" title="三、基础使用"></a>三、基础使用</h2><h3 id="命令行操作"><a href="#命令行操作" class="headerlink" title="命令行操作"></a>命令行操作</h3><h4 id="Topic使用"><a href="#Topic使用" class="headerlink" title="Topic使用"></a>Topic使用</h4><ul>
<li><p>新建Topic</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">[root@HadoopNode01</span> <span class="string">kafka_2.11-2.2.0]# bin/kafka-topics.sh --bootstrap-server Zk01:9092,Zk02:9092,Zk03:9092 --topic t1 --partitions 3 --replication-factor 3 --create</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>展示Topic列表</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode01 kafka_2.11-2.2.0]# bin/kafka-topics.sh --bootstrap-server Zk01:9092,Zk02:9092,Zk03:9092  --list</span><br></pre></td></tr></table></figure>
</li>
<li><p>删除Topic</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">[root@HadoopNode02</span> <span class="string">kafka_2.11-2.2.0]#  bin/kafka-topics.sh --bootstrap-server Zk01:9092,Zk02:9092,Zk03:9092  --delete --topic baizhi</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>描述Topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode02 kafka_2.11-2.2.0]# bin/kafka-topics.sh --bootstrap-server HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092  --describe --topic t1</span><br><span class="line">Topic:t1        PartitionCount:3        ReplicationFactor:3     Configs:segment.bytes=1073741824</span><br><span class="line">        Topic: t1       Partition: 0    Leader: 0       Replicas: 0,2,1 Isr: 0,2,1</span><br><span class="line">        Topic: t1       Partition: 1    Leader: 2       Replicas: 2,1,0 Isr: 2,1,0</span><br><span class="line">        Topic: t1       Partition: 2    Leader: 1       Replicas: 1,0,2 Isr: 1,0,2</span><br><span class="line">        </span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试：<span class="built_in">kill</span>掉Node03上的kafka服务实例  也就是broker【2】</span></span><br><span class="line">[root@HadoopNode02 kafka_2.11-2.2.0]# bin/kafka-topics.sh --bootstrap-server HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092  --describe --topic t1</span><br><span class="line">Topic:t1        PartitionCount:3        ReplicationFactor:3     Configs:segment.bytes=1073741824</span><br><span class="line">        Topic: t1       Partition: 0    Leader: 0       Replicas: 0,2,1 Isr: 0,1</span><br><span class="line">        Topic: t1       Partition: 1    Leader: 1       Replicas: 2,1,0 Isr: 1,0</span><br><span class="line">        Topic: t1       Partition: 2    Leader: 1       Replicas: 1,0,2 Isr: 1,0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 测试：恢复运行Node03上的Kafka服务实例Broker[2]，第三列信息不改变的原因：（分区的Leader都存在，不会触发ZK的故障转移），第五列信息不变</span></span><br><span class="line">[root@HadoopNode02 kafka_2.11-2.2.0]# bin/kafka-topics.sh --bootstrap-server HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092  --describe --topic t1</span><br><span class="line">Topic:t1        PartitionCount:3        ReplicationFactor:3     Configs:segment.bytes=1073741824</span><br><span class="line">        Topic: t1       Partition: 0    Leader: 0       Replicas: 0,2,1 Isr: 0,1,2</span><br><span class="line">        Topic: t1       Partition: 1    Leader: 1       Replicas: 2,1,0 Isr: 1,0,2</span><br><span class="line">        Topic: t1       Partition: 2    Leader: 1       Replicas: 1,0,2 Isr: 1,0,2</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/26/Kafka-md/1570787863465.png" alt="1570787863465"></p>
</li>
</ul>
<ul>
<li><p>修改Topic</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">[root@HadoopNode02</span> <span class="string">kafka_2.11-2.2.0]# bin/kafka-topics.sh --bootstrap-server HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092  --alter --topic t1 --partitions 5</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="发布和订阅"><a href="#发布和订阅" class="headerlink" title="发布和订阅"></a>发布和订阅</h4><ul>
<li><p>发布消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode01 kafka_2.11-2.2.0]# bin/kafka-console-producer.sh --broker-list Zk01:9092,Zk02:9092,Zk03:9092 --topic t1</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">Hello World</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">Hello Kafka</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">Hello Hadoop</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>订阅消息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode02 kafka_2.11-2.2.0]# bin/kafka-console-consumer.sh --topic t1 --bootstrap-server Zk01:9092,Zk02:9092,Zk03:9092</span><br></pre></td></tr></table></figure>



</li>
</ul>
<h3 id="JAVA-Driver"><a href="#JAVA-Driver" class="headerlink" title="JAVA Driver"></a>JAVA Driver</h3><h4 id="Maven依赖"><a href="#Maven依赖" class="headerlink" title="Maven依赖"></a>Maven依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="准备工作-1"><a href="#准备工作-1" class="headerlink" title="准备工作"></a>准备工作</h4><ul>
<li><p>配置Windows Hosts主机名和IP映射</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">192.168.11.20 HadoopNode00</span><br><span class="line">192.168.11.21 HadoopNode01</span><br><span class="line">192.168.11.22 HadoopNode02</span><br><span class="line">192.168.11.23 HadoopNode03</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.UUID;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kafka 生产者的测试类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 准备Kafka生产者配置信息</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092"</span>);</span><br><span class="line">        <span class="comment">// string 序列化（Object ---&gt; byte[]）器</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 生产记录并将其发布</span></span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"t2"</span>, UUID.randomUUID().toString(),<span class="string">"Hello Kafka"</span>);</span><br><span class="line"></span><br><span class="line">        producer.send(record);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 释放资源</span></span><br><span class="line">        producer.flush();</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>1） Kafka的消息生产者，负责生产数据（Record K\V\Timestamp），最终发布（Publish）保存到Kafka集群</p>
<p>2）数据的保存策略：</p>
<ul>
<li>如果Record的Key不为<code>Null</code>，采用哈希算法：<code>key.hashCode % numPartitions = 余数（分区序号）</code></li>
<li>如果Record的Key为<code>Null</code>, 采用轮询策略</li>
<li>手动指定存放的分区</li>
</ul>
<p>3） 数据会以一种分布式的方式保存在Kafka集群中，每一个分区都会维护一个队列的数据结构，新产生的数据会追加到队列的末尾，并且分配<code>offset</code>, </p>
<p>4）数据在Kafka集群中默认最多保留7天（168Hours），不论是否消费，在保留周期到达后都会自动被删除。</p>
<p>5）数据在Kafka中可以进行重复消费，重置消费offset即可</p>
</blockquote>
<h4 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kafka消费者测试类</span></span><br><span class="line"><span class="comment"> * 1. 订阅 subscribe</span></span><br><span class="line"><span class="comment"> * 2. 拉取 pull</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 指定kafka消费者的配置信息</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092"</span>);</span><br><span class="line">        <span class="comment">// 反序列化器 byte[] ---&gt; Object</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 消费组必须得指定</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"group1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 创建kafka消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 订阅主体topic</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">"t2"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 拉取新产生的记录</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class="number">10</span>));</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                System.out.println(record.key() + <span class="string">"\t"</span> + record.value() + <span class="string">"\t"</span></span><br><span class="line">                        + record.topic() + <span class="string">"\t"</span> + record.offset()</span><br><span class="line">                        + <span class="string">"\t"</span> + record.timestamp() + <span class="string">"\t"</span> + record.partition());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/1570852913358.png" alt="1570852913358"></p>
<blockquote>
<p>1）消费者并不是独立存在，kafka中消费者会以消费组的方式进行组织和管理</p>
<p>2）<strong>消费组符合特征： 组外广播、组内负载均衡</strong></p>
<ul>
<li>组外广播： 保证不同的消费组，能够独立消费新产生的数据</li>
<li>组内负载均衡： 消息只会被消费组中的一个消费着进行处理，多个消费组提高了Kafka并行处理能力</li>
</ul>
<p>3）消费者可以订阅一个到多个感兴趣的Topic，一旦这些Topic有新的数据产生，消费者会自动拉取新产生的数据，进行相应的业务处理</p>
<p>4）消费者在消费消息时，会维护一个消费的位置（offset），下一次消费时会自动从offset向后进行消费。</p>
<p>​      在kafka中数据会有一个默认的保留周期（7天），在保留期内数据是可以进行重复消费的，只需要重置消费者消费的offset即可。</p>
<p>5）<code>__consumer_offsets</code>是一个特殊topic，主要记录了Kafka消费组的消费位置。</p>
</blockquote>
<h2 id="四、高级部分"><a href="#四、高级部分" class="headerlink" title="四、高级部分"></a>四、高级部分</h2><h3 id="偏移量控制"><a href="#偏移量控制" class="headerlink" title="偏移量控制"></a>偏移量控制</h3><p>Kafka消费者在订阅Topic时，会自动拉取Topic中新产生的数据。首次消费时使用默认的偏移量消费策略==lastest==</p>
<p>偏移量消费策略：</p>
<ul>
<li><p>==lastest（默认）==：如果有已提交的offset，从已提交的offset之后消费消息。如果无提交的offset，从最后的offset之后消费数据</p>
</li>
<li><p>==earliest==：如果有已提交的offset，从已提交的offset之后消费消息。如果无提交的offset，从最早的offset消费消息</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注意：此配置项 修改偏移量消费策略的默认行为 </span></span><br><span class="line">properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class="string">"earliest"</span>);</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>Kafka消费者消费位置offset，默认采用自动提交的方式，将消费位置提交保存到特殊Topic<code>__consumer_offsets</code>中</p>
<p>自动提交策略：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 默认自动提交消费的位置offset</span></span><br><span class="line">properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// 默认每隔5秒提交一次消费位置</span></span><br><span class="line">properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,<span class="number">5000</span>);</span><br></pre></td></tr></table></figure>

<p>通常情况需要手动提交消费位置：</p>
<blockquote>
<p>为什么需要手动提交消费位置（offset）的原因？</p>
<p>原因：如果自动提交消费位置，有可能在进行业务处理时出现错误，会造成数据没有被正确处理。</p>
<p>​            手动提交消费位置，可以保证数据一定能够被完整的正确处理。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 关闭消费位置offset的自动提交功能</span></span><br><span class="line">properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,<span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 手动提交消费位置</span></span><br><span class="line">consumer.commitSync();</span><br></pre></td></tr></table></figure>

<h3 id="消费方式"><a href="#消费方式" class="headerlink" title="消费方式"></a>消费方式</h3><h4 id="订阅（Subscribe）"><a href="#订阅（Subscribe）" class="headerlink" title="订阅（Subscribe）"></a>订阅（Subscribe）</h4><blockquote>
<p>消费者订阅1到N个感兴趣的Topic，一旦Topic中有新的数据产生，会自动拉取Topic分区内的所有数据</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 订阅（消费）Topic所有的分区</span></span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">"t3"</span>));</span><br></pre></td></tr></table></figure>

<h4 id="指定消费分区"><a href="#指定消费分区" class="headerlink" title="指定消费分区"></a>指定消费分区</h4><blockquote>
<p>消费者在消费数据时，可以只消费某个Topic特定分区内的数据</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定消费Topic的特定分区</span></span><br><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> TopicPartition(<span class="string">"t3"</span>,<span class="number">0</span>)));</span><br></pre></td></tr></table></figure>

<h4 id="重置消费位置"><a href="#重置消费位置" class="headerlink" title="重置消费位置"></a>重置消费位置</h4><blockquote>
<p>消费者在消费数据时，可以重置消费的offset，消费已消费的数据或者跳过不感兴趣的数据</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> TopicPartition(<span class="string">"t3"</span>,<span class="number">0</span>)));</span><br><span class="line"><span class="comment">// 重置消费位置</span></span><br><span class="line">consumer.seek(<span class="keyword">new</span> TopicPartition(<span class="string">"t3"</span>,<span class="number">0</span>),<span class="number">1</span>);</span><br></pre></td></tr></table></figure>

<h3 id="消费组"><a href="#消费组" class="headerlink" title="消费组"></a>消费组</h3><p>（略）</p>
<h3 id="自定义对象类型的传输"><a href="#自定义对象类型的传输" class="headerlink" title="自定义对象类型的传输"></a>自定义对象类型的传输</h3><h4 id="序列化接口"><a href="#序列化接口" class="headerlink" title="序列化接口"></a>序列化接口</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Serializer</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; var1, <span class="keyword">boolean</span> var2)</span></span>;</span><br><span class="line">	<span class="comment">// 序列化方法</span></span><br><span class="line">    <span class="keyword">byte</span>[] serialize(String var1, T var2);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">default</span> <span class="keyword">byte</span>[] serialize(String topic, Headers headers, T data) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.serialize(topic, data);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>发序列化接口</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Deserializer</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; var1, <span class="keyword">boolean</span> var2)</span></span>;</span><br><span class="line">	</span><br><span class="line">    <span class="comment">// 反序列化方法</span></span><br><span class="line">    <span class="function">T <span class="title">deserialize</span><span class="params">(String var1, <span class="keyword">byte</span>[] var2)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">default</span> T <span class="title">deserialize</span><span class="params">(String topic, Headers headers, <span class="keyword">byte</span>[] data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.deserialize(topic, data);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="自定义对象"><a href="#自定义对象" class="headerlink" title="自定义对象"></a>自定义对象</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Integer id;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="keyword">private</span> Date birthday;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="导入工具包的依赖jar包"><a href="#导入工具包的依赖jar包" class="headerlink" title="导入工具包的依赖jar包"></a>导入工具包的依赖jar包</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>commons-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-lang<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="自定义编解码器类"><a href="#自定义编解码器类" class="headerlink" title="自定义编解码器类"></a>自定义编解码器类</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> transfer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.SerializationUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Deserializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Serializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义对象的编解码器类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ObjectCodec</span> <span class="keyword">implements</span> <span class="title">Serializer</span>, <span class="title">Deserializer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * bytes[] ---&gt; Object</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> s</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> bytes</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">deserialize</span><span class="params">(String s, <span class="keyword">byte</span>[] bytes)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> SerializationUtils.deserialize(bytes);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map map, <span class="keyword">boolean</span> b)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Object ---&gt; bytes[]</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> s</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> o</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">byte</span>[] serialize(String s, Object o) &#123;</span><br><span class="line">        <span class="keyword">return</span> SerializationUtils.serialize((Serializable) o);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><blockquote>
<p>建议新创建Topic进行测试，避免旧的Topic中历史数据对我们产生干扰</p>
</blockquote>
<h5 id="生产者API"><a href="#生产者API" class="headerlink" title="生产者API"></a>生产者API</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> transfer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.UUID;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kafka 生产者的测试类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 准备Kafka生产者配置信息</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092"</span>);</span><br><span class="line">        <span class="comment">// string 序列化（Object ---&gt; byte[]）器</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,ObjectCodec<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, User&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;String, User&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 生产记录并将其发布</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">            <span class="comment">// key不为null  第一种策略</span></span><br><span class="line">            ProducerRecord&lt;String, User&gt; record = <span class="keyword">new</span> ProducerRecord&lt;String, User&gt;(<span class="string">"t4"</span>, UUID.randomUUID().toString(),</span><br><span class="line">                    <span class="keyword">new</span> User(i,<span class="string">"zs:"</span>+i,<span class="keyword">new</span> Date()));</span><br><span class="line">            <span class="comment">// key为null 轮询策略</span></span><br><span class="line">            producer.send(record);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 释放资源</span></span><br><span class="line">        producer.flush();</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="消费者API"><a href="#消费者API" class="headerlink" title="消费者API"></a>消费者API</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> transfer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kafka消费者测试类</span></span><br><span class="line"><span class="comment"> * 1. 订阅 subscribe</span></span><br><span class="line"><span class="comment"> * 2. 拉取 pull</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerDemo</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//1. 指定kafka消费者的配置信息</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092"</span>);</span><br><span class="line">        <span class="comment">// 反序列化器 byte[] ---&gt; Object</span></span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ObjectCodec<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">// 注意：此配置项 修改偏移量消费策略的默认行为</span></span><br><span class="line">        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 关闭消费位置offset的自动提交功能</span></span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        <span class="comment">//properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,5000);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 消费组必须得指定</span></span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"group1"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 创建kafka消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, User&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, User&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 订阅主体topic</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">"t4"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 拉取新产生的记录</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, User&gt; records = consumer.poll(Duration.ofSeconds(<span class="number">10</span>));</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, User&gt; record : records) &#123;</span><br><span class="line">                User user = record.value();</span><br><span class="line">                System.out.println(user);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 手动提交消费位置</span></span><br><span class="line">            consumer.commitSync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="生产者的批量发送"><a href="#生产者的批量发送" class="headerlink" title="生产者的批量发送"></a>生产者的批量发送</h3><blockquote>
<p>kafka生产者产生的多条数据共享同一个连接，发送保存到Kafka集群，这种操作方式称为：==Batch（批处理）==。</p>
<p>批处理相比于传统的发送方式，资源利用率更为高效，是一种比较常用的生产者<strong>优化策略。</strong></p>
</blockquote>
<h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生产者方 添加如下配置项即可</span></span><br><span class="line"><span class="comment"># 两个条件 满足其一即可</span></span><br><span class="line"><span class="meta">batch.size</span> = <span class="string">16384Bytes  16kb// 缓冲区大小</span></span><br><span class="line"><span class="meta">linger.ms</span> = <span class="string">毫秒值    // 缓冲区中数据的驻留时长</span></span><br></pre></td></tr></table></figure>

<h4 id="具体使用方法"><a href="#具体使用方法" class="headerlink" title="具体使用方法"></a>具体使用方法</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">properties.put(ProducerConfig.BATCH_SIZE_CONFIG,<span class="number">16384</span>);</span><br><span class="line">properties.put(ProducerConfig.LINGER_MS_CONFIG,<span class="number">2000</span>);</span><br></pre></td></tr></table></figure>

<h3 id="Kafka和Spring-Boot整合"><a href="#Kafka和Spring-Boot整合" class="headerlink" title="Kafka和Spring Boot整合"></a>Kafka和Spring Boot整合</h3><h4 id="创建Spring-Boot工程并选择Kakfa和SpirngBoot的整合依赖"><a href="#创建Spring-Boot工程并选择Kakfa和SpirngBoot的整合依赖" class="headerlink" title="创建Spring Boot工程并选择Kakfa和SpirngBoot的整合依赖"></a>创建Spring Boot工程并选择Kakfa和SpirngBoot的整合依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">     <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-kafka-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">spring.kafka.bootstrap-servers</span>= <span class="string">HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092</span></span><br><span class="line"><span class="meta">spring.kafka.consumer.group-id</span>=<span class="string">g1</span></span><br><span class="line"><span class="meta">spring.kafka.producer.key-serializer</span>=<span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line"><span class="meta">spring.kafka.producer.value-serializer</span>=<span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line"><span class="meta">spring.kafka.consumer.key-deserializer</span>=<span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line"><span class="meta">spring.kafka.consumer.value-deserializer</span>=<span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br></pre></td></tr></table></figure>

<h4 id="生产者API-1"><a href="#生产者API-1" class="headerlink" title="生产者API"></a>生产者API</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.springframework.beans.factory.annotation.Autowired;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.core.KafkaTemplate;</span><br><span class="line"><span class="keyword">import</span> org.springframework.scheduling.annotation.Scheduled;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Date;</span><br><span class="line"><span class="keyword">import</span> java.util.UUID;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String,String&gt; template;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计划任务，定时发送数据</span></span><br><span class="line">    <span class="comment">// cron 秒 分 时 日 月 周 年(省略)</span></span><br><span class="line">    <span class="meta">@Scheduled</span>(cron = <span class="string">"0/10 * * * * ?"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">send</span><span class="params">()</span></span>&#123;</span><br><span class="line">        template.send(<span class="string">"t5"</span>, UUID.randomUUID().toString(),<span class="string">"Hello Kafka"</span>);</span><br><span class="line">        <span class="comment">//System.out.println(new Date());</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="消费者API-1"><a href="#消费者API-1" class="headerlink" title="消费者API"></a>消费者API</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.kafka;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.springframework.kafka.annotation.KafkaListener;</span><br><span class="line"><span class="keyword">import</span> org.springframework.stereotype.Component;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumerDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener</span>(topics = <span class="string">"t5"</span>)</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">receive</span><span class="params">(ConsumerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line">        System.out.println(record.key() + <span class="string">"\t"</span> + record.value());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="生产者幂等操作"><a href="#生产者幂等操作" class="headerlink" title="生产者幂等操作"></a>生产者幂等操作</h3><blockquote>
<p>幂等： 指的多次操作，影响结果是一致的，这种操作方式就被成为幂等操作</p>
<p>==结论：使用Kafka生产者幂等操作原因，kafka生产者在重试发送生产数据时，多次重试操作只会在Kafka的分区队列的末尾写入一条记录==</p>
</blockquote>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/1571036808522.png" alt="1571036808522"></p>
<h4 id="使用方法-1"><a href="#使用方法-1" class="headerlink" title="使用方法"></a>使用方法</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG,<span class="keyword">true</span>); <span class="comment">// 开启幂等操作支持</span></span><br><span class="line">properties.put(ProducerConfig.ACKS_CONFIG,<span class="string">"all"</span>);  <span class="comment">// ack时机 -1或者all 所有  1 leader  0 立即应答</span></span><br><span class="line">properties.put(ProducerConfig.RETRIES_CONFIG,<span class="number">5</span>);   <span class="comment">// 重复次数</span></span><br><span class="line">properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, <span class="number">3000</span>); <span class="comment">// 请求超时时间</span></span><br></pre></td></tr></table></figure>



<h3 id="Kafka事务"><a href="#Kafka事务" class="headerlink" title="Kafka事务"></a>Kafka事务</h3><blockquote>
<p>数据库事务： 一个连接中多个操作不可分割，是一个整体，要么同时成功，同时失败。</p>
</blockquote>
<p>Kafka的事务类似于数据库事务，每一个事务操作都需要一个唯一的事务ID（<code>Transaction-ID</code>），并且事务默认的隔离级别为<code>READ_UNCOMMITTED</code>和<code>READ_COMMITTED</code></p>
<h4 id="生产者事务"><a href="#生产者事务" class="headerlink" title="生产者事务"></a>生产者事务</h4><blockquote>
<p>生产者事务： Kakfka生产者生产的多条数据是一个整体，不可分割，要么同时写入要么同时放弃</p>
</blockquote>
<h5 id="要求"><a href="#要求" class="headerlink" title="要求"></a>要求</h5><ul>
<li>kafka生产者提供唯一的事务ID</li>
<li>必须开启kafka的幂等性支持</li>
</ul>
<h5 id="事务操作"><a href="#事务操作" class="headerlink" title="事务操作"></a>事务操作</h5><ul>
<li>初始化事务</li>
<li>开启事务</li>
<li>正确操作 提交事务</li>
<li>操作失败  回滚事务</li>
</ul>
<h5 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h5><h6 id="生产者API-2"><a href="#生产者API-2" class="headerlink" title="生产者API"></a>生产者API</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> transaction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.UUID;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kafka 生产者的测试类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 准备Kafka生产者配置信息</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">"HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092"</span>);</span><br><span class="line">        <span class="comment">// string 序列化（Object ---&gt; byte[]）器</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 事务ID， 唯一不可重复</span></span><br><span class="line">        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG,UUID.randomUUID().toString());</span><br><span class="line">        <span class="comment">// 开启幂等操作支持</span></span><br><span class="line">        properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG,<span class="keyword">true</span>);</span><br><span class="line">        properties.put(ProducerConfig.ACKS_CONFIG,<span class="string">"all"</span>);  <span class="comment">// ack时机 -1或者all 所有  1 leader  0 立即应答</span></span><br><span class="line">        properties.put(ProducerConfig.RETRIES_CONFIG,<span class="number">5</span>);   <span class="comment">// 重复次数</span></span><br><span class="line">        properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, <span class="number">3000</span>); <span class="comment">// 请求超时时间</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 创建kafka生产者对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 初始化事务</span></span><br><span class="line">        producer.initTransactions();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 开启事务</span></span><br><span class="line">        producer.beginTransaction();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">//3. 生产记录并将其发布</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">50</span>; i &lt; <span class="number">60</span>; i++) &#123;</span><br><span class="line">                <span class="keyword">if</span>(i == <span class="number">56</span>) &#123;</span><br><span class="line">                    <span class="keyword">int</span> m = <span class="number">1</span>/<span class="number">0</span>; <span class="comment">//人为制造错误</span></span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// key不为null  第一种策略</span></span><br><span class="line">                ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(<span class="string">"t3"</span>, UUID.randomUUID().toString(),<span class="string">"Hello Kafka"</span>+i);</span><br><span class="line">                <span class="comment">// key为null 轮询策略</span></span><br><span class="line">                producer.send(record);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 提交事务</span></span><br><span class="line">            producer.commitTransaction();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            <span class="comment">// 取消事务</span></span><br><span class="line">            producer.abortTransaction();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">//4. 释放资源</span></span><br><span class="line">            producer.flush();</span><br><span class="line">            producer.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="消费者API-2"><a href="#消费者API-2" class="headerlink" title="消费者API"></a>消费者API</h6><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 其余代码 一致</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 修改消费者默认的事务隔离级别</span></span><br><span class="line">properties.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG,<span class="string">"read_committed"</span>);</span><br></pre></td></tr></table></figure>

<h4 id="消费生产并存事务（consume-transform-produce）"><a href="#消费生产并存事务（consume-transform-produce）" class="headerlink" title="消费生产并存事务（consume-transform-produce）"></a>消费生产并存事务（consume-transform-produce）</h4><blockquote>
<p>指消费和生产处于同一个事务环境中，要么消费生产同时成功，要么同时失败</p>
</blockquote>
<h5 id="要求-1"><a href="#要求-1" class="headerlink" title="要求"></a>要求</h5><ul>
<li>kafka生产者提供唯一的事务ID</li>
<li>必须开启kafka的幂等性支持</li>
<li>关闭<code>offset</code>的自动提交功能</li>
<li>不能调用手动提交的方法，如: <code>consumer.commitSync();</code></li>
</ul>
<h5 id="实战-1"><a href="#实战-1" class="headerlink" title="实战"></a>实战</h5><blockquote>
<p>创建消费Topic，以及发布的Topic</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode01 kafka_2.11-2.2.0]# bin/kafka-topics.sh --bootstrap-server HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092 --topic t6 --partitions 3 --replication-factor 3 --create</span><br><span class="line">[root@HadoopNode01 kafka_2.11-2.2.0]# bin/kafka-topics.sh --bootstrap-server HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092 --topic t7 --partitions 3 --replication-factor 3 --create</span><br><span class="line">[root@HadoopNode01 kafka_2.11-2.2.0]#</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> transaction.ctp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费生产并存事务</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumeTransformProduceDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 初始化生产者和消费者的配置对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;String, String&gt;(consumerConfig());</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(producerConfig());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 消费者订阅topic</span></span><br><span class="line">        consumer.subscribe(Arrays.asList(<span class="string">"t6"</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 事务操作</span></span><br><span class="line">        producer.initTransactions();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            producer.beginTransaction();</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class="number">5</span>));</span><br><span class="line">                Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                    <span class="comment">// 需要业务处理的内容</span></span><br><span class="line">                    System.out.println(record.key() + <span class="string">"---&gt;"</span> + record.value());</span><br><span class="line">                    producer.send(<span class="keyword">new</span> ProducerRecord&lt;String,String&gt;(<span class="string">"t7"</span>,<span class="string">"t7:"</span>+record.value()));</span><br><span class="line">                    <span class="comment">// 模拟错误</span></span><br><span class="line">                    <span class="comment">// int m = 1/0;</span></span><br><span class="line">                    <span class="comment">// 将消费位置记录到map集合中</span></span><br><span class="line">                    offsets.put(<span class="keyword">new</span> TopicPartition(<span class="string">"t6"</span>,record.partition()),<span class="keyword">new</span> OffsetAndMetadata(record.offset()+<span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// 维护消费位置  将事务内的消费位置信息 提交到kafka中</span></span><br><span class="line">                producer.sendOffsetsToTransaction(offsets,<span class="string">"g1"</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 正确操作 提交事务</span></span><br><span class="line">                producer.commitTransaction();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">                producer.abortTransaction();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Properties <span class="title">producerConfig</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092"</span>);</span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        properties.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, UUID.randomUUID().toString());</span><br><span class="line">        properties.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, Boolean.TRUE);</span><br><span class="line">        properties.put(ProducerConfig.RETRIES_CONFIG, <span class="number">5</span>);</span><br><span class="line">        properties.put(ProducerConfig.ACKS_CONFIG, <span class="string">"all"</span>);</span><br><span class="line">        properties.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, <span class="number">3000</span>);</span><br><span class="line">        properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line">        properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">2000</span>);</span><br><span class="line">        <span class="keyword">return</span> properties;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Properties <span class="title">consumerConfig</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092"</span>);</span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"g1"</span>);</span><br><span class="line">        properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        properties.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, <span class="string">"read_committed"</span>);</span><br><span class="line">        <span class="keyword">return</span> properties;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>作业：</p>
<ol>
<li>重构周六用户注册系统</li>
<li>课堂练习</li>
<li>上网查找总结MQ产品具体的应用场景 <a href="https://www.cnblogs.com/leeego-123/p/10900256.html" target="_blank" rel="noopener">https://www.cnblogs.com/leeego-123/p/10900256.html</a></li>
</ol>
<h1 id="Kafka-Streaming"><a href="#Kafka-Streaming" class="headerlink" title="Kafka Streaming"></a>Kafka Streaming</h1><h2 id="一、什么是批处理和流处理？"><a href="#一、什么是批处理和流处理？" class="headerlink" title="一、什么是批处理和流处理？"></a>一、什么是批处理和流处理？</h2><p>大数据进行处理时有两种处理方式： 批处理和流处理</p>
<h3 id="批处理-Batch-Processing"><a href="#批处理-Batch-Processing" class="headerlink" title="批处理 Batch Processing"></a>批处理 Batch Processing</h3><p>在批处理中，新到达的数据元素被收集到一个组中。整个组在未来的时间进行处理（作为批处理，因此称为“批处理”）。确切地说，何时处理每个组可以用多种方式来确定 - 例如，它可以基于预定的时间间隔（例如，每五分钟，处理任何新的数据已被收集）或在某些触发的条件下（例如，处理只要它包含五个数据元素或一旦它拥有超过1MB的数据）。</p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/20190830063445252.gif" alt="在这里插入图片描述"></p>
<p>批处理模式中使用的数据集通常符合下列特征</p>
<ul>
<li>有界：批处理数据集代表数据的有限集合</li>
<li>持久：数据通常始终存储在某种类型的持久存储位置中</li>
<li>大量：批处理操作通常是处理极为海量数据集的唯一方法</li>
<li>高延迟：大量数据的处理需要付出大量时间，因此批处理不适合对处理时间要求较高的场合</li>
</ul>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUvZGV2ZWxvcGVyLW5ld3MvdjZiZjhxZ3B3dC5qcGVn.jpg" alt="img"></p>
<p><strong>批处理架构的应用场景：日志分析、计费应用程序、数据仓库等</strong>，相关的开源项目（由Google MapReduce衍生）：Apache Hadoop、Apache Spark、Apache Flink等</p>
<h3 id="流处理-Stream-Processing"><a href="#流处理-Stream-Processing" class="headerlink" title="流处理 Stream Processing"></a>流处理 Stream Processing</h3><p>在流处理中，每一条新数据都会在到达时进行处理。与批处理不同，在下一批处理间隔之前不会等待，数据将作为单独的碎片进行处理，而不是一次处理批量。尽管每个新的数据都是单独处理的，但许多流处理系统也支持“窗口”操作，这些操作允许处理也引用在当前数据到达之前和/或之后在指定时间间隔内到达的数据。</p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/20190830064713109.gif" alt="在这里插入图片描述"></p>
<p>流处理模式中使用的数据集通常符合下列特征</p>
<ul>
<li>无界：流处理的输入数据基本上都是无边界数据，而流处理系统将依据具体的应用场景来关注数据的事件时间还是处理时间</li>
<li>高吞吐：大多数的流处理框架都支持分布式并行处理流数据</li>
<li>低延迟：流处理所需的响应时间应该以毫秒（或微秒）来进行计算</li>
</ul>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/aHR0cHM6Ly9hc2sucWNsb3VkaW1nLmNvbS9odHRwLXNhdmUvZGV2ZWxvcGVyLW5ld3MvcWg4dGZlYjlwNC5qcGVn.jpg" alt="在这里插入图片描述"></p>
<p><strong>流处理的应用场景：实时监控、风险评估、实时商业智能（如智能汽车）、实时分析等</strong>，开源项目：Apache Kafka、Apache Flink、Apache Storm、Apache Samza等。</p>
<h2 id="二、Kafka-Straming概述"><a href="#二、Kafka-Straming概述" class="headerlink" title="二、Kafka Straming概述"></a>二、Kafka Straming概述</h2><p>Kafka Streams是一个用于构建应用程序和微服务的<strong>客户端库</strong>，其中的<strong>输入和输出</strong>数据存储在Kafka集群中。它结合了在客户端编写和部署标准<strong>Java和Scala</strong>应用程序的简单性，以及Kafka<strong>服务器端集群技术的优点</strong>。</p>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ol>
<li>弹性、高可扩展、容错</li>
<li>可以部署在容器、虚拟机、单独、云环境中</li>
<li>同样适用于小型、中型和大型用例</li>
<li>集成Kafka Security</li>
<li>写标准的JAVA和Scala应用</li>
<li>精确一次处理语义（exactly once）</li>
<li>无需单独的处理群集</li>
<li>支持多种开发平台</li>
</ol>
<h3 id="核心概念-1"><a href="#核心概念-1" class="headerlink" title="核心概念"></a>核心概念</h3><ul>
<li>==Topology(拓扑)==： 一个用来进行流数据处理的任务，类似于MapReduce的Job。对于MapReduce Job一定会运行结束，而拓扑Topology任务会持续运行，除非人为手动关闭</li>
<li>==Stream(流)==:  数据流，  源源不断，持续产生的数据集合</li>
<li>==Processor（处理器）==： 代表Topology一个计算单元（逻辑）</li>
<li>==State（状态）==： 流处理计算产生的中间结果，通常用于容错和故障恢复</li>
<li>==Time（时间）==： 数据产生时间 &lt;=数据摄入时间&lt;=数据处理时间</li>
</ul>
<blockquote>
<p>注意：<strong>所谓的流处理就是通过Topology编织程序对Stream中Record元素的处理的逻辑/流程。</strong></p>
</blockquote>
<h3 id="架构篇"><a href="#架构篇" class="headerlink" title="架构篇"></a>架构篇</h3><h3 id="架构-1"><a href="#架构-1" class="headerlink" title="架构"></a>架构</h3><p>Kafka Streams通过构建Kafka生产者和消费者库并利用Kafka的本机功能来提供数据并行性，分布式协调，容错和操作简便性，从而简化了应用程序开发。</p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/1571122121002.png" alt="1571122121002"></p>
<p>Kafka的消息分区用于存储和传递消息， Kafka Streams对数据进行分区以进行处理。 Kafka Streams使用Partition和Task的概念作为基于Kafka Topic分区的并行模型的逻辑单元。在并行化的背景下，Kafka Streams和Kafka之间有着密切的联系：</p>
<ol>
<li>每个stream分区都是完全有序的数据记录序列，并映射到Kafka Topic分区。</li>
<li>Stream中的数据记录映射到该Topic的Kafka消息。</li>
<li>数据记录的key决定了Kafka和Kafka Streams中数据的分区，即数据如何路由到Topic的特定分区。</li>
</ol>
<h4 id="任务的并行度"><a href="#任务的并行度" class="headerlink" title="任务的并行度"></a>任务的并行度</h4><p>Kafka Streams基于应用程序的输入流分区创建固定数量的Task，每个任务(Task)分配来自输入流的分区列表（即Kafka主题）。分区到任务的分配永远不会改变，因此每个任务都是应用程序的固定平行单元。然后，任务可以根据分配的分区实例化自己的处理器拓扑; 它们还为每个分配的分区维护一个缓冲区，并从这些记录缓冲区一次一个地处理消息。因此，流任务可以独立并行地处理，无需人工干预。</p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/1571122141260.png" alt="1571122141260"></p>
<p>用户可以启动多个KafkaStream实例，这样等价启动了多个Stream Tread，每个Thread处理1~n个Task。一个Task对应一个分区，因此Kafka Stream流处理的并行度不会超越Topic的分区数。需要值得注意的是Kafka的每个Task都维护这自身的一些状态，线程之间不存在状态共享和通信。因此Kafka在实现流处理的过程中扩展是非常高效的。</p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/1571122150248.png" alt="1571122150248"></p>
<h4 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h4><p>Kafka Streams构建于Kafka本地集成的容错功能之上。 Kafka分区具有高可用性和复制性;因此当流数据持久保存到Kafka时，即使应用程序失败并需要重新处理它也可用。 Kafka Streams中的任务利用Kafka消费者客户端提供的容错功能来处理故障。如果任务运行的计算机故障了，Kafka Streams会自动在其余一个正在运行的应用程序实例中重新启动该任务。</p>
<p>此外，Kafka Streams还确保local state store也很有力处理故障容错。对于每个state store，Kafka Stream维护一个带有副本changelog的Topic，在该Topic中跟踪任何状态更新。这些changelog Topic也是分区的，该分区和Task是一一对应的。如果Task在运行失败并Kafka Stream会在另一台计算机上重新启动该任务，Kafka Streams会保证在重新启动对新启动的任务的处理之前，通过重播相应的更改日志主题，将其关联的状态存储恢复到故障之前的内容。</p>
<h2 id="三、实战操作"><a href="#三、实战操作" class="headerlink" title="三、实战操作"></a>三、实战操作</h2><h3 id="low-level"><a href="#low-level" class="headerlink" title="low-level"></a>low-level</h3><blockquote>
<p>自定义Processor，编程较为复杂，需要手动编程Topology</p>
</blockquote>
<h4 id="maven依赖"><a href="#maven依赖" class="headerlink" title="maven依赖"></a>maven依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.kafka/kafka-streams --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-streams<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> streams.lowlevel.statefuless;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.LongSerializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Serdes;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.KafkaStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.Topology;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.Processor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.ProcessorSupplier;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 通过流处理 实现实时的WordCount</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountApplication</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 指定Kafka Streaming配置信息</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092"</span>);</span><br><span class="line">        <span class="comment">// 声明key和value的序列化和反序列化器</span></span><br><span class="line">        properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line">        properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line">        <span class="comment">// 流处理应用程序的名称 默认会成为消费组的名称</span></span><br><span class="line">        properties.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"wordcount-application"</span>);</span><br><span class="line">        properties.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 手动编织拓扑任务</span></span><br><span class="line">        Topology topology = <span class="keyword">new</span> Topology();</span><br><span class="line"></span><br><span class="line">        topology.addSource(<span class="string">"s1"</span>, <span class="string">"t8"</span>);</span><br><span class="line">        <span class="comment">// 添加计算计算逻辑</span></span><br><span class="line">        topology.addProcessor(<span class="string">"p1"</span>, <span class="keyword">new</span> ProcessorSupplier() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Processor <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> WordCountProcessor();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, <span class="string">"s1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// s1 ---&gt; p1 ---&gt; k1</span></span><br><span class="line">        <span class="comment">// 注意：此时结果输出类型不匹配默认类型，需要手动指定输出类型</span></span><br><span class="line">        topology.addSink(<span class="string">"k1"</span>, <span class="string">"t9"</span>, <span class="keyword">new</span> StringSerializer(),<span class="keyword">new</span> LongSerializer(),<span class="string">"p1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 初始化KafkaStreaming应用</span></span><br><span class="line">        KafkaStreams kafkaStreams = <span class="keyword">new</span> KafkaStreams(topology, properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 启动流处理应用</span></span><br><span class="line">        kafkaStreams.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="创建输入和输出Topic"><a href="#创建输入和输出Topic" class="headerlink" title="创建输入和输出Topic"></a>创建输入和输出Topic</h4><blockquote>
<p><u>注意输入Topic分区数量必须为1个</u></p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode01 kafka_2.11-2.2.0]# bin/kafka-topics.sh --bootstrap-server HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092 --topic t9 --partitions 1 --replication-factor 1 --create</span><br><span class="line">[root@HadoopNode01 kafka_2.11-2.2.0]# bin/kafka-topics.sh --bootstrap-server HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092 --topic t8 --partitions 1 --replication-factor 1 --create</span><br><span class="line">[root@HadoopNode01 kafka_2.11-2.2.0]#</span><br></pre></td></tr></table></figure>

<h4 id="生产者-1"><a href="#生产者-1" class="headerlink" title="生产者"></a>生产者</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode01 kafka_2.11-2.2.0]# bin/kafka-console-producer.sh --topic t8  --broker-list HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092</span><br><span class="line"><span class="meta">&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="消费者-1"><a href="#消费者-1" class="headerlink" title="消费者"></a>消费者</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092 \</span><br><span class="line">   --topic t9 \</span><br><span class="line">   --from-beginning \</span><br><span class="line">   --formatter kafka.tools.DefaultMessageFormatter \</span><br><span class="line">   --property print.key=true \</span><br><span class="line">   --property print.value=true \</span><br><span class="line">   --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \</span><br><span class="line">   --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</span><br></pre></td></tr></table></figure>

<h4 id="分析原因"><a href="#分析原因" class="headerlink" title="分析原因"></a>分析原因</h4><p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/1571130666409.png" alt="1571130666409"></p>
<p>使用HashMap存放流处理计算的累积结果，存在如下问题：</p>
<ul>
<li>应用重启，会操作Map集合中累积的数据丢失</li>
<li>Map集合的容量是有上限的，并且不可能无限制的在集合中存放数据，这样做容易造成JVM的内存溢出，导致服务奔溃。</li>
</ul>
<h4 id="状态管理"><a href="#状态管理" class="headerlink" title="状态管理"></a>状态管理</h4><h5 id="流处理应用的代码（local-state-store）"><a href="#流处理应用的代码（local-state-store）" class="headerlink" title="流处理应用的代码（local state store）"></a>流处理应用的代码（local state store）</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> streams.lowlevel.stateful;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.LongSerializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Serdes;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.KafkaStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.Topology;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.Processor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.ProcessorSupplier;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.KeyValueStore;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.StoreBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.Stores;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 通过流处理 实现实时的WordCount</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountApplication</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 指定Kafka Streaming配置信息</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092"</span>);</span><br><span class="line">        <span class="comment">// 声明key和value的序列化和反序列化器</span></span><br><span class="line">        properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line">        properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line">        <span class="comment">// 流处理应用程序的名称 默认会成为消费组的名称</span></span><br><span class="line">        properties.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"wordcount-application"</span>);</span><br><span class="line">        properties.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 手动编织拓扑任务</span></span><br><span class="line">        Topology topology = <span class="keyword">new</span> Topology();</span><br><span class="line"></span><br><span class="line">        topology.addSource(<span class="string">"s1"</span>, <span class="string">"t8"</span>);</span><br><span class="line">        <span class="comment">// 添加计算计算逻辑</span></span><br><span class="line">        topology.addProcessor(<span class="string">"p1"</span>, <span class="keyword">new</span> ProcessorSupplier() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> Processor <span class="title">get</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> WordCountProcessor();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, <span class="string">"s1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 状态管理的初始化代码</span></span><br><span class="line">        StoreBuilder&lt;KeyValueStore&lt;String, Long&gt;&gt; countStoreSupplier = Stores.keyValueStoreBuilder(</span><br><span class="line">                Stores.persistentKeyValueStore(<span class="string">"Counts"</span>), <span class="comment">// 状态存储的类型</span></span><br><span class="line">                Serdes.String(),  <span class="comment">// 状态存储的key的序列化和反序列化器</span></span><br><span class="line">                Serdes.Long())    <span class="comment">// value的序列化和反序列化器</span></span><br><span class="line">                .withLoggingDisabled(); <span class="comment">// 关闭remote state store   disable backing up the store to a changelog topic</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将p1处理器计算产生的中间结果 状态存储</span></span><br><span class="line">        topology.addStateStore(countStoreSupplier,<span class="string">"p1"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">// s1 ---&gt; p1 ---&gt; k1</span></span><br><span class="line">        <span class="comment">// 注意：此时结果输出类型不匹配默认类型，需要手动指定输出类型</span></span><br><span class="line">        topology.addSink(<span class="string">"k1"</span>, <span class="string">"t9"</span>, <span class="keyword">new</span> StringSerializer(),<span class="keyword">new</span> LongSerializer(),<span class="string">"p1"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 初始化KafkaStreaming应用</span></span><br><span class="line">        KafkaStreams kafkaStreams = <span class="keyword">new</span> KafkaStreams(topology, properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//4. 启动流处理应用</span></span><br><span class="line">        kafkaStreams.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="自定义处理器代码"><a href="#自定义处理器代码" class="headerlink" title="自定义处理器代码"></a>自定义处理器代码</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> streams.lowlevel.stateful;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.KeyValue;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.Processor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.ProcessorContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.PunctuationType;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.processor.Punctuator;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.KeyValueIterator;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.KeyValueStore;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountProcessor</span> <span class="keyword">implements</span> <span class="title">Processor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> ProcessorContext processorContext;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> KeyValueStore&lt;String, Long&gt; state;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> processorContext 处理器的上下文对象</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">(ProcessorContext processorContext)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.state = (KeyValueStore&lt;String, Long&gt;) processorContext.getStateStore(<span class="string">"Counts"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.processorContext = processorContext;</span><br><span class="line">        <span class="comment">// 周期性将处理器的处理结果 发送给下游的处理器</span></span><br><span class="line">        processorContext.schedule(Duration.ofSeconds(<span class="number">1</span>), PunctuationType.STREAM_TIME, <span class="keyword">new</span> Punctuator() &#123;</span><br><span class="line">            <span class="comment">/**</span></span><br><span class="line"><span class="comment">             * 指定方法</span></span><br><span class="line"><span class="comment">             * <span class="doctag">@param</span> timestamp</span></span><br><span class="line"><span class="comment">             */</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">punctuate</span><span class="params">(<span class="keyword">long</span> timestamp)</span> </span>&#123;</span><br><span class="line">                KeyValueIterator&lt;String, Long&gt; iterator = state.all();</span><br><span class="line">                <span class="keyword">while</span> (iterator.hasNext()) &#123;</span><br><span class="line">                    KeyValue&lt;String, Long&gt; keyValue = iterator.next();</span><br><span class="line">                    processorContext.forward(keyValue.key, keyValue.value);</span><br><span class="line">                &#125;</span><br><span class="line">                iterator.close();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;); <span class="comment">// 第三个参数：周期性执行的内容</span></span><br><span class="line">        processorContext.commit();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 处理方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">process</span><span class="params">(String key, String value)</span> </span>&#123;</span><br><span class="line">        String[] words = value.split(<span class="string">" "</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            Long num = state.get(word);</span><br><span class="line">            System.out.println(word + <span class="string">"\t"</span> + num);</span><br><span class="line">            <span class="keyword">if</span> (num == <span class="keyword">null</span>) &#123;</span><br><span class="line">                state.put(word, <span class="number">1L</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                state.put(word, num + <span class="number">1L</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 关闭</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>问题：</strong></p>
<p><code>Exception in thread &quot;wordcount-application-834c5456-d4fa-4077-bcb9-14ad824e0196-StreamThread-2&quot; java.lang.UnsatisfiedLinkError: C:\Users\Administrator\AppData\Local\Temp\librocksdbjni6558818401015537035.dll: Can&#39;t find dependent libraries</code></p>
<p><strong>解决办法：</strong></p>
<p>​        从<a href="https://link.jianshu.com/?t=https://www.microsoft.com/en-us/download/details.aspx?id=48145" target="_blank" rel="noopener">https://www.microsoft.com/en-us/download/details.aspx?id=48145</a>下载<em>Microsoft Visual C++ 2015 Redistributable 并安装。</em></p>
<p>​         如果还未解决，请安装所有的VC++ 版本</p>
</blockquote>
<h5 id="Remote-State-Store"><a href="#Remote-State-Store" class="headerlink" title="Remote State Store"></a>Remote State Store</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Map&lt;String,String&gt; config = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// kafka topic的数据删除策略： delete(默认) 数据保留周期 7天</span></span><br><span class="line"><span class="comment">//                          compact(紧凑)  新值覆盖旧值</span></span><br><span class="line"><span class="comment">// Hello 2</span></span><br><span class="line"><span class="comment">// Hello 6</span></span><br><span class="line">config.put(<span class="string">"cleanup.policy"</span>,<span class="string">"compact"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 状态管理的初始化代码</span></span><br><span class="line">StoreBuilder&lt;KeyValueStore&lt;String, Long&gt;&gt; countStoreSupplier = Stores.keyValueStoreBuilder(</span><br><span class="line">    Stores.persistentKeyValueStore(<span class="string">"Counts"</span>), <span class="comment">// 状态存储的类型</span></span><br><span class="line">    Serdes.String(),  <span class="comment">// 状态存储的key的序列化和反序列化器</span></span><br><span class="line">    Serdes.Long())    <span class="comment">// value的序列化和反序列化器</span></span><br><span class="line">    .withLoggingEnabled(config); <span class="comment">// 开启remote state store</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>compact数据删除策略表示：数据写入Topic时，如果key存在则新值覆盖原有的历史值，key不存在则追加数据</p>
</blockquote>
<h3 id="high-level"><a href="#high-level" class="headerlink" title="high-level"></a>high-level</h3><blockquote>
<p>通过Kafka Streaming提供的DSL编程风格，编写流处理应用（各种操作算子会自动编织为Topology）</p>
</blockquote>
<p>Kafka Streams DSL（Domain Specific Language）构建于Streams Processor API之上。它是大多数用户推荐的，特别是初学者。大多数数据处理操作只能用几行DSL代码表示。在 Kafka Streams DSL 中有这么几个概念<code>KTable</code>、<code>KStream</code>和<code>GlobalKTable</code></p>
<p>KStream是一个数据流，可以认为所有记录都通过Insert only的方式插入进这个数据流里。而KTable代表一个完整的数据集，可以理解为数据库中的表。由于每条记录都是Key-Value对，这里可以将Key理解为数据库中的Primary Key，而Value可以理解为一行记录。可以认为KTable中的数据都是通过Update only的方式进入的。也就意味着，如果KTable对应的Topic中新进入的数据的Key已经存在，那么从KTable只会取出同一Key对应的最后一条数据，相当于新的数据更新了旧的数据。</p>
<p>以下图为例，假设有一个KStream和KTable，基于同一个Topic创建，并且该Topic中包含如下图所示5条数据。此时遍历KStream将得到与Topic内数据完全一样的所有5条数据，且顺序不变。而此时遍历KTable时，因为这5条记录中有3个不同的Key，所以将得到3条记录，每个Key对应最新的值，并且这三条数据之间的顺序与原来在Topic中的顺序保持一致。这一点与Kafka的日志compact相同。</p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/963903-20180823012822162-142241598.png" alt="img"></p>
<p>此时如果对该KStream和KTable分别基于key做Group，对Value进行Sum，得到的结果将会不同。对KStream的计算结果是&lt;Jack，4&gt;，&lt;Lily，7&gt;，&lt;Mike，4&gt;。而对Ktable的计算结果是&lt;Mike，4&gt;，&lt;Jack，3&gt;，&lt;Lily，5&gt;。</p>
<blockquote>
<p><strong>GlobalKTable</strong>:和KTable类似，不同点在于KTable只能表示一个分区的信息，但是GlobalKTable表示的是全局的状态信息。</p>
</blockquote>
<h4 id="Maven依赖-1"><a href="#Maven依赖-1" class="headerlink" title="Maven依赖"></a>Maven依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-streams<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="应用-1"><a href="#应用-1" class="headerlink" title="应用"></a>应用</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> streams.highlevel.statefulless;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Serdes;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kafka streaming dsl风格（高级API）版的WordCount</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountApplication</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//1. 指定流处理应用的配置信息</span></span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"HadoopNode01:9092,HadoopNode02:9092,HadoopNode03:9092"</span>);</span><br><span class="line">        properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line">        properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line">        properties.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"wordcount-highlevel-application"</span>);</span><br><span class="line">        properties.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2. 编织拓扑任务</span></span><br><span class="line">        StreamsBuilder sb = <span class="keyword">new</span> StreamsBuilder();</span><br><span class="line">        KStream&lt;String, String&gt; stream = sb.stream(<span class="string">"t10"</span>);</span><br><span class="line">        KTable&lt;String, Long&gt; kTable = stream</span><br><span class="line">                <span class="comment">// null hello</span></span><br><span class="line">                <span class="comment">// null world</span></span><br><span class="line">                .flatMap((key, value) -&gt; &#123;</span><br><span class="line">                    String[] words = value.toLowerCase().split(<span class="string">" "</span>);</span><br><span class="line">                    ArrayList&lt;KeyValue&lt;String, String&gt;&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">                    <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                        KeyValue&lt;String, String&gt; keyValue = <span class="keyword">new</span> KeyValue&lt;&gt;(key, word);</span><br><span class="line">                        list.add(keyValue);</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">return</span> list;</span><br><span class="line"></span><br><span class="line">                &#125;)</span><br><span class="line">                <span class="comment">// hello 1L</span></span><br><span class="line">                <span class="comment">// world 1L</span></span><br><span class="line">                .map((k, v) -&gt; <span class="keyword">new</span> KeyValue&lt;String, Long&gt;(v, <span class="number">1L</span>))</span><br><span class="line">                <span class="comment">// hello [1,1...]</span></span><br><span class="line">                <span class="comment">// shuffle</span></span><br><span class="line">                .groupByKey(Grouped.with(Serdes.String(), Serdes.Long()))</span><br><span class="line">                <span class="comment">// hello 2</span></span><br><span class="line">                .count();</span><br><span class="line"></span><br><span class="line">        kTable.toStream().to(<span class="string">"t11"</span>, Produced.with(Serdes.String(), Serdes.Long()));</span><br><span class="line"></span><br><span class="line">        Topology topology = sb.build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 打印自动生产的Topology信息</span></span><br><span class="line">        System.out.println(topology.describe().toString());</span><br><span class="line"></span><br><span class="line">        <span class="comment">//3. 初始化流处理应用</span></span><br><span class="line">        KafkaStreams kafkaStreams = <span class="keyword">new</span> KafkaStreams(topology, properties);</span><br><span class="line">        <span class="comment">//4. 启动流处理应用</span></span><br><span class="line">        kafkaStreams.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="分析拓扑运行过程"><a href="#分析拓扑运行过程" class="headerlink" title="分析拓扑运行过程"></a>分析拓扑运行过程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Topologies:</span><br><span class="line">   Sub-topology: 0</span><br><span class="line">    Source: KSTREAM-SOURCE-0000000000 (topics: [t10])</span><br><span class="line">      --&gt; KSTREAM-FLATMAP-0000000001</span><br><span class="line">    Processor: KSTREAM-FLATMAP-0000000001 (stores: [])</span><br><span class="line">      --&gt; KSTREAM-MAP-0000000002</span><br><span class="line">      &lt;-- KSTREAM-SOURCE-0000000000</span><br><span class="line">    Processor: KSTREAM-MAP-0000000002 (stores: [])</span><br><span class="line">      --&gt; KSTREAM-FILTER-0000000006</span><br><span class="line">      &lt;-- KSTREAM-FLATMAP-0000000001</span><br><span class="line">    Processor: KSTREAM-FILTER-0000000006 (stores: [])</span><br><span class="line">      --&gt; KSTREAM-SINK-0000000005</span><br><span class="line">      &lt;-- KSTREAM-MAP-0000000002</span><br><span class="line">    Sink: KSTREAM-SINK-0000000005 (topic: KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition)</span><br><span class="line">      &lt;-- KSTREAM-FILTER-0000000006</span><br><span class="line"></span><br><span class="line">  Sub-topology: 1</span><br><span class="line">    Source: KSTREAM-SOURCE-0000000007 (topics: [KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition])</span><br><span class="line">      --&gt; KSTREAM-AGGREGATE-0000000004</span><br><span class="line">    Processor: KSTREAM-AGGREGATE-0000000004 (stores: [KSTREAM-AGGREGATE-STATE-STORE-0000000003])</span><br><span class="line">      --&gt; KTABLE-TOSTREAM-0000000008</span><br><span class="line">      &lt;-- KSTREAM-SOURCE-0000000007</span><br><span class="line">    Processor: KTABLE-TOSTREAM-0000000008 (stores: [])</span><br><span class="line">      --&gt; KSTREAM-SINK-0000000009</span><br><span class="line">      &lt;-- KSTREAM-AGGREGATE-0000000004</span><br><span class="line">    Sink: KSTREAM-SINK-0000000009 (topic: t11)</span><br><span class="line">      &lt;-- KTABLE-TOSTREAM-0000000008</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/1571211039841.png" alt="1571211039841"></p>
<blockquote>
<p>剖析：</p>
<ol>
<li>在kafka streaming拓扑关系图中有两个子拓扑Sub-topology: 0和Sub-topology: 1</li>
<li>Sub-topology: 0的<code>KSTREAM-SOURCE-0000000000会</code>将topic 10中的record作为数据源，然后经过处理器（Processor）<code>KSTREAM-FLATMAP-0000000001</code>、<code>KSTREAM-MAP-0000000002</code>、<code>KSTREAM-FILTER-0000000006</code>（<em>过滤掉key为空的中间结果</em>）,最终将处理完成的结果存放到topic <code>KSTREAM-AGGREGATE-STATE-STORE-0000000003-repartition</code>中。</li>
</ol>
<p><strong>为什么这里需要*-repartition的topic呢？主要原因是保证在shuffle结束后key相同的record存放在*-repartition相同的分区中，这样就为下一步的统计做好了准备</strong></p>
<ol start="3">
<li>Sub-topology: 1的<code>KSTREAM-SOURCE-0000000007</code>将<code>*-repartition</code>topic中的record作为数据源，然后经过Processor<code>KSTREAM-AGGREGATE-0000000004</code>进行聚合操作，并且将聚合的状态信息存放大topic<code>KSTREAM-AGGREGATE-STATE-STORE-0000000003</code>中，继续经过Processor<code>KTABLE-TOSTREAM-0000000008</code>，最终将处理完成的结果存放到<code>topic 11</code>中</li>
</ol>
</blockquote>
<h4 id="操作算子"><a href="#操作算子" class="headerlink" title="操作算子"></a>操作算子</h4><h5 id="无状态的操作算子-stateless"><a href="#无状态的操作算子-stateless" class="headerlink" title="无状态的操作算子(stateless)"></a>无状态的操作算子(stateless)</h5><blockquote>
<p>无状态的操作算子, 指进行数据转换操作时不会涉及到状态的管理</p>
</blockquote>
<ul>
<li><p><strong>Branch</strong></p>
<blockquote>
<p>KStream  —-&gt; KStream[]</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">KStream&lt;String, String&gt;[] kStreams = stream.branch(</span><br><span class="line">                (k, v) -&gt; v.startsWith(<span class="string">"A"</span>),   <span class="comment">// stream: A开头</span></span><br><span class="line">                (k, v) -&gt; <span class="keyword">true</span>                 <span class="comment">// 其它数据</span></span><br><span class="line">        );</span><br><span class="line">kStreams[<span class="number">0</span>].foreach((k,v) -&gt; System.out.println(k + <span class="string">"\t"</span>+v));</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Filter</strong></p>
<blockquote>
<p>KStream —&gt; KStream </p>
<p>保留符合Boolean条件（true）的数据</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">     .filter((k,v) -&gt; v.startsWith(<span class="string">"H"</span>))</span><br><span class="line">    .foreach((k,v) -&gt; System.out.println(k+<span class="string">"\t"</span>+v));</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>filterNot</strong></p>
<blockquote>
<p>KStream → KStream</p>
<p>KTable → KTable</p>
<p>保留不符合Boolean条件的数据</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">    .filterNot((k,v) -&gt; v.startsWith(<span class="string">"H"</span>))</span><br><span class="line">    .foreach((k,v) -&gt; System.out.println(k+<span class="string">"\t"</span>+v));</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>FlatMap</strong></p>
<blockquote>
<p>KStream → KStream</p>
<p>将一个Record展开为0-n个Record</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">     .flatMap((k,v) -&gt; Arrays.asList(</span><br><span class="line">          <span class="keyword">new</span> KeyValue&lt;String,String&gt;(k,v.toUpperCase()+<span class="string">"!"</span>),</span><br><span class="line">          <span class="keyword">new</span> KeyValue&lt;String,String&gt;(k,v.toLowerCase()+<span class="string">"?"</span>)))</span><br><span class="line">    .foreach((k,v) -&gt; System.out.println(k +<span class="string">"\t"</span> + v));</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>flatMapValues</strong></p>
<blockquote>
<p>KStream → KStream</p>
<p>将一个Record的value展开为1到N个新的value（key不变）</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">                <span class="comment">// null Hello World</span></span><br><span class="line">                <span class="comment">//--------------------</span></span><br><span class="line">                <span class="comment">// null Hello</span></span><br><span class="line">                <span class="comment">// null World</span></span><br><span class="line">    .flatMapValues((v) -&gt; Arrays.asList(v.split(<span class="string">" "</span>)))</span><br><span class="line">    .foreach((k, v) -&gt; System.out.println(k + <span class="string">"\t"</span> + v));</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Foreach</strong></p>
<blockquote>
<p>KStream → void （终止操作）</p>
<p>对KStream中的数据进行迭代遍历，无返回值</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">                <span class="comment">// null Hello World</span></span><br><span class="line">                <span class="comment">//--------------------</span></span><br><span class="line">                <span class="comment">// null Hello</span></span><br><span class="line">                <span class="comment">// null World</span></span><br><span class="line">    .flatMapValues((v) -&gt; Arrays.asList(v.split(<span class="string">" "</span>)))</span><br><span class="line">    .foreach((k, v) -&gt; System.out.println(k + <span class="string">"\t"</span> + v));</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>GroupBy</strong></p>
<blockquote>
<p>KStream → KGroupedStream</p>
<p>根据指定的信息 进行分区操作，注意分组时会进行Shuffle（洗牌）</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//============================groupBy===================================</span></span><br><span class="line">stream</span><br><span class="line">                <span class="comment">// null Hello World</span></span><br><span class="line">                <span class="comment">//--------------------</span></span><br><span class="line">                <span class="comment">// null Hello</span></span><br><span class="line">                <span class="comment">// null World</span></span><br><span class="line">     .flatMapValues((v) -&gt; Arrays.asList(v.split(<span class="string">" "</span>)))</span><br><span class="line">    .groupBy((k,v) -&gt; v)</span><br><span class="line">    .count()</span><br><span class="line">    .toStream()</span><br><span class="line">    .foreach((k,v) -&gt; System.out.println(k+<span class="string">"\t"</span>+v));</span><br><span class="line"><span class="comment">//======================================================================</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>GroupByKey</strong></p>
<blockquote>
<p>KStream → KGroupedStream</p>
<p>根据已存在的key值进行分区操作（洗牌）</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">                <span class="comment">// null Hello World</span></span><br><span class="line">                <span class="comment">//--------------------</span></span><br><span class="line">                <span class="comment">// null Hello</span></span><br><span class="line">                <span class="comment">// null World</span></span><br><span class="line">    .flatMapValues((v) -&gt; Arrays.asList(v.split(<span class="string">" "</span>)))</span><br><span class="line">    .map((k,v) -&gt; <span class="keyword">new</span> KeyValue&lt;String,Long&gt;(v,<span class="number">1L</span>))</span><br><span class="line">    .groupByKey(Grouped.with(Serdes.String(),Serdes.Long()))</span><br><span class="line">    .count()</span><br><span class="line">    .toStream()</span><br><span class="line">    .foreach((k,v) -&gt; System.out.println(k+<span class="string">"\t"</span>+v));</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Map</strong></p>
<blockquote>
<p>KStream → KStream</p>
<p>将一个流中的一条数据映射为另外一条数据</p>
</blockquote>
</li>
<li><p><strong>mapValues</strong></p>
<blockquote>
<p>类似于map操作，不同key不可变，V可变</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">    <span class="comment">// null Hello World</span></span><br><span class="line">    <span class="comment">//--------------------</span></span><br><span class="line">    <span class="comment">// null Hello</span></span><br><span class="line">    <span class="comment">// null World</span></span><br><span class="line">    .flatMapValues((v) -&gt; Arrays.asList(v.split(<span class="string">" "</span>)))</span><br><span class="line">    .map((k,v) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(v,<span class="number">1L</span>))</span><br><span class="line">    .mapValues(v -&gt; v = v+<span class="number">1</span>)</span><br><span class="line">    .foreach((k,v) -&gt; System.out.println(k+<span class="string">"\t"</span>+v));</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Merge</strong></p>
<blockquote>
<p>KStream → KStream</p>
<p>将两个流合并为一个大流</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">KStream&lt;String, String&gt;[] streams = stream</span><br><span class="line">                .branch(</span><br><span class="line">                        (k, v) -&gt; v.startsWith(<span class="string">"A"</span>),</span><br><span class="line">                        (k, v) -&gt; v.startsWith(<span class="string">"B"</span>),</span><br><span class="line">                        (k, v) -&gt; <span class="keyword">true</span></span><br><span class="line">                );</span><br><span class="line">        streams[<span class="number">0</span>].merge(streams[<span class="number">2</span>])</span><br><span class="line">                .foreach((k,v) -&gt; System.out.println(k+<span class="string">"\t"</span>+v));</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Peek</strong></p>
<blockquote>
<p>KStream → KStream</p>
<p>探针（调试程序）： 不会改变数据流内容</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.peek((k,v) -&gt; System.out.println(k+<span class="string">"\t"</span>+v));</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Print</strong></p>
<blockquote>
<p>等价于<code>foreach((key, value) -&gt; System.out.println(key + &quot;, &quot; + value))</code></p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.print(Printed.toSysOut());</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>SelectKey</strong></p>
<blockquote>
<p>KStream → KStream</p>
<p>给流中的数据，分配新的k值（k变，v不变）</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stream.selectKey((k,v) -&gt; <span class="string">"Hello:"</span>).print(Printed.toSysOut());</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Table to Stream</strong></p>
<blockquote>
<p>KTable → KStream</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">table.toStream();</span><br></pre></td></tr></table></figure>



</li>
</ul>
<h5 id="有状态的操作算子-stateful"><a href="#有状态的操作算子-stateful" class="headerlink" title="有状态的操作算子(stateful)"></a>有状态的操作算子(stateful)</h5><p>有状态转换值得是每一次的处理都需要操作关联StateStore实现有状态更新。例如，在aggregating 操作中，window state store用于收集每个window的最新聚合结果。在join操作中，窗口状态存储用于收集到目前为止在定义的window边界内接收的所有记录。状态存储是容错的。如果发生故障，Kafka Streams保证在恢复处理之前完全恢复所有状态存储。</p>
<p>DSL中可用的有状态转换包括:</p>
<ul>
<li><a href="http://kafka.apache.org/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-aggregating" target="_blank" rel="noopener">Aggregating</a></li>
<li><a href="http://kafka.apache.org/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-joins" target="_blank" rel="noopener">Joining</a></li>
<li><a href="http://kafka.apache.org/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-windowing" target="_blank" rel="noopener">Windowing</a> (as part of aggregations and joins)</li>
<li><a href="http://kafka.apache.org/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-process" target="_blank" rel="noopener">Applying custom processors and transformers</a>, which may be stateful, for Processor API integration</li>
</ul>
<p>下图显示了它们之间的关系：</p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/streams-stateful_operations.png" alt="img"></p>
<ul>
<li><p>aggregate（聚合）</p>
<blockquote>
<p>KGroupedStream –&gt; KTable</p>
<p>滚动聚合： 根据分组的key，聚合values</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">                <span class="comment">// null Hello Hello</span></span><br><span class="line">     .flatMapValues(v -&gt; Arrays.asList(v.split(<span class="string">" "</span>)))</span><br><span class="line">    <span class="comment">// null Hello</span></span><br><span class="line">    <span class="comment">// null Hello</span></span><br><span class="line">    .groupBy((k,v) -&gt; v,Grouped.with(Serdes.String(),Serdes.String()))</span><br><span class="line">    <span class="comment">// Hello [Hello,Hello].length</span></span><br><span class="line">    <span class="comment">// Hello 2+0</span></span><br><span class="line">    <span class="comment">// 参数一： 初始化器  参数二：聚合器(k: word, v: [])</span></span><br><span class="line">    .aggregate(()-&gt;<span class="number">0L</span>,(k,v,aggs) -&gt; aggs + <span class="number">1L</span>,Materialized.with(Serdes.String(),Serdes.Long()))</span><br><span class="line">    .toStream()</span><br><span class="line">    .print(Printed.toSysOut());</span><br></pre></td></tr></table></figure>
</li>
<li><p>count（计数）</p>
<blockquote>
<p>KGroupedStream → KTable</p>
<p>滚动聚合： 根据分组的key，统计value的数量</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">                <span class="comment">// null Hello Hello</span></span><br><span class="line">     .flatMapValues(v -&gt; Arrays.asList(v.split(<span class="string">" "</span>)))</span><br><span class="line">    <span class="comment">// null Hello</span></span><br><span class="line">    <span class="comment">// null Hello</span></span><br><span class="line">    .groupBy((k,v) -&gt; v,Grouped.with(Serdes.String(),Serdes.String()))</span><br><span class="line">    .count()</span><br><span class="line">    .toStream()</span><br><span class="line">    .print(Printed.toSysOut());</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Reduce</strong></p>
<blockquote>
<p>KGroupedStream → KTable</p>
<p>滚动聚合：根据分组的key，合并value值列表</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">stream</span><br><span class="line">                <span class="comment">// null Hello Hello</span></span><br><span class="line">    .flatMapValues(v -&gt; Arrays.asList(v.split(<span class="string">" "</span>)))</span><br><span class="line">    <span class="comment">// null Hello</span></span><br><span class="line">    <span class="comment">// null Hello</span></span><br><span class="line">    .map((k,v) -&gt; <span class="keyword">new</span> KeyValue&lt;String,Long&gt;(v,<span class="number">1L</span>))</span><br><span class="line">    .groupByKey(Grouped.with(Serdes.String(),Serdes.Long()))</span><br><span class="line">    <span class="comment">// Hello [1,1,1]</span></span><br><span class="line">    <span class="comment">// World [1,1,1,1]</span></span><br><span class="line">    <span class="comment">// 参数一： 初始化器  参数二：聚合器(k: word, v: [])</span></span><br><span class="line">    .reduce((v1,v2) -&gt; &#123;</span><br><span class="line">        System.out.println(v1 +<span class="string">"\t"</span>+v2);</span><br><span class="line">        Long result = v1+v2;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;,Materialized.with(Serdes.String(),Serdes.Long()))</span><br><span class="line">    .toStream()</span><br><span class="line">    .print(Printed.toSysOut());</span><br></pre></td></tr></table></figure>



</li>
</ul>
<h4 id="Window"><a href="#Window" class="headerlink" title="Window"></a>Window</h4><ul>
<li><p>Tumbling（翻滚） 固定大小 无重叠</p>
<p>翻滚窗口将流元素按照固定的时间间隔，拆分成指定的窗口，窗口和窗口间元素之间没有重叠。在下图不同颜色的record表示不同的key。可以看是在时间窗口内，每个key对应一个窗口。<code>前闭后开</code></p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/streams-time-windows-tumbling-1571297999267.png" alt></p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/963903-20180823013225516-1830552448-1571297996894.gif" alt></p>
</li>
</ul>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Serdes;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.utils.Bytes;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.KafkaStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.KeyValue;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.KeyValueStore;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.WindowStore;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.text.SimpleDateFormat;</span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaStreamingWordCountWithWindow</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">        properties.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">"wordcount22"</span>);</span><br><span class="line">        properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"gaozhy:9092"</span>);</span><br><span class="line">        properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line">        properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line"></span><br><span class="line">        StreamsBuilder builder = <span class="keyword">new</span> StreamsBuilder();</span><br><span class="line">        <span class="comment">// 流处理 数据的来源</span></span><br><span class="line">        KStream&lt;String, String&gt; kStream = builder.stream(<span class="string">"input"</span>);</span><br><span class="line"></span><br><span class="line">        kStream</span><br><span class="line">                .flatMap((k, v) -&gt; &#123;</span><br><span class="line">                    ArrayList&lt;KeyValue&lt;String, String&gt;&gt; keyValues = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">                    String[] words = v.split(<span class="string">" "</span>);</span><br><span class="line">                    <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">                        keyValues.add(<span class="keyword">new</span> KeyValue&lt;String, String&gt;(k, word));</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">return</span> keyValues;</span><br><span class="line">                &#125;)</span><br><span class="line">                .map((k, v) -&gt; <span class="keyword">new</span> KeyValue&lt;String, Long&gt;(v, <span class="number">1L</span>))</span><br><span class="line">                .groupBy((k, v) -&gt; k, Grouped.with(Serdes.String(), Serdes.Long()))</span><br><span class="line">          			<span class="comment">// 滚动窗口的大小</span></span><br><span class="line">                .windowedBy(TimeWindows.of(Duration.ofSeconds(<span class="number">10</span>)))</span><br><span class="line">                .reduce((value1, value2) -&gt; value1 + value2, Materialized.&lt;String, Long, WindowStore&lt;Bytes, <span class="keyword">byte</span>[]&gt;&gt;as(<span class="string">"counts"</span>).withKeySerde(Serdes.String()).withValueSerde(Serdes.Long()))</span><br><span class="line">                .toStream()</span><br><span class="line">                .peek(((Windowed&lt;String&gt; key, Long value) -&gt; &#123;</span><br><span class="line">                    Window window = key.window();</span><br><span class="line">                    SimpleDateFormat sdf = <span class="keyword">new</span> SimpleDateFormat(<span class="string">"HH:mm:ss"</span>);</span><br><span class="line">                    <span class="keyword">long</span> start = window.start();</span><br><span class="line">                    <span class="keyword">long</span> end = window.end();</span><br><span class="line">                    System.out.println(sdf.format(start) + <span class="string">" ~ "</span> + sdf.format(end) + <span class="string">"\t"</span> + key.key() + <span class="string">"\t"</span> + value);</span><br><span class="line">                &#125;));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 构建kafka streaming 应用</span></span><br><span class="line">        KafkaStreams kafkaStreams = <span class="keyword">new</span> KafkaStreams(builder.build(), properties);</span><br><span class="line">        kafkaStreams.start();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<ul>
<li><p>Hopping （跳跃） 固定大小 有重叠</p>
<p>Hopping time windows是基于时间间隔的窗口。他们模拟固定大小的（可能）重叠窗口。跳跃窗口由两个属性定义：窗口大小和其提前间隔（又名“hop”）。</p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/streams-time-windows-hopping-1571297998866.png" alt></p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/963903-20180823013127357-1860834711-1571297996613.gif" alt></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.TimeWindows;</span><br><span class="line"></span><br><span class="line"><span class="comment">// A hopping time window with a size of 5 minutes and an advance interval of 1 minute.</span></span><br><span class="line"><span class="comment">// The window's name -- the string parameter -- is used to e.g. name the backing state store.</span></span><br><span class="line">Duration windowSizeMs = Duration.ofMinutes(<span class="number">5</span>);</span><br><span class="line">Duration advanceMs =    Duration.ofMinutes(<span class="number">1</span>);</span><br><span class="line">TimeWindows.of(windowSizeMs).advanceBy(advanceMs);</span><br></pre></td></tr></table></figure>



</li>
</ul>
<ul>
<li><p>Sliding (滑动)  固定大小 有重合  每一个窗口至少有一个事件</p>
<p>窗口只用于2个KStream进行Join计算时。该窗口的大小定义了Join两侧KStream的数据记录被认为在同一个窗口的最大时间差。假设该窗口的大小为5秒，则参与Join的2个KStream中，记录时间差小于5的记录被认为在同一个窗口中，可以进行Join计算。</p>
</li>
</ul>
<ul>
<li><p>Session   动态 无重叠 数据驱动的窗口</p>
<p>Session Window该窗口用于对Key做Group后的聚合操作中。它需要对Key做分组，然后对组内的数据根据业务需求定义一个窗口的起始点和结束点。一个典型的案例是，希望通过Session Window计算某个用户访问网站的时间。对于一个特定的用户（用Key表示）而言，当发生登录操作时，该用户（Key）的窗口即开始，当发生退出操作或者超时时，该用户（Key）的窗口即结束。窗口结束时，可计算该用户的访问时间或者点击次数等。</p>
<p>Session Windows用于将基于key的事件聚合到所谓的会话中，其过程称为session化。会话表示由定义的不活动间隔（或“空闲”）分隔的活动时段。处理的任何事件都处于任何现有会话的不活动间隙内，并合并到现有会话中。如果事件超出会话间隙，则将创建新会话。会话窗口的主要应用领域是用户行为分析。基于会话的分析可以包括简单的指标.</p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/streams-session-windows-01-1571297997083.png" alt></p>
<p>如果我们接收到另外三条记录（包括两条迟到的记录），那么绿色记录key的两个现有会话将合并为一个会话，从时间0开始到结束时间6，包括共有三条记录。蓝色记录key的现有会话将延长到时间5结束，共包含两个记录。最后，将在11时开始和结束蓝键的新会话。</p>
<p><img src="/2020/02/26/Kafka-md/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E7%AC%94%E8%AE%B0/Day14-kafka%E8%B5%84%E6%96%99/assets/streams-session-windows-02-1571297997111.png" alt></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.SessionWindows;</span><br><span class="line"></span><br><span class="line"><span class="comment">// A session window with an inactivity gap of 5 minutes.</span></span><br><span class="line">SessionWindows.with(Duration.ofMinutes(<span class="number">5</span>));</span><br></pre></td></tr></table></figure>

</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://gjq-666.github.io/2020/02/22/postgreSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/./source/image/xixi.gif">
      <meta itemprop="name" content="骚白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome gjq-666">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/22/postgreSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">postgreSQL学习笔记</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Publicado el</span>

              <time title="Creado por: 2020-02-22 19:23:59" itemprop="dateCreated datePublished" datetime="2020-02-22T19:23:59+08:00">2020-02-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Editado el</span>
                <time title="Modificado por: 2020-02-21 16:48:11" itemprop="dateModified" datetime="2020-02-21T16:48:11+08:00">2020-02-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="postgreSQL学习笔记"><a href="#postgreSQL学习笔记" class="headerlink" title="postgreSQL学习笔记"></a>postgreSQL学习笔记</h1><p>版本从6.0开始支持SQL解释器<br>安装一路下一步。</p>
<h3 id="1-创建数据库"><a href="#1-创建数据库" class="headerlink" title="1.创建数据库"></a>1.创建数据库</h3><p>​    //命令行操作<br>​    createdb database_name;<br>​    CREATE DATABASE database_name;</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">psql -l     //展示所有数据库</span><br><span class="line">psql database_name   //进入数据库</span><br></pre></td></tr></table></figure>
<h3 id="2-删除数据库"><a href="#2-删除数据库" class="headerlink" title="2.删除数据库"></a>2.删除数据库</h3><p>​    dropdb database;<br>​    DROP DATABASE database_name;</p>
<h3 id="3-创建表"><a href="#3-创建表" class="headerlink" title="3.创建表"></a>3.创建表</h3><p>​    create table 表名 (title varchar(255), content text);</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//修改表名</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> posts <span class="keyword">rename</span> <span class="keyword">to</span> 表名;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">//查看表信息(结构)</span><br><span class="line">\d 表名</span><br><span class="line">//执行这个sql文件</span><br><span class="line">\i a.sql</span><br><span class="line">//切换数据显示方式（横向和纵向显示） </span><br><span class="line">\x</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> posts (</span><br><span class="line">        <span class="keyword">id</span> <span class="built_in">serial</span> primary <span class="keyword">key</span>,</span><br><span class="line">        title <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">not</span> <span class="literal">null</span>,</span><br><span class="line">        <span class="keyword">content</span> <span class="built_in">text</span> <span class="keyword">check</span>(<span class="keyword">length</span>(<span class="keyword">content</span>) &gt; <span class="number">3</span>),</span><br><span class="line">        is_draft <span class="built_in">boolean</span> <span class="keyword">default</span> <span class="literal">FALSE</span>,</span><br><span class="line">        created_date <span class="built_in">timestamp</span> <span class="keyword">default</span> <span class="string">'now'</span>);</span><br></pre></td></tr></table></figure>

<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> public.students</span><br><span class="line">(</span><br><span class="line">  <span class="keyword">id</span> <span class="built_in">integer</span> <span class="keyword">NOT</span> <span class="literal">NULL</span>,</span><br><span class="line">  <span class="keyword">name</span> <span class="built_in">character</span>(<span class="number">128</span>),</span><br><span class="line">  subjects <span class="built_in">character</span>(<span class="number">1</span>),</span><br><span class="line">  <span class="keyword">CONSTRAINT</span> student2_pkey PRIMARY <span class="keyword">KEY</span> (<span class="keyword">id</span>)</span><br><span class="line">)</span><br><span class="line"><span class="keyword">WITH</span> (</span><br><span class="line">  OIDS=<span class="literal">FALSE</span></span><br><span class="line">);</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> public.students</span><br><span class="line">  OWNER <span class="keyword">TO</span> postgres;</span><br><span class="line"><span class="keyword">COMMENT</span> <span class="keyword">ON</span> <span class="keyword">TABLE</span> public.students</span><br><span class="line">  <span class="keyword">IS</span> <span class="string">'这是一个学生信息表'</span>;</span><br></pre></td></tr></table></figure>
<p>案例表：</p>
<p><img src="/2020/02/22/postgreSQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/C:%5CUsers%5CAdministrator%5CDesktop%5Cwork%5Cpg.jpg" alt="pg"></p>
<h3 id="4-删除表"><a href="#4-删除表" class="headerlink" title="4.删除表"></a>4.删除表</h3><p>​    DROP TABLE students;</p>
<h3 id="5-数据类型"><a href="#5-数据类型" class="headerlink" title="5.数据类型"></a>5.数据类型</h3><p>​    数值型：<br>​     <code>integer</code> <code>real</code> <code>serial</code>(序列型，一般用于自增字段)<br>​    文字型：<br>​      <code>char</code>  <code>varchar</code>  <code>text</code><br>​    布尔型：<br>​      <code>boolean</code><br>​    日期型：<br>​      <code>date</code> <code>time</code> <code>timestamp</code><br>​    特色类型：<br>​      <code>Array</code>   <code>inet</code>(网口类型)  <code>JSON</code>  <code>XML</code></p>
<h3 id="6-添加表约束"><a href="#6-添加表约束" class="headerlink" title="6.添加表约束"></a>6.添加表约束</h3><p>  <code>unique</code>:所在数据中值必须唯一<br>  <code>check</code>：字段设置条件，可以指定函数check(length(content) &gt; 3)必须超过3个字符<br> <code>default</code>：字段默认值</p>
<h3 id="7-INSERT语句"><a href="#7-INSERT语句" class="headerlink" title="7.INSERT语句"></a>7.INSERT语句</h3><p>  insert into 表名(cloum1,cloum2) value(a,b); </p>
<h3 id="8-数据抽出选项："><a href="#8-数据抽出选项：" class="headerlink" title="8.数据抽出选项："></a>8.数据抽出选项：</h3><p>  <code>order by</code>   <code>asc</code>升序  <code>desc</code>降序<br>  <code>limit</code>  限制返回数据条数<br>  <code>offset</code> 偏移量(从哪条数据开始)   </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//分页查询limit和offset结合使用。 </span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">users</span> <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">3</span>;</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">users</span> <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> <span class="keyword">limit</span> <span class="number">3</span> <span class="keyword">offset</span> <span class="number">3</span>;</span><br></pre></td></tr></table></figure>

<h3 id="9-统计抽出数据"><a href="#9-统计抽出数据" class="headerlink" title="9.统计抽出数据"></a>9.统计抽出数据</h3><p>  <code>distinct</code>  去重<br>  <code>group by</code>/<code>having</code>(在<code>group by</code>之后进行再次筛选) 分组  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> team,<span class="keyword">max</span>(score) <span class="keyword">from</span> <span class="keyword">users</span> <span class="keyword">group</span> <span class="keyword">by</span> team <span class="keyword">having</span> <span class="keyword">max</span>(score)&gt;<span class="number">25</span> <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">max</span>(score);</span><br></pre></td></tr></table></figure>



<h3 id="10-方便的函数"><a href="#10-方便的函数" class="headerlink" title="10.方便的函数"></a>10.方便的函数</h3><p>  <code>length</code>  <code>concat</code>(连接两个字符串)   <code>alias</code>(别名)  <code>substring</code>(截取字符串)  <code>random</code>  </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> player,<span class="keyword">length</span>(player) <span class="keyword">from</span> <span class="keyword">users</span>;</span><br><span class="line"><span class="keyword">select</span> player,<span class="keyword">concat</span>(player,<span class="string">"/"</span>,team) <span class="keyword">from</span> <span class="keyword">users</span>;</span><br><span class="line"><span class="keyword">select</span> player,<span class="keyword">concat</span>(player,<span class="string">"/"</span>,team) <span class="keyword">as</span> <span class="string">"球员信息"</span> <span class="keyword">from</span> <span class="keyword">users</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">concat</span>(<span class="string">'我'</span>,<span class="keyword">substring</span>(team,<span class="number">1</span>,<span class="number">1</span>)) <span class="keyword">as</span> <span class="string">"球队首文字"</span> <span class="keyword">from</span> <span class="keyword">users</span>;</span><br><span class="line">//随机抽奖</span><br><span class="line"><span class="keyword">select</span> player <span class="keyword">from</span> <span class="keyword">users</span> <span class="keyword">order</span> <span class="keyword">by</span> random() <span class="keyword">limit</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure>



<h3 id="11-更新和删除"><a href="#11-更新和删除" class="headerlink" title="11.更新和删除"></a>11.更新和删除</h3><p>  update [table] set [field=newvalue,…] where …<br>  delete from [table] where …<br>  eg:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> <span class="keyword">users</span> <span class="keyword">set</span> score = score + <span class="number">100</span> <span class="keyword">where</span> team <span class="keyword">IN</span> (<span class="string">"勇士"</span>,<span class="string">"骑士"</span>);</span><br></pre></td></tr></table></figure>



<h3 id="12-变更表结构"><a href="#12-变更表结构" class="headerlink" title="12.变更表结构"></a>12.变更表结构</h3><p>  //alter table [tablename] …</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//给表添加一条fullname字段</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> <span class="keyword">users</span> <span class="keyword">add</span> fullname <span class="built_in">varchar</span>(<span class="number">255</span>);</span><br></pre></td></tr></table></figure>

<p>  //给哪个表的字段建索引(索引提高查询效率，但是增删效果不好)<br>  create index 索引名 on users(字段名);   </p>
<p>  //删除索引<br>  drop index 索引名;</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//修改列名</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> <span class="keyword">users</span> <span class="keyword">rename</span> player <span class="keyword">to</span> nba_player;</span><br><span class="line">//修改字段的的长度</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> <span class="keyword">users</span> <span class="keyword">alter</span> nba_player <span class="keyword">type</span> <span class="built_in">varchar</span>(<span class="number">128</span>);</span><br></pre></td></tr></table></figure>

<h3 id="13-操作多个表"><a href="#13-操作多个表" class="headerlink" title="13.操作多个表"></a>13.操作多个表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">//两表的关联查询</span><br><span class="line"><span class="keyword">select</span> users.player,twitters.content <span class="keyword">from</span> <span class="keyword">users</span>,twitters <span class="keyword">where</span> users.id = twitters.user_id;</span><br><span class="line">//别名</span><br><span class="line"><span class="keyword">select</span> u.player,t.content <span class="keyword">from</span> <span class="keyword">users</span> <span class="keyword">as</span> u,twitters <span class="keyword">as</span> t <span class="keyword">where</span> u.id = t.user_id;</span><br></pre></td></tr></table></figure>
<h3 id="14-使用视图"><a href="#14-使用视图" class="headerlink" title="14.使用视图"></a>14.使用视图</h3><p>​    <code>视图</code>：视图就是一个select语句，把业务系统中常用的select语句简化成一个类似于表的对象，便于简单读取和开发。        (对于经常使用的select语句建立视图便于编码和管理)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//创建一个视图(通过 \dv 查看视图描述)</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> curry_twitters <span class="keyword">as</span> <span class="keyword">select</span> u.player,t.content <span class="keyword">from</span> <span class="keyword">users</span> <span class="keyword">as</span> u,twitters <span class="keyword">as</span> t <span class="keyword">where</span> u.id = t.user_id;</span><br><span class="line"></span><br><span class="line">//要进行查询时</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> curry_twitters;</span><br><span class="line"></span><br><span class="line">//删除视图</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> curry_twitters;</span><br></pre></td></tr></table></figure>
<h3 id="15-使用事务"><a href="#15-使用事务" class="headerlink" title="15.使用事务"></a>15.使用事务</h3><p>   <code>数据库事务</code>：是指作为单个逻辑工作单元执行的一系列操作，要么一起成功，要么一起失败。必须满足ACID(原子性、            一致性、隔离性、持久性)   </p>
<p>   <code>PostgreSQL数据库事务使用</code><br>      begin      开启事务<br>      commit     提交<br>      rollback   回滚 </p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">eg：</span><br><span class="line"><span class="keyword">begin</span>;</span><br><span class="line"><span class="keyword">update</span> <span class="keyword">users</span> <span class="keyword">set</span> score = <span class="number">50</span> <span class="keyword">where</span> player = <span class="string">'库里'</span>;</span><br><span class="line"><span class="keyword">update</span> <span class="keyword">users</span> <span class="keyword">set</span> score = <span class="number">60</span> <span class="keyword">where</span> player = <span class="string">'哈登'</span>;</span><br><span class="line"><span class="keyword">commit</span>;    //如果不想跟新可以 <span class="keyword">rollback</span>;</span><br></pre></td></tr></table></figure>



      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://gjq-666.github.io/2020/02/22/Hadoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/./source/image/xixi.gif">
      <meta itemprop="name" content="骚白">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="welcome gjq-666">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/22/Hadoop/" class="post-title-link" itemprop="url">Hadoop</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Publicado el</span>

              <time title="Creado por: 2020-02-22 19:16:55" itemprop="dateCreated datePublished" datetime="2020-02-22T19:16:55+08:00">2020-02-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Editado el</span>
                <time title="Modificado por: 2019-11-22 09:39:51" itemprop="dateModified" datetime="2019-11-22T09:39:51+08:00">2019-11-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h1><h2 id="1-1-大数据概念"><a href="#1-1-大数据概念" class="headerlink" title="1.1 大数据概念"></a>1.1 大数据概念</h2><p>大数据是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力来适应海量、高增长率和多样化的信息资产。</p>
<p>无法在一定时间范围内，使用常规软件工具对其内容进行抓取，管理和处理的数据集。</p>
<h2 id="1-2-大数据面临的问题"><a href="#1-2-大数据面临的问题" class="headerlink" title="1.2 大数据面临的问题"></a>1.2 大数据面临的问题</h2><p><code>存储</code>：单机存储有限，需要使用集群（多台机器）存储数据；硬件上必须有足够的存储容量，软件上有对应的容灾机制。</p>
<p><code>分析</code>：单机算力有限，也需要使用集群进行计算（需要在合理的时间内将数据变废为宝）</p>
<h2 id="1-3-大数据的特点"><a href="#1-3-大数据的特点" class="headerlink" title="1.3 大数据的特点"></a>1.3 大数据的特点</h2><blockquote>
<p>4V Volume 数据量大  Velocity 时效性   Variety 多样性 Value 价值大</p>
</blockquote>
<h3 id="1）数据量大"><a href="#1）数据量大" class="headerlink" title="1）数据量大"></a>1）数据量大</h3><p>B-KB-MB-GB-TB-PB-EB-ZB….</p>
<p>各种个人云存储解决方案：百度网盘、腾讯微云、115、lanzou、诚通、OneDriver、GoogleDriver 等</p>
<p>大数据产生于21世纪的互联网时代，日益进步的科技和日益增长的物质文化需求，导致了数据的大爆炸；</p>
<p>淘宝、支付宝、微信、QQ、抖音这些App是目前国内顶尖的流量，使用人数及其的庞大，每天可以产生极多的数据量。</p>
<h3 id="2）数据时效性"><a href="#2）数据时效性" class="headerlink" title="2）数据时效性"></a>2）数据时效性</h3><p>双十一、618</p>
<p>大数据是在短时间内迅速产生（产生的时效性非常高），分析的时效性就必须因场景而异，需要在合理的时间内分析出有价值的数据。</p>
<h3 id="3）数据多样性"><a href="#3）数据多样性" class="headerlink" title="3）数据多样性"></a>3）数据多样性</h3><h4 id="（1）数据存储类型多样性"><a href="#（1）数据存储类型多样性" class="headerlink" title="（1）数据存储类型多样性"></a>（1）数据存储类型多样性</h4><p>结构化的数据：表格、文本、SQL等</p>
<p>非结构化数据：视频、音频、图片</p>
<h4 id="（2）数据分析类型多样性"><a href="#（2）数据分析类型多样性" class="headerlink" title="（2）数据分析类型多样性"></a>（2）数据分析类型多样性</h4><p>地理位置：来自北京、中国、上海</p>
<p>设备信息：来自PC、手机、平板、手表、手环、眼镜</p>
<p>个人喜好：美女、面膜、ctrl、 数码、篮球、足球</p>
<p>社交网络：A可能认识B 、C ，B就可能认识C</p>
<p>电话号码：110,11086</p>
<p>网络身份证：设备MAC+电话+IP+地区</p>
<h3 id="4）数据价值"><a href="#4）数据价值" class="headerlink" title="4）数据价值"></a>4）数据价值</h3><p>警察叔叔：只关注的是否哪里违规</p>
<p>AI研究：犯罪预测、下棋、无人驾驶</p>
<p>所以在海量数据中有用的数据最为关键、这是分析数据的第一步，也就是对数据进行降噪处理（数据清洗|数据预处理）</p>
<h2 id="1-4-应用场景"><a href="#1-4-应用场景" class="headerlink" title="1.4 应用场景"></a>1.4 应用场景</h2><h3 id="1）个人推荐"><a href="#1）个人推荐" class="headerlink" title="1）个人推荐"></a>1）个人推荐</h3><p>根据用户喜好，推荐相关资源</p>
<p>千人一面、千人千面、一人千面</p>
<h3 id="2）风控"><a href="#2）风控" class="headerlink" title="2）风控"></a>2）风控</h3><p>大数据实时流处理，根据用户行为模型进行支撑，判断该行为是否正常 </p>
<h3 id="3）成本预测"><a href="#3）成本预测" class="headerlink" title="3）成本预测"></a>3）成本预测</h3><h3 id="4）气候预测"><a href="#4）气候预测" class="headerlink" title="4）气候预测"></a>4）气候预测</h3><h3 id="5）人工智能"><a href="#5）人工智能" class="headerlink" title="5）人工智能"></a>5）人工智能</h3><h2 id="1-5-工作方向"><a href="#1-5-工作方向" class="headerlink" title="1.5 工作方向"></a>1.5 工作方向</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 业务</span><br><span class="line">电商推荐、智能广告系统、专家系统、智能交通、智能医疗</span><br><span class="line">2 工作方向</span><br><span class="line"> 大数据开发工程师（实时计算、批处理、ETL、数据挖掘）、大数据运维工程师</span><br></pre></td></tr></table></figure>

<h2 id="1-6分布式"><a href="#1-6分布式" class="headerlink" title="1.6分布式"></a>1.6分布式</h2><p>为了解决大数据存储和计算的问题，需要使用一定数量的机器，硬件设施必须足够，那软件解决方案怎么办？</p>
<p>如何使用软件去解决存储和分析的问题？</p>
<h1 id="二、Hadoop"><a href="#二、Hadoop" class="headerlink" title="二、Hadoop"></a>二、Hadoop</h1><p><img src="/2020/02/22/Hadoop/622762d0f703918f3c528de35c3d269759eec41c.jpg" alt="img"></p>
<p>Hadoop由 Apache Software Foundation 公司于 2005 年秋天作为<a href="https://baike.baidu.com/item/Lucene" target="_blank" rel="noopener">Lucene</a>的子项目<a href="https://baike.baidu.com/item/Nutch" target="_blank" rel="noopener">Nutch</a>的一部分正式引入。它受到最先由 Google Lab 开发的 Map/Reduce 和 Google File System(<a href="https://baike.baidu.com/item/GFS" target="_blank" rel="noopener">GFS</a>) 的启发。</p>
<p>2006 年 3 月份，Map/Reduce 和 Nutch Distributed File System (NDFS) 分别被纳入称为 Hadoop 的项目中。</p>
<p>Hadoop 是最受欢迎的在 Internet 上对搜索<a href="https://baike.baidu.com/item/关键字" target="_blank" rel="noopener">关键字</a>进行内容分类的工具，但它也可以解决许多要求极大伸缩性的问题。例如，如果您要 grep 一个 10TB 的巨型文件，会出现什么情况？在传统的系统上，这将需要很长的时间。但是 Hadoop 在设计时就考虑到这些问题，采用<a href="https://baike.baidu.com/item/并行执行" target="_blank" rel="noopener">并行执行</a>机制，因此能大大提高效率。</p>
<p><code>HDFS</code>：Hadoop Distributed File System  作为Hadoop 生态体系中数据的存储的软件解决方案</p>
<p><code>MapReduce</code>：Hadoop中分布式计算框架（只需要实现少量的代码，就可以开发一个分布式的应用程序），对海量数据并行分析和计算</p>
<h2 id="2-1-Hadoop生态系统"><a href="#2-1-Hadoop生态系统" class="headerlink" title="2.1 Hadoop生态系统"></a>2.1 Hadoop生态系统</h2><p><code>HDFS</code>：Hadoop Distributed File System  作为Hadoop 生态体系中数据的存储的软件解决方案</p>
<p><code>MapReduce</code>：Hadoop中分布式计算框架（只需要实现少量的代码，就可以开发一个分布式的应用程序），对海量数据并行分析和计算</p>
<p><code>HBase</code>: 基于HDFS 的列式存储的NoSql</p>
<p><code>Hive</code>:是一款SQL解释引擎，能够将SQL语句翻译成MR代码</p>
<p><code>Flume</code>:分布式的日志收集系统，用于收集海量日志数据，并将其存储在hdfS中</p>
<p><code>kafka</code>:消息对列，实现对分布式应用程序间的解耦和数据缓冲</p>
<p><code>Zookeeper</code>：分布式协调服务，用户注册中心、配置中心、集群选举、状态检测、分布式锁</p>
<h2 id="2-2-大数据分析方案"><a href="#2-2-大数据分析方案" class="headerlink" title="2.2 大数据分析方案"></a>2.2 大数据分析方案</h2><p><code>MapReduce</code>:大数据离线批处理（代表基于磁盘，延迟30分钟+）</p>
<p><code>Spark</code>：大数据离线批处理（代表基于内存，速度相对于MR来说快的多）</p>
<p><code>Strom/Spark Streaming/Kafka Streaming/Flink</code>:实时流处理框架，达到对记录级别消息的毫秒级处理</p>
<h1 id="三、HDFS"><a href="#三、HDFS" class="headerlink" title="三、HDFS"></a>三、HDFS</h1><h2 id="3-1-安装（伪集群）"><a href="#3-1-安装（伪集群）" class="headerlink" title="3.1 安装（伪集群）"></a>3.1 安装（伪集群）</h2><h3 id="1）准备虚拟机"><a href="#1）准备虚拟机" class="headerlink" title="1）准备虚拟机"></a>1）准备虚拟机</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">更改IP</span><br><span class="line">删除MAC地址 </span><br><span class="line">更改主机名     vi &#x2F;etc&#x2F;sysconfig&#x2F;network</span><br></pre></td></tr></table></figure>

<h3 id="2）安装JDK-8"><a href="#2）安装JDK-8" class="headerlink" title="2）安装JDK 8"></a>2）安装JDK 8</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">略</span><br></pre></td></tr></table></figure>

<h3 id="3）配置Java环境变量"><a href="#3）配置Java环境变量" class="headerlink" title="3）配置Java环境变量"></a>3）配置Java环境变量</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">export</span> <span class="string">JAVA_HOME=/usr/java/jdk1.8.0_171-amd64/</span></span><br><span class="line"><span class="attr">export</span> <span class="string">PATH=$PATH:$JAVA_HOME/bin</span></span><br></pre></td></tr></table></figure>

<h3 id="4）配置主机名与IP的映射关系"><a href="#4）配置主机名与IP的映射关系" class="headerlink" title="4）配置主机名与IP的映射关系"></a>4）配置主机名与IP的映射关系</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]#  vi /etc/sysconfig/network</span><br><span class="line">NETWORKING=yes</span><br><span class="line">HOSTNAME=HadoopNode00</span><br><span class="line"></span><br><span class="line">[root@HadoopNode00 ~]# vi /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.11.20 HadoopNode00</span><br></pre></td></tr></table></figure>

<h3 id="5）关闭防火墙"><a href="#5）关闭防火墙" class="headerlink" title="5）关闭防火墙"></a>5）关闭防火墙</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# service iptables stop   #  关闭防火墙</span><br><span class="line">[root@HadoopNode00 ~]# chkconfig iptables off  # 关闭防火墙开机自动启动</span><br></pre></td></tr></table></figure>

<h3 id="6）ssh免密登陆"><a href="#6）ssh免密登陆" class="headerlink" title="6）ssh免密登陆"></a>6）ssh免密登陆</h3><p>SSH是Secure Shell 的缩写，SSH为建立在应用层山的安全协议，专为远程登陆会话和其他网络服务提供安全协议支持。</p>
<p><code>基于口令的安全验证</code>：基于用户名和密码  root | 123456</p>
<p>基于密钥的安全验证：需要依靠密钥</p>
<p><img src="/2020/02/22/Hadoop/ssh%E5%85%8D%E5%AF%86%E7%99%BB%E9%99%86-1568702316720.png" alt="ssh免密登陆"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# ssh-keygen -t rsa   # 生成密钥</span><br><span class="line">[root@HadoopNode00 ~]# ssh-copy-id HadoopNOde00</span><br></pre></td></tr></table></figure>

<h3 id="7）解压Hadoop"><a href="#7）解压Hadoop" class="headerlink" title="7）解压Hadoop"></a>7）解压Hadoop</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">解压Hadoop到指定目录</span><br><span class="line">[root@HadoopNode00 ~]# mkdir /home/hadoop/</span><br><span class="line">[root@HadoopNode00 ~]# tar -zxvf /home/hadoop/hadoop-2.6.0.tar.gz  -C /home/hadoop</span><br></pre></td></tr></table></figure>

<h3 id="8）配置Hadoop环境变量"><a href="#8）配置Hadoop环境变量" class="headerlink" title="8）配置Hadoop环境变量"></a>8）配置Hadoop环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/home/hadoop/hadoop-2.6.0</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>HADOOP_HOME</code>环境变量别第三方依赖，hbase hive flume在集成HADOOP的时候，是通过HADOOP_HOME找到hadoop的位置</p>
</blockquote>
<h3 id="9）配置-etc-hadoop-core-site-xml"><a href="#9）配置-etc-hadoop-core-site-xml" class="headerlink" title="9）配置 etc/hadoop/core-site.xml"></a>9）配置 etc/hadoop/core-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://HadoopNode00:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hadoop-2.6.0/hadoop-$&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="10）配置-etc-hadoop-hdfs-site-xml"><a href="#10）配置-etc-hadoop-hdfs-site-xml" class="headerlink" title="10）配置 etc/hadoop/hdfs-site.xml"></a>10）配置 etc/hadoop/hdfs-site.xml</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="11）格式化namenode"><a href="#11）格式化namenode" class="headerlink" title="11）格式化namenode"></a>11）格式化namenode</h3><blockquote>
<p>第一次启动hdfs的时候，需要格式化namenode</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hdfs namenode -format</span><br><span class="line">[root@HadoopNode00 ~]# tree /home/hadoop/hadoop-2.6.0/hadoop-root</span><br><span class="line">/home/hadoop/hadoop-2.6.0/hadoop-root</span><br><span class="line">└── dfs</span><br><span class="line">    └── name</span><br><span class="line">        └── current</span><br><span class="line">            ├── fsimage_0000000000000000000</span><br><span class="line">            ├── fsimage_0000000000000000000.md5</span><br><span class="line">            ├── seen_txid</span><br><span class="line">            └── VERSION</span><br><span class="line"></span><br><span class="line">3 directories, 4 files</span><br></pre></td></tr></table></figure>

<h3 id="12）启动hdfs"><a href="#12）启动hdfs" class="headerlink" title="12）启动hdfs"></a>12）启动hdfs</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh   # 开启HDFS </span><br><span class="line">stop-dfs.sh    # 关闭hdfs</span><br></pre></td></tr></table></figure>

<p>进入web界面</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http:&#x2F;&#x2F;主机名:50070</span><br></pre></td></tr></table></figure>

<blockquote>
<p>windows下 配置域名与ip的映射：C:\Windows\System32\drivers\etc \hosts</p>
</blockquote>
<h2 id="3-2-HDFS-Shell-相关操作"><a href="#3-2-HDFS-Shell-相关操作" class="headerlink" title="3.2 HDFS Shell 相关操作"></a>3.2 HDFS Shell 相关操作</h2><h3 id="1）hdfs-shell"><a href="#1）hdfs-shell" class="headerlink" title="1）hdfs shell"></a>1）hdfs shell</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs</span><br><span class="line">Usage: hadoop fs [generic options]</span><br><span class="line">        [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-cat [-ignoreCrc] &lt;src&gt; ...]</span><br><span class="line">        [-checksum &lt;src&gt; ...]</span><br><span class="line">        [-chgrp [-R] GROUP PATH...]</span><br><span class="line">        [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">        [-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br><span class="line">        [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">        [-count [-q] [-h] &lt;path&gt; ...]</span><br><span class="line">        [-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;]</span><br><span class="line">        [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]]</span><br><span class="line">        [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;]</span><br><span class="line">        [-df [-h] [&lt;path&gt; ...]]</span><br><span class="line">        [-du [-s] [-h] &lt;path&gt; ...]</span><br><span class="line">        [-expunge]</span><br><span class="line">        [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">        [-getfacl [-R] &lt;path&gt;]</span><br><span class="line">        [-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;]</span><br><span class="line">        [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">        [-help [cmd ...]]</span><br><span class="line">        [ ..]</span><br><span class="line">        [-usage [cmd ...]]</span><br><span class="line"></span><br><span class="line">Generic options supported are</span><br><span class="line">-conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line">-D &lt;property=value&gt;            use value for given property</span><br><span class="line">-fs &lt;local|namenode:port&gt;      specify a namenode</span><br><span class="line">-jt &lt;local|resourcemanager:port&gt;    specify a ResourceManager</span><br><span class="line">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span><br><span class="line">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include in the classpath.</span><br><span class="line">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span><br><span class="line"></span><br><span class="line">The general command line syntax is</span><br><span class="line">bin/hadoop command [genericOptions] [commandOptions]</span><br></pre></td></tr></table></figure>



<h3 id="2）上传文件"><a href="#2）上传文件" class="headerlink" title="2）上传文件"></a>2）上传文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 上传 root目录下的install.log  到hdfs 根目录下</span></span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -put  /root/install.log  /1.txt</span><br></pre></td></tr></table></figure>

<h3 id="3-）-ls文件"><a href="#3-）-ls文件" class="headerlink" title="3 ） ls文件"></a>3 ） ls文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 找到到了刚才上传为文件命名为1.txt</span></span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -ls /</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup       8901 2019-09-17 23:28 /1.txt</span><br></pre></td></tr></table></figure>

<h3 id="4）下载文件"><a href="#4）下载文件" class="headerlink" title="4）下载文件"></a>4）下载文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs -get  /1.txt /root/baizhi.txt</span><br></pre></td></tr></table></figure>

<h3 id="5）删除文件"><a href="#5）删除文件" class="headerlink" title="5）删除文件"></a>5）删除文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs -rm /2.txt</span><br><span class="line">19/09/17 23:36:05 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.</span><br><span class="line">Deleted /2.txt</span><br></pre></td></tr></table></figure>

<h3 id="6）查看文件"><a href="#6）查看文件" class="headerlink" title="6）查看文件"></a>6）查看文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs -cat /1.txt</span><br><span class="line">Installing libgcc-4.4.7-23.el6.x86_64</span><br><span class="line">warning: libgcc-4.4.7-23.el6.x86_64: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY</span><br><span class="line">Installing setup-2.8.14-23.el6.noarch</span><br></pre></td></tr></table></figure>

<h3 id="7）创建文件夹"><a href="#7）创建文件夹" class="headerlink" title="7）创建文件夹"></a>7）创建文件夹</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs -mkdir /baizhi</span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 root supergroup       8901 2019-09-17 23:28 /1.txt</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-09-17 23:37 /baizhi</span><br></pre></td></tr></table></figure>

<h3 id="8）复制文件"><a href="#8）复制文件" class="headerlink" title="8）复制文件"></a>8）复制文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop fs -cp /1.txt /baizhi/</span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -ls /</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   1 root supergroup       8901 2019-09-17 23:28 /1.txt</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2019-09-17 23:38 /baizhi</span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -ls /baizhi</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   1 root supergroup       8901 2019-09-17 23:38 /baizhi/1.txt</span><br></pre></td></tr></table></figure>



<h3 id="9）开启回收站机制"><a href="#9）开启回收站机制" class="headerlink" title="9）开启回收站机制"></a>9）开启回收站机制</h3><p>core-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>设置一分钟延迟</p>
</blockquote>
<h2 id="3-3-Java-API-操作HDFS"><a href="#3-3-Java-API-操作HDFS" class="headerlink" title="3.3  Java API 操作HDFS"></a>3.3  Java API 操作HDFS</h2><h3 id="（1）-依赖"><a href="#（1）-依赖" class="headerlink" title="（1） 依赖"></a>（1） 依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-hdfs --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="（2）Windows-配置Hadoop环境"><a href="#（2）Windows-配置Hadoop环境" class="headerlink" title="（2）Windows 配置Hadoop环境"></a>（2）Windows 配置Hadoop环境</h3><ul>
<li>解压hadoop到指定的目录</li>
<li>拷贝hadoop.dll和winutils.exe到hadoop/bin 目录下</li>
<li>配置Hadoop环境变量</li>
<li>配置主机名和IP的映射关系</li>
</ul>
<h3 id="（3）权限不足解决方案"><a href="#（3）权限不足解决方案" class="headerlink" title="（3）权限不足解决方案"></a>（3）权限不足解决方案</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.security.AccessControlException: Permission denied: user=Administrator, access=WRITE, inode=<span class="string">"/baizhi"</span>:root:supergroup:drwxr-xr-x</span><br></pre></td></tr></table></figure>



<h4 id="1）配置-hdfs-site-xml"><a href="#1）配置-hdfs-site-xml" class="headerlink" title="1）配置 hdfs-site.xml"></a>1）配置 hdfs-site.xml</h4><blockquote>
<p>将权限检查关闭</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h4 id="2）方案2"><a href="#2）方案2" class="headerlink" title="2）方案2"></a>2）方案2</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-DHADOOP_USER_NAME&#x3D;root</span><br></pre></td></tr></table></figure>

<h4 id="3）方案3"><a href="#3）方案3" class="headerlink" title="3）方案3"></a>3）方案3</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"root"</span>);</span><br></pre></td></tr></table></figure>

<h3 id="（3）相关操作"><a href="#（3）相关操作" class="headerlink" title="（3）相关操作"></a>（3）相关操作</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"><span class="keyword">import</span> org.junit.experimental.theories.suppliers.TestedOn;</span><br><span class="line"><span class="keyword">import</span> sun.awt.geom.AreaOp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.FileOutputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Configuration configuration;</span><br><span class="line">    <span class="keyword">private</span> FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Before</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getClient</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//配置读写权限</span></span><br><span class="line">        System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"root"</span>);</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 准备配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 添加相应的配置文件*/</span></span><br><span class="line">        configuration.addResource(<span class="string">"core-site.xml"</span>);</span><br><span class="line">        configuration.addResource(<span class="string">"hdfs-site.xml"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 通过FileSystem.newInstance 获得客户端对象*/</span></span><br><span class="line">        fileSystem = FileSystem.newInstance(configuration);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testUpload01</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 源文件  |   目标文件</span></span><br><span class="line"><span class="comment">         * Path 对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        fileSystem.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"G:\\A.docx"</span>), <span class="keyword">new</span> Path(<span class="string">"/baizhi/2.docx"</span>));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testUpload02</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 准备 本地输入流</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="string">"G:\\A.docx"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 准备 hdfs 输出流</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        FSDataOutputStream outputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"/baizhi/3.docx"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 使用工具类进行拷贝</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        IOUtils.copyBytes(inputStream, outputStream, <span class="number">1024</span>, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDownload01</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        fileSystem.copyToLocalFile(<span class="keyword">false</span>, <span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>), <span class="keyword">new</span> Path(<span class="string">"G:\\3.txt"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDownload02</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        FileOutputStream outputStream = <span class="keyword">new</span> FileOutputStream(<span class="string">"G:\\4.txt"</span>);</span><br><span class="line"></span><br><span class="line">        FSDataInputStream inputStream = fileSystem.open(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>));</span><br><span class="line">        IOUtils.copyBytes(inputStream, outputStream, <span class="number">1024</span>, <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test011</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        RemoteIterator&lt;LocatedFileStatus&gt; list = fileSystem.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (list.hasNext()) &#123;</span><br><span class="line"></span><br><span class="line">            LocatedFileStatus locatedFileStatus = list.next();</span><br><span class="line">            Path path = locatedFileStatus.getPath();</span><br><span class="line">            System.out.println(path.toString());</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test02</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">        fileSystem.delete(<span class="keyword">new</span> Path(<span class="string">"/baizhi"</span>),<span class="keyword">false</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">test03</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">boolean</span> exists = fileSystem.exists(<span class="keyword">new</span> Path(<span class="string">"/1.txt"</span>));</span><br><span class="line">        <span class="keyword">if</span> (exists)&#123;</span><br><span class="line">            System.out.println(<span class="string">"文件存在"</span>);</span><br><span class="line">        &#125;<span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">            System.out.println(<span class="string">"文件不存在"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testy04</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">        fileSystem.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/baizhi1243"</span>));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="3-4-HDFS-Architecture（架构-）"><a href="#3-4-HDFS-Architecture（架构-）" class="headerlink" title="3.4 HDFS Architecture（架构 ）"></a>3.4 HDFS Architecture（架构 ）</h2><p>HDFS为主从架构，HDFS中有一个主的NameNode，管理系统命名空间和管理客户端对文件的访问，其中还有DataNode负责和NameNode进行协调工作，DataNode负责数据的存储，在存储数据（文件）的过程中一个文件会被分成一个块或者多个块，在NameNode中存储了一些数据（存储的数据是块到DataNode的映射关系），datanode还根据NameNode的指令创建删除复制块。</p>
<p><img src="/2020/02/22/Hadoop/hdfsarchitecture.png" alt="HDFS Architecture"></p>
<p><code>namenode</code>:存储元数据（用户描述数据的数据），负责管理DataNode</p>
<p><code>datanode</code>：用于存储数据块的节点，负责响应客户端的对块的读写请求，向NameNode汇报自己的块信息</p>
<p><code>block块</code>：数据块，hdfs中对文件拆分的最小单元，切分尺度默认为128MB，每个块在默认情况下有三个副本</p>
<p><code>rack</code>：机架，使用机架配置文件对存储节点进行物理编排，用于优化存储和计算</p>
<h3 id="1）什么是Block块"><a href="#1）什么是Block块" class="headerlink" title="1）什么是Block块"></a>1）什么是Block块</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">      The default block size for new files, in bytes.</span><br><span class="line">      You can use the following suffix (case insensitive):</span><br><span class="line">      k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.),</span><br><span class="line">      Or provide complete size in bytes (such as 134217728 for 128 MB).</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<p><img src="/2020/02/22/Hadoop/1568773592079.png" alt="1568773592079"></p>
<h3 id="（1）为什么块的大小为128MB？"><a href="#（1）为什么块的大小为128MB？" class="headerlink" title="（1）为什么块的大小为128MB？"></a>（1）为什么块的大小为128MB？</h3><p>在Hadoop1.x 块大小默认为64MB，在Hadoop2.x 默认为128MB</p>
<p>工业限制：一般来说机械硬盘的读取速度100MB左右</p>
<p>软件优化：通常认为最佳状态为寻址时间为传输时间的100分之一</p>
<h3 id="（2）Block块的大小能否随意设置？"><a href="#（2）Block块的大小能否随意设置？" class="headerlink" title="（2）Block块的大小能否随意设置？"></a>（2）Block块的大小能否随意设置？</h3><p>不能，如果BlockSize过大，可能导致多余存储空间浪费，导致存取时间过长 如果BlockSize过小，会导致寻址时间过长，同样造成效率低下。</p>
<h3 id="（3）HDFS为什么不适合存储小文件"><a href="#（3）HDFS为什么不适合存储小文件" class="headerlink" title="（3）HDFS为什么不适合存储小文件"></a>（3）HDFS为什么不适合存储小文件</h3><table>
<thead>
<tr>
<th>文件</th>
<th>namenode内存占用</th>
<th>datanode磁盘占用</th>
</tr>
</thead>
<tbody><tr>
<td>128MB 单文件</td>
<td>1个Blcok元数据的大小</td>
<td>128MB</td>
</tr>
<tr>
<td>128*1MB</td>
<td>128个Block元数据的大小</td>
<td>128MB</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>namenode内存会过于紧张</p>
<h3 id="2）Rack-Awareness-机架感知"><a href="#2）Rack-Awareness-机架感知" class="headerlink" title="2）Rack Awareness  机架感知"></a>2）Rack Awareness  机架感知</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">对于常见情况，当复制因子为3时，HDFS的放置策略是将一个副本放在本地机架中的一个节点上，另一个放在本地机架中的另一个节点上，将最后一个放在另一个机架中的另一个节点上。此策略可以减少机架间写入流量，从而提高写入性能。机架故障的可能性远小于节点故障的可能性;此策略不会影响数据可靠性和可用性保证。但是，它确实减少了读取数据时使用的聚合网络带宽，因为块只放在两个唯一的机架而不是三个。使用此策略时，文件的副本不会均匀分布在机架上。三分之一的副本位于一个节点上，三分之二的副本位于一个机架上，另外三个副本均匀分布在剩余的机架上。此策略可提高写入性能，而不会影响数据可靠性或读取性能。</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/22/Hadoop/1568776795084.png" alt="1568776795084"></p>
<p>查看默认机架</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hdfs  dfsadmin  -printTopology</span><br><span class="line">Rack: /default-rack</span><br><span class="line">   192.168.11.20:50010 (HadoopNode00)</span><br></pre></td></tr></table></figure>



<h3 id="3）NameNode-和-SecondaryNameNode-的-关系-（重点）"><a href="#3）NameNode-和-SecondaryNameNode-的-关系-（重点）" class="headerlink" title="3）NameNode 和 SecondaryNameNode 的 关系 （重点）"></a>3）NameNode 和 SecondaryNameNode 的 关系 （重点）</h3><p>fsimage文件：元数据信息的备份，会被加载到内存中</p>
<p>edits文件：Edits文件帮助记录增加和更新操作，提高效率</p>
<p>namenode在启动时会加载fsimage和edits的文件，所以在第一次启动的时候需要格式化namenode</p>
<p>当用户上传文件的时候或者进行其他操作的时候，操作记录会写入edits文件中，这样edits和fsimage文件加起来的元数据永远是最新的。</p>
<p>如果此时用户一直进行操作的话，edits文件会越来越大，这就导致了在下次启动的时候启动速度过慢。</p>
<p>为了解决这个问题，出现了SecondaryNameNode ，将当前的NameNode的edits和fsimage文件拷贝到自己的节点上，进行合并操作，在合并完成后，将新的fsimage文件传输到原来的namenode中，此时namanode再去加载最新的fsimage。</p>
<p>新的问题：在SecondaryNameNode 进行拷贝操作的时候，如果有客户端读写请求过来，势必要追加相应的操作记录到edits文件中，但是此时正在进行拷贝操作，改变则代表会造成数据紊乱，怎么办？解办法是：会有一个新的叫做edits-inprogress的文件被创建，新的操作将写入此文件中，等待SecondaryNameNode合并完成，将edits-inprogress文件改名成为当前的edits文件。</p>
<p><img src="/2020/02/22/Hadoop/1568778812355.png" alt="1568778812355"></p>
<h3 id="4）检查点"><a href="#4）检查点" class="headerlink" title="4）检查点"></a>4）检查点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">namenode使用fsimage和edits文件保存元数据，2nn会定期的下载主的（Active）namenode的fsimage文件和edits 文件，并在本地进行合并。</span><br><span class="line">合并的时机就称之为检查点</span><br><span class="line">检查点有两种触发机制：</span><br><span class="line">（1） 默认一个小时进行合并</span><br><span class="line">（2） 操作数量达到100W次进行合并</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600s<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    The number of seconds between two periodic checkpoints.</span><br><span class="line">    Support multiple time unit suffix(case insensitive), as described</span><br><span class="line">    in dfs.heartbeat.interval.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The Secondary NameNode or CheckpointNode will create a checkpoint</span><br><span class="line">  of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless</span><br><span class="line">  of whether 'dfs.namenode.checkpoint.period' has expired.</span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="5）Safemode"><a href="#5）Safemode" class="headerlink" title="5）Safemode"></a>5）Safemode</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">在启动时候加载fsimage和edits文件，等待其他的DataNode报告块信息，直至大部分块可用。在次期间，集群处于SafeMode，NameNode的安全模式本质上是HDFS集群的只读模式，它不允许对文件系统或块进行任何修改。</span><br><span class="line">通常，在DataNode报告大多数文件系统块可用之后，NameNode会自动离开Safemode。</span><br><span class="line">可以手动的进入或者退出SafeMode</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hdfs dfsadmin -safemode  enter</span><br><span class="line">Safe mode is ON</span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -put /root/1.txt  /</span><br><span class="line">put: Cannot create file/1.txt._COPYING_. Name node is in safe mode.</span><br><span class="line">[root@HadoopNode00 ~]# hdfs dfsadmin -safemode  leave</span><br><span class="line">Safe mode is OFF</span><br><span class="line">[root@HadoopNode00 ~]# hadoop fs -put /root/1.txt  /</span><br></pre></td></tr></table></figure>

<h3 id="6）DataNode工作机制"><a href="#6）DataNode工作机制" class="headerlink" title="6）DataNode工作机制"></a>6）DataNode工作机制</h3><p><img src="/2020/02/22/Hadoop/1568789485184.png" alt="1568789485184"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动的时候会注册DataNode</span><br><span class="line">周期向NameNode上报块信息，并且对其信息状态进行反馈，DataNode进行相应的操作</span><br><span class="line">心跳不能出现10分钟以上的断连，必须重启DataNode才能重现上线</span><br></pre></td></tr></table></figure>

<h1 id="四、MapReduce"><a href="#四、MapReduce" class="headerlink" title="四、MapReduce"></a>四、MapReduce</h1><h2 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h2><p>MapReduce是一种编程模型，用于大规模数据集（大于1TB）的并行运算。概念”Map（映射）”和”Reduce（归约）”，是它们的主要思想，都是从函数式编程语言里借来的，还有从矢量编程语言里借来的特性。它极大地方便了编程人员在不会分布式并行编程的情况下，将自己的程序运行在<a href="https://baike.baidu.com/item/分布式系统/4905336" target="_blank" rel="noopener">分布式系统</a>上。 当前的软件实现是指定一个Map（映射）函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce（归约）函数，用来保证所有映射的键值对中的每一个共享相同的键组。</p>
<p><code>MapReduce</code>是Hadoop框架的一个<code>并行计算框架</code>,将一个计算任务拆分成两个阶段：Map和Reduce</p>
<p>MapReduce计算框架充分利用了 存储节点（DataNode）所在物理主机的计算资源进行并行计算</p>
<p>默认情况下NodeManager会将本进程运行 的节点的计算资源抽像成8个计算单元，每个单元称之为一个<code>Contioner</code>，所有的NodeManager都由ResourceManager调度，ResourceManager负责计算资源的统筹分配。</p>
<blockquote>
<p>一是软件框架   二是并行处理   三 可靠容错  四 大规模计算   五 处理海量数据</p>
</blockquote>
<p><img src="/2020/02/22/Hadoop/1568860846937.png" alt="1568860846937"></p>
<p>MapReduce擅长做大数据处理，MapReduce的思想就是<code>分而治之</code></p>
<ul>
<li><p>Map负责<strong>”分“</strong>，即把庞大且复杂的任务分解成若干个”简单的任务“来处理，简单的任务包含三层</p>
<ul>
<li>是对数据或者计算模型相对于原任务要大大缩小</li>
<li>就近计算原则，就是任务会被分配到存放所需数据的节点上进行计算</li>
<li>这些小任务不止一个且并行计算，而且彼此间没有依赖关系</li>
</ul>
</li>
</ul>
<ul>
<li>Reducer负责对Map的计算结果<strong>进行汇总</strong></li>
</ul>
<h2 id="4-2-为什么使用MR？"><a href="#4-2-为什么使用MR？" class="headerlink" title="4.2 为什么使用MR？"></a>4.2 为什么使用MR？</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.hdfs;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.File;</span><br><span class="line"><span class="keyword">import</span> java.io.FileReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileWriter;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CleanApp</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        File file = <span class="keyword">new</span> File(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\access.tmp2019-05-19-10-28.log"</span>);</span><br><span class="line"></span><br><span class="line">        BufferedReader bufferedReader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> FileReader(file));</span><br><span class="line"></span><br><span class="line">        FileWriter fileWriter = <span class="keyword">new</span> FileWriter(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\clean.log"</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"></span><br><span class="line">            String line = bufferedReader.readLine();</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (line == <span class="keyword">null</span>) &#123;</span><br><span class="line">                bufferedReader.close();</span><br><span class="line">                fileWriter.close();</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">boolean</span> contains = line.contains(<span class="string">"thisisshortvideoproject'slog"</span>);</span><br><span class="line">            <span class="keyword">if</span> (contains) &#123;</span><br><span class="line"></span><br><span class="line">                String s = line.split(<span class="string">"thisisshortvideoproject'slog"</span>)[<span class="number">0</span>];</span><br><span class="line">                fileWriter.write(s.trim() + <span class="string">"\n"</span>);</span><br><span class="line">                fileWriter.flush();</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述代码是对日志进行简单的清晰，在数据量少的时候一点问题都没有，但是数据量一旦增加，就可能无法胜任需求，因为无法在合理的时间内完成计算，此时单机性能已经成为计算的瓶颈，但是手写分布式应用程序难度太大，有现成的框架可以使用，那就是MR!</p>
<h2 id="4-3-YARN-环境搭建"><a href="#4-3-YARN-环境搭建" class="headerlink" title="4.3 YARN  环境搭建"></a>4.3 YARN  环境搭建</h2><h3 id="（1）什么是-YARN-？"><a href="#（1）什么是-YARN-？" class="headerlink" title="（1）什么是 YARN ？"></a>（1）什么是 YARN ？</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Yarn作为一个资源调度平台,有一个全局的管理者叫做ResourceManager，ResourceManager负责对集群的整体计算及资源做统筹规划，有各个节点的管理者叫做NodeManager，负责向ResourceManager报告其计算资源的使用情况，在NodeManger中有一个MRAppMaster管理这里当前运行的MRApp，其任务是协调来自ResourceManager的资源，并与NodeManager一起执行和监视任务。</span><br></pre></td></tr></table></figure>

<p><code>ResourceManager</code>：负责对集群的整体计算及资源做统筹规划</p>
<p><code>NodeManager</code>：管理主机上的计算组员，负责向RM 汇报自身的状态信息</p>
<p><code>MRAppMaster</code>：计算任务的Master，负责申请计算资源，协调计算任务</p>
<p><code>YARN Child</code>：负责做实际计算任务</p>
<p><code>Container：</code>计算资源的抽象单元</p>
<p><img src="/2020/02/22/Hadoop/yarn_architecture.gif" alt="MapReduce NextGen Architecture"></p>
<h3 id="（2）配置YARN"><a href="#（2）配置YARN" class="headerlink" title="（2）配置YARN"></a>（2）配置YARN</h3><p><code>etc/hadoop/yarn-site.xml</code></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span>The hostname of the RM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>HadoopNode00<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><code>etc/hadoop/mapred-site.xml</code></p>
<blockquote>
<p> etc/hadoop/  下其实是没有这个文件 的但是有yitmp结尾的文件，将其改名即可</p>
</blockquote>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="（3）启动YARN"><a href="#（3）启动YARN" class="headerlink" title="（3）启动YARN"></a>（3）启动YARN</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# start-yarn.sh</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /home/hadoop/hadoop-2.6.0/logs/yarn-root-resourcemanager-HadoopNode00.out</span><br><span class="line">localhost: starting nodemanager, logging to /home/hadoop/hadoop-2.6.0/logs/yarn-root-nodemanager-HadoopNode00.out</span><br><span class="line">[root@HadoopNode00 ~]# jps</span><br><span class="line">60192 Jps</span><br><span class="line">60046 ResourceManager</span><br><span class="line">60142 NodeManager</span><br></pre></td></tr></table></figure>

<blockquote>
<p>web 界面： hostname:8088</p>
</blockquote>
<h2 id="4-4-MR-入门程序"><a href="#4-4-MR-入门程序" class="headerlink" title="4.4 MR 入门程序"></a>4.4 MR 入门程序</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">需求：</span><br><span class="line">wangkai gjf zkf suns gzy</span><br><span class="line">wangkai zkf suns gzy</span><br><span class="line">zkf suns gzy hxz leijun</span><br><span class="line"></span><br><span class="line">wangkai 2</span><br><span class="line">gjf 1</span><br><span class="line">zkf 3 </span><br><span class="line">suns 3</span><br><span class="line">gzy 3</span><br><span class="line">hxz 1</span><br><span class="line">leijun 1</span><br></pre></td></tr></table></figure>

<h3 id="（1）依赖"><a href="#（1）依赖" class="headerlink" title="（1）依赖"></a>（1）依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-hdfs<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">      </span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line"> 	 	<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-jobclient<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  	<span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-mapreduce-client-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="（2）Mapper-逻辑"><a href="#（2）Mapper-逻辑" class="headerlink" title="（2）Mapper 逻辑"></a>（2）Mapper 逻辑</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* keyIn  LongWritable (Long) 输入文本字节偏移量</span></span><br><span class="line"><span class="comment">* valueIn Text (String)      输入文本行</span></span><br><span class="line"><span class="comment">*  keyOut Text(String)</span></span><br><span class="line"><span class="comment">*  valueOut IntWritable(Int)</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCMapper</span>  <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String[] names = value.toString().split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String name : names) &#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(name),<span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="（3）Reduce-逻辑"><a href="#（3）Reduce-逻辑" class="headerlink" title="（3）Reduce 逻辑"></a>（3）Reduce 逻辑</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *keyIn Text 与mapper的keyOut的数据类型相对应</span></span><br><span class="line"><span class="comment"> *valeuIn IntWritable   与mapper的ValueOut的数据类型相对应</span></span><br><span class="line"><span class="comment"> * KeyOut</span></span><br><span class="line"><span class="comment"> * valueOut</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="（4）Job封装"><a href="#（4）Job封装" class="headerlink" title="（4）Job封装"></a>（4）Job封装</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取Job对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置数据输入输出组件</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *设置数据输入输出路径</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/wordcount.txt"</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 注意： 此输出路径不能存在</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"/baizhi/out1"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置MAP 和 REDUCE 处理逻辑</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapperClass(WCMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(WCReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置 map任务和reduce任务的输出泛型</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//  提交</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.submit();</span></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="4-5-部署运行"><a href="#4-5-部署运行" class="headerlink" title="4.5 部署运行"></a>4.5 部署运行</h2><h3 id="（1）远程Jar-包部署"><a href="#（1）远程Jar-包部署" class="headerlink" title="（1）远程Jar 包部署"></a>（1）远程Jar 包部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 设置jar 类加载器 否则MapReduce框架找不到Map和Reuce</span><br><span class="line">job.setJarByClass(JobRunner.class);</span><br></pre></td></tr></table></figure>

<ul>
<li>打包</li>
<li>运行 hadoop jar 包的名字  主类名</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@HadoopNode00 ~]# hadoop jar Hadoop_Test-1.0-SNAPSHOT.jar com.baizhi.mr.test01.JobRunner</span><br></pre></td></tr></table></figure>

<h3 id="（2）本地仿真"><a href="#（2）本地仿真" class="headerlink" title="（2）本地仿真"></a>（2）本地仿真</h3><h4 id="填坑"><a href="#填坑" class="headerlink" title="填坑"></a>填坑</h4><p><img src="/2020/02/22/Hadoop/webwxgetmsgimg.jpg" alt="webwxgetmsgimg"></p>
<p><img src="/2020/02/22/Hadoop/1568944749720.png" alt="1568944749720"></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">           <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>log4j.properties</p>
</blockquote>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">### 配置根 ###</span></span><br><span class="line"><span class="meta">log4j.rootLogger</span> = <span class="string">info,console</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 配置输出到控制台 ###</span></span><br><span class="line"><span class="meta">log4j.appender.console</span> = <span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.console.Target</span> = <span class="string">System.out</span></span><br><span class="line"><span class="meta">log4j.appender.console.layout</span> = <span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.console.layout.ConversionPattern</span> =  <span class="string">%p %d&#123;yyyy-MM-dd HH:mm:ss&#125; %c %m%n</span></span><br></pre></td></tr></table></figure>



<h3 id="（3）跨平台提交"><a href="#（3）跨平台提交" class="headerlink" title="（3）跨平台提交"></a>（3）跨平台提交</h3><ul>
<li>需要拷贝相关配置文件到resource目录<ul>
<li>core-site.xml</li>
<li>hdfs-site.xml</li>
<li>yarn-site.xml</li>
<li>mapred-site.xml</li>
</ul>
</li>
</ul>
<p>代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"root"</span>);</span><br><span class="line"></span><br><span class="line">     conf.addResource(<span class="string">"conf2/core-site.xml"</span>);</span><br><span class="line">     conf.addResource(<span class="string">"conf2/hdfs-site.xml"</span>);</span><br><span class="line">     conf.addResource(<span class="string">"conf2/mapred-site.xml"</span>);</span><br><span class="line">     conf.addResource(<span class="string">"conf2/yarn-site.xml"</span>);</span><br><span class="line">     conf.set(MRJobConfig.JAR, <span class="string">"G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar"</span>);</span><br></pre></td></tr></table></figure>

<p>配置跨平台提交</p>
<ul>
<li>配置mapred-site.xml</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.app-submission.cross-platform<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>代码的方式</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.set(<span class="string">"mapreduce.app-submission.cross-platform"</span>, <span class="string">"true"</span>);</span><br></pre></td></tr></table></figure>





<h2 id="4-6-自定义Bean对象"><a href="#4-6-自定义Bean对象" class="headerlink" title="4.6  自定义Bean对象"></a>4.6  自定义Bean对象</h2><h3 id="（1）什么是自定义Bean对象"><a href="#（1）什么是自定义Bean对象" class="headerlink" title="（1）什么是自定义Bean对象"></a>（1）什么是自定义Bean对象</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">开发不是一成不变的，Hadoop中提供了集中数据类型的序列化，但是在实际的开发中往往是不够用的，需要自定义序列化对象</span><br><span class="line">在Java中使用的序列化技术是内置的Serializable</span><br><span class="line">但是Hadoop并没有采取这种序列化方式，使用了自己实现的一套序列化机制，叫做Writable</span><br><span class="line"></span><br><span class="line">需要进行序列化后才能在网络中进行传输</span><br><span class="line">编码（序列化）----解码（反序列化）</span><br></pre></td></tr></table></figure>

<h3 id="（2）需求"><a href="#（2）需求" class="headerlink" title="（2）需求"></a>（2）需求</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">18611781163 700000 10000</span><br><span class="line">18611781161 123 123123  </span><br><span class="line">18611781163 700000 10000</span><br><span class="line">18236529965 123 1223123</span><br><span class="line">18236529964 123123 123</span><br><span class="line">18236529965 546 45645</span><br><span class="line">18611781163 300000 70000</span><br><span class="line">18236529965 123 234523</span><br><span class="line">18236529965 31243 436543</span><br></pre></td></tr></table></figure>

<p>这是一组运营商的流量信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">电话         上行    下行   总流量</span><br><span class="line">18611781163 700000 10000   ？</span><br></pre></td></tr></table></figure>

<h3 id="（3）定义Bean对象"><a href="#（3）定义Bean对象" class="headerlink" title="（3）定义Bean对象"></a>（3）定义Bean对象</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test02;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> sun.rmi.runtime.Log;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String phone;</span><br><span class="line">    <span class="keyword">private</span> Long upFlow;</span><br><span class="line">    <span class="keyword">private</span> Long downFlow;</span><br><span class="line">    <span class="keyword">private</span> Long sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(String phone, Long upFlow, Long downFlow, Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPhone</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPhone</span><span class="params">(String phone)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(Long upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(Long downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span> +</span><br><span class="line">                <span class="string">"phone='"</span> + phone + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">" upFlow="</span> + upFlow +</span><br><span class="line">                <span class="string">" downFlow="</span> + downFlow +</span><br><span class="line">                <span class="string">" sumFlow="</span> + sumFlow ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 序列化 编码</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.phone);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.upFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.downFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 反序列化  解码</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.phone = dataInput.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test02;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] data = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *  phone</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        context.write(<span class="keyword">new</span> Text(data[<span class="number">0</span>]), <span class="keyword">new</span> FlowBean(data[<span class="number">0</span>], Long.valueOf(data[<span class="number">1</span>]), Long.valueOf(data[<span class="number">2</span>]), (Long.valueOf(data[<span class="number">1</span>]) + Long.valueOf(data[<span class="number">2</span>]))));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test02;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 18611781163  FlowBean[]</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">NullWritable</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Long up = <span class="number">0L</span>;</span><br><span class="line">        Long down = <span class="number">0L</span>;</span><br><span class="line">        Long sum = <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FlowBean flowBean : values) &#123;</span><br><span class="line"></span><br><span class="line">            up += flowBean.getUpFlow();</span><br><span class="line">            down += flowBean.getDownFlow();</span><br><span class="line">            sum += flowBean.getSumFlow();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.write(NullWritable.get(), <span class="keyword">new</span> FlowBean(key.toString(), up, down, sum));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test02;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.MRJobConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        System.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"root"</span>);</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        conf.addResource(<span class="string">"conf2/core-site.xml"</span>);</span><br><span class="line">        conf.addResource(<span class="string">"conf2/hdfs-site.xml"</span>);</span><br><span class="line">        conf.addResource(<span class="string">"conf2/mapred-site.xml"</span>);</span><br><span class="line">        conf.addResource(<span class="string">"conf2/yarn-site.xml"</span>);</span><br><span class="line">        conf.set(MRJobConfig.JAR, <span class="string">"G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar"</span>);</span><br><span class="line">        conf.set(<span class="string">"mapreduce.app-submission.cross-platform"</span>, <span class="string">"true"</span>);</span><br><span class="line"></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setJarByClass(FlowRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"/flow.dat"</span>));</span><br><span class="line"></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"/baizhi/out333"</span>));</span><br><span class="line">        job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="4-7-MapReduce-计算流程（重点）"><a href="#4-7-MapReduce-计算流程（重点）" class="headerlink" title="4.7 MapReduce 计算流程（重点）"></a>4.7 MapReduce 计算流程（重点）</h2><p><img src="/2020/02/22/Hadoop/1768269-20190829210253564-211954667.png" alt="8img"></p>
<p>1  首先是通过程序员所编写的MR程序通过命令行本地提交或者IDE远程提交</p>
<p>2    一个MR程序就是一个Job，Job信息会给Resourcemanger，向Resourcemanger注册信息</p>
<p>3  在注册通过后，Job会拷贝相关的资源信息（从HDFS中）</p>
<p>4 紧接着会向Resourcemanger提交完整的Job信息（包括资源信息）</p>
<p>5a  Resourcemanger 会通过提交的Job信息，计算出Job所需的资源，为Job分配Container资源</p>
<p>5b 计算资源会分发给对应的NodeManger，NodeManager会创建一个MRAppMaster</p>
<p>6  MRAppMaster初始化Job</p>
<p>7 <strong>获取输入切片信息</strong></p>
<p>8 MRAppMaster向ResourceManager 请求资源</p>
<p>9a 启动计算资源（连接到对应的资源所在NodeManager）</p>
<p>9b 启动YARN Child</p>
<p>10 从文件系统中获取完整的Job信息</p>
<p>11 启动对应的Maptask或者ReduceTask 进程，执行计算。</p>
<p><img src="/2020/02/22/Hadoop/1568950626404.png" alt="1568950626404"></p>
<h2 id="4-8-Job-提交流程（重点）"><a href="#4-8-Job-提交流程（重点）" class="headerlink" title="4.8 Job 提交流程（重点）"></a>4.8 Job 提交流程（重点）</h2><p><img src="/2020/02/22/Hadoop/1568963134494.png" alt="1568963134494"></p>
<h3 id="（1）建立连接"><a href="#（1）建立连接" class="headerlink" title="（1）建立连接"></a>（1）建立连接</h3><p>判断是在本地运行还是集群运行，分别会创建不同的运行对象  YARN | Local</p>
<h3 id="（2）提交Job"><a href="#（2）提交Job" class="headerlink" title="（2）提交Job"></a>（2）提交Job</h3><h4 id="1）校验空间-checkSpecs"><a href="#1）校验空间-checkSpecs" class="headerlink" title="1）校验空间 checkSpecs()"></a>1）校验空间 checkSpecs()</h4><p><img src="/2020/02/22/Hadoop/1568963471055.png" alt="1568963471055"></p>
<p><img src="/2020/02/22/Hadoop/1568963766792.png" alt="1568963766792"></p>
<h4 id="2）缓存处理"><a href="#2）缓存处理" class="headerlink" title="2）缓存处理"></a>2）缓存处理</h4><p><img src="/2020/02/22/Hadoop/1568964000325.png" alt="1568964000325"></p>
<h4 id="3）创建资源路径-Staging路径"><a href="#3）创建资源路径-Staging路径" class="headerlink" title="3）创建资源路径 Staging路径"></a>3）创建资源路径 Staging路径</h4><p><img src="/2020/02/22/Hadoop/1568964277657.png" alt="1568964277657"></p>
<h4 id="4）获取Job-ID-，在Staging路径下创建Job路径"><a href="#4）获取Job-ID-，在Staging路径下创建Job路径" class="headerlink" title="4）获取Job ID ，在Staging路径下创建Job路径"></a>4）获取Job ID ，在Staging路径下创建Job路径</h4><p><img src="/2020/02/22/Hadoop/1568964431696.png" alt="1568964431696"></p>
<h4 id="5）拷贝相关资源到jobID路径"><a href="#5）拷贝相关资源到jobID路径" class="headerlink" title="5）拷贝相关资源到jobID路径"></a>5）拷贝相关资源到jobID路径</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">files</span><br><span class="line">libjars</span><br><span class="line">archives</span><br><span class="line">jobJar</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/22/Hadoop/1568966050505.png" alt="1568966050505"></p>
<h4 id="6）计算切片-生成切片规划文件"><a href="#6）计算切片-生成切片规划文件" class="headerlink" title="6）计算切片 生成切片规划文件"></a>6）计算切片 生成切片规划文件</h4><blockquote>
<p>切片是一个逻辑上的概念，不会文件进行实际物理拆分，默认切分为128MB（本地为32MB）</p>
</blockquote>
<p><img src="/2020/02/22/Hadoop/1568964863438.png" alt="1568964863438"></p>
<p><img src="/2020/02/22/Hadoop/1568964930859.png" alt="1568964930859"></p>
<h4 id="7）向Staging路径写XML-配置文件"><a href="#7）向Staging路径写XML-配置文件" class="headerlink" title="7）向Staging路径写XML 配置文件"></a>7）向Staging路径写XML 配置文件</h4><p><img src="/2020/02/22/Hadoop/1568965043669.png" alt="1568965043669"></p>
<p><img src="/2020/02/22/Hadoop/1568965159913.png" alt="1568965159913"></p>
<h2 id="4-9-MapReduce-组件解析"><a href="#4-9-MapReduce-组件解析" class="headerlink" title="4 .9 MapReduce 组件解析"></a>4 .9 MapReduce 组件解析</h2><h3 id="（1）概述"><a href="#（1）概述" class="headerlink" title="（1）概述"></a>（1）概述</h3><p>通过WC案例的编写，不难发现，其实我们是按照一定的规则进行程序的输入和输出，将作业放在本地运行或者提交到Hadoop集群中运行。</p>
<p>Hadoop是将数据切分成了若干个输入切片（Input Split），并将每个切片交由一个MapTask的进程处理，MapTask不断从对应的Split中解析出来一个一个的 key、value，并交由map()函数进行处理。处理完成之后根据ReduceTask的个数将结果集分成若干个分片（partition）写到磁盘中。</p>
<p>然后，每个ReduceTask会从MapTask所在的节点上的磁盘读取属于的那个分区（partition），然后使用基于排序方法将key 相同的数据聚合在一起，调用Reduce函数，处理完成后输出到磁盘。</p>
<p>从上面的描述中，可以看出，还有一些组件是没有在（目前的）编程中没有体现到：</p>
<p>（1）指定文本格式。将输入数据切分成若干个切片，且将每个Split（切片）解析成满足map函数要求的keyvalue对。</p>
<p>（2）确定map（）函数产生的keyvalue对象发送给那个Reduce 函数处理</p>
<p>（3）指定输出文件格式，即每个keyvalue已何种形式保存成输出文件。</p>
<p>所以在MR中，这个三个组件分别是InputFormat 、Partitioner、OutputFormat ，他们均需要用户根据自己的需求进行配置，但是对于WC 来说，都是默认的。</p>
<p>但是最终。Hadoop还是提供五个可以编程的组件：分别 Mapper Reducer InputFormat   Partitioner OutputFormat。</p>
<p>按照顺序来：InputFormat     Mapper  Partitioner  Reducer  OutputFormat。</p>
<p>还有不是必备的组件：Canbiner  ，通常是用于优化MR程序的性能，但是不能随意添加。</p>
<h3 id="（2）InputFormat组件"><a href="#（2）InputFormat组件" class="headerlink" title="（2）InputFormat组件"></a>（2）InputFormat组件</h3><p>InputFormat主要用于描述输入数据的格式，它提供了如下的两个功能：</p>
<ul>
<li><p>数据切分：按照某个策略将输入数据切分成若干输入切片，确认MapTask个数和对应的Split</p>
</li>
<li><p>为Mapper提供输入数据：给定某个InputSplit，将其解析成一个一个的key、value</p>
</li>
</ul>
<h4 id="1）什么是切片，如何分割？"><a href="#1）什么是切片，如何分割？" class="headerlink" title="1）什么是切片，如何分割？"></a>1）什么是切片，如何分割？</h4><p><img src="/2020/02/22/Hadoop/1568970205814.png" alt="1568970205814"></p>
<p><code>切片</code>：逻辑上对数据文件进行划分</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">package org.apache.hadoop.mapreduce.lib.input;</span><br><span class="line">-|FileInputFormat</span><br><span class="line">	-|getSplits</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/22/Hadoop/1568971578955.png" alt="1568971578955"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;InputSplit&gt; <span class="title">getSplits</span><span class="params">(JobContext job)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    	<span class="comment">// 秒表 不用关注</span></span><br><span class="line">        Stopwatch sw = (<span class="keyword">new</span> Stopwatch()).start();</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    	<span class="comment">// 获取最小大小</span></span><br><span class="line">        <span class="keyword">long</span> minSize = Math.max(<span class="keyword">this</span>.getFormatMinSplitSize(), getMinSplitSize(job));</span><br><span class="line">    	<span class="comment">// 获取最大大小</span></span><br><span class="line">        <span class="keyword">long</span> maxSize = getMaxSplitSize(job);</span><br><span class="line">    </span><br><span class="line">   		 <span class="comment">// 准备存放InputSplit 的集合</span></span><br><span class="line">        List&lt;InputSplit&gt; splits = <span class="keyword">new</span> ArrayList();</span><br><span class="line">    	 <span class="comment">// 准备存放FileStatus 的集合</span></span><br><span class="line">        List&lt;FileStatus&gt; files = <span class="keyword">this</span>.listStatus(job);</span><br><span class="line">        Iterator i$ = files.iterator();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">            <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">                <span class="keyword">while</span>(i$.hasNext()) &#123;</span><br><span class="line">                    FileStatus file = (FileStatus)i$.next();</span><br><span class="line">                    <span class="comment">//获得当前文件路径</span></span><br><span class="line">                    Path path = file.getPath();</span><br><span class="line">                    <span class="comment">// 获取到当前的长度</span></span><br><span class="line">                    <span class="keyword">long</span> length = file.getLen();</span><br><span class="line">                    <span class="keyword">if</span> (length != <span class="number">0L</span>) &#123;</span><br><span class="line">                        BlockLocation[] blkLocations;</span><br><span class="line">                        <span class="comment">// 判断是否是本地文件还是hdfs文件</span></span><br><span class="line">                        <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span><br><span class="line">                            blkLocations = ((LocatedFileStatus)file).getBlockLocations();</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            FileSystem fs = path.getFileSystem(job.getConfiguration());</span><br><span class="line">                            blkLocations = fs.getFileBlockLocations(file, <span class="number">0L</span>, length);</span><br><span class="line">                        &#125;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// 判断是否可以进行切分</span></span><br><span class="line">                        <span class="comment">// hadoop默认数据都可以进行切割</span></span><br><span class="line">                        <span class="keyword">if</span> (<span class="keyword">this</span>.isSplitable(job, path)) &#123;</span><br><span class="line">                            <span class="comment">// 多的块的大小</span></span><br><span class="line">                            <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">                            <span class="comment">// 计算切片大小</span></span><br><span class="line">                            <span class="keyword">long</span> splitSize = <span class="keyword">this</span>.computeSplitSize(blockSize, minSize, maxSize);</span><br><span class="line">							<span class="comment">// 准备描述剩余数据的字段</span></span><br><span class="line">                            <span class="keyword">long</span> bytesRemaining;</span><br><span class="line">                            <span class="keyword">int</span> blkIndex;</span><br><span class="line">                            <span class="keyword">for</span>(bytesRemaining = length; (<span class="keyword">double</span>)bytesRemaining / (<span class="keyword">double</span>)splitSize &gt; <span class="number">1.1</span>D; bytesRemaining -= splitSize) &#123;</span><br><span class="line">                                blkIndex = <span class="keyword">this</span>.getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">                                splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, splitSize, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">                            &#125;</span><br><span class="line"></span><br><span class="line">                            <span class="keyword">if</span> (bytesRemaining != <span class="number">0L</span>) &#123;</span><br><span class="line">                                blkIndex = <span class="keyword">this</span>.getBlockIndex(blkLocations, length - bytesRemaining);</span><br><span class="line">                                splits.add(<span class="keyword">this</span>.makeSplit(path, length - bytesRemaining, bytesRemaining, blkLocations[blkIndex].getHosts(), blkLocations[blkIndex].getCachedHosts()));</span><br><span class="line">                            &#125;</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            splits.add(<span class="keyword">this</span>.makeSplit(path, <span class="number">0L</span>, length, blkLocations[<span class="number">0</span>].getHosts(), blkLocations[<span class="number">0</span>].getCachedHosts()));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        splits.add(<span class="keyword">this</span>.makeSplit(path, <span class="number">0L</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                job.getConfiguration().setLong(<span class="string">"mapreduce.input.fileinputformat.numinputfiles"</span>, (<span class="keyword">long</span>)files.size());</span><br><span class="line">                sw.stop();</span><br><span class="line">                <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">                    LOG.debug(<span class="string">"Total # of splits generated by getSplits: "</span> + splits.size() + <span class="string">", TimeTaken: "</span> + sw.elapsedMillis());</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">return</span> splits;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>





<h4 id="2）如何为Mapper提供数据？"><a href="#2）如何为Mapper提供数据？" class="headerlink" title="2）如何为Mapper提供数据？"></a>2）如何为Mapper提供数据？</h4><p>TextInpuFormat使用的是org.apache.hadoop.mapreduce.lib.input.LineRecordReader . 这个类中，首先是initialize（）方法，该方法主要是获取切片信息初始化位置和结束位置，以及输入流；</p>
<p> Mapper的key、value是通过nextKeyValue（）判断是否还有下一个，在这个方法中可以被设置成了文件的偏移量，value通过LineReader.readLine()方法将每一行的值拿到</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit genericSplit, TaskAttemptContext context)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        FileSplit split = (FileSplit)genericSplit;</span><br><span class="line">        Configuration job = context.getConfiguration();</span><br><span class="line">        <span class="keyword">this</span>.maxLineLength = job.getInt(<span class="string">"mapreduce.input.linerecordreader.line.maxlength"</span>, <span class="number">2147483647</span>);</span><br><span class="line">        <span class="keyword">this</span>.start = split.getStart();</span><br><span class="line">        <span class="keyword">this</span>.end = <span class="keyword">this</span>.start + split.getLength();</span><br><span class="line">        Path file = split.getPath();</span><br><span class="line">        FileSystem fs = file.getFileSystem(job);</span><br><span class="line">        <span class="keyword">this</span>.fileIn = fs.open(file);</span><br><span class="line">        CompressionCodec codec = (<span class="keyword">new</span> CompressionCodecFactory(job)).getCodec(file);</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> != codec) &#123;</span><br><span class="line">            <span class="keyword">this</span>.isCompressedInput = <span class="keyword">true</span>;</span><br><span class="line">            <span class="keyword">this</span>.decompressor = CodecPool.getDecompressor(codec);</span><br><span class="line">            <span class="keyword">if</span> (codec <span class="keyword">instanceof</span> SplittableCompressionCodec) &#123;</span><br><span class="line">                SplitCompressionInputStream cIn = ((SplittableCompressionCodec)codec).createInputStream(<span class="keyword">this</span>.fileIn, <span class="keyword">this</span>.decompressor, <span class="keyword">this</span>.start, <span class="keyword">this</span>.end, READ_MODE.BYBLOCK);</span><br><span class="line">                <span class="keyword">this</span>.in = <span class="keyword">new</span> CompressedSplitLineReader(cIn, job, <span class="keyword">this</span>.recordDelimiterBytes);</span><br><span class="line">                <span class="keyword">this</span>.start = cIn.getAdjustedStart();</span><br><span class="line">                <span class="keyword">this</span>.end = cIn.getAdjustedEnd();</span><br><span class="line">                <span class="keyword">this</span>.filePosition = cIn;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">this</span>.in = <span class="keyword">new</span> SplitLineReader(codec.createInputStream(<span class="keyword">this</span>.fileIn, <span class="keyword">this</span>.decompressor), job, <span class="keyword">this</span>.recordDelimiterBytes);</span><br><span class="line">                <span class="keyword">this</span>.filePosition = <span class="keyword">this</span>.fileIn;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">this</span>.fileIn.seek(<span class="keyword">this</span>.start);</span><br><span class="line">            <span class="keyword">this</span>.in = <span class="keyword">new</span> SplitLineReader(<span class="keyword">this</span>.fileIn, job, <span class="keyword">this</span>.recordDelimiterBytes);</span><br><span class="line">            <span class="keyword">this</span>.filePosition = <span class="keyword">this</span>.fileIn;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">this</span>.start != <span class="number">0L</span>) &#123;</span><br><span class="line">            <span class="keyword">this</span>.start += (<span class="keyword">long</span>)<span class="keyword">this</span>.in.readLine(<span class="keyword">new</span> Text(), <span class="number">0</span>, <span class="keyword">this</span>.maxBytesToConsume(<span class="keyword">this</span>.start));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.pos = <span class="keyword">this</span>.start;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">       <span class="keyword">if</span> (<span class="keyword">this</span>.key == <span class="keyword">null</span>) &#123;</span><br><span class="line">           <span class="keyword">this</span>.key = <span class="keyword">new</span> LongWritable();</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">this</span>.key.set(<span class="keyword">this</span>.pos);</span><br><span class="line">       <span class="keyword">if</span> (<span class="keyword">this</span>.value == <span class="keyword">null</span>) &#123;</span><br><span class="line">           <span class="keyword">this</span>.value = <span class="keyword">new</span> Text();</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">int</span> newSize = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">while</span>(<span class="keyword">this</span>.getFilePosition() &lt;= <span class="keyword">this</span>.end || <span class="keyword">this</span>.in.needAdditionalRecordAfterSplit()) &#123;</span><br><span class="line">           <span class="keyword">if</span> (<span class="keyword">this</span>.pos == <span class="number">0L</span>) &#123;</span><br><span class="line">               newSize = <span class="keyword">this</span>.skipUtfByteOrderMark();</span><br><span class="line">           &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">               newSize = <span class="keyword">this</span>.in.readLine(<span class="keyword">this</span>.value, <span class="keyword">this</span>.maxLineLength, <span class="keyword">this</span>.maxBytesToConsume(<span class="keyword">this</span>.pos));</span><br><span class="line">               <span class="keyword">this</span>.pos += (<span class="keyword">long</span>)newSize;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           <span class="keyword">if</span> (newSize == <span class="number">0</span> || newSize &lt; <span class="keyword">this</span>.maxLineLength) &#123;</span><br><span class="line">               <span class="keyword">break</span>;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           LOG.info(<span class="string">"Skipped line of size "</span> + newSize + <span class="string">" at pos "</span> + (<span class="keyword">this</span>.pos - (<span class="keyword">long</span>)newSize));</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> (newSize == <span class="number">0</span>) &#123;</span><br><span class="line">           <span class="keyword">this</span>.key = <span class="keyword">null</span>;</span><br><span class="line">           <span class="keyword">this</span>.value = <span class="keyword">null</span>;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">       &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">           <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">readLine</span><span class="params">(Text str, <span class="keyword">int</span> maxLineLength, <span class="keyword">int</span> maxBytesToConsume)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.recordDelimiterBytes != <span class="keyword">null</span> ? <span class="keyword">this</span>.readCustomLine(str, maxLineLength, maxBytesToConsume) : <span class="keyword">this</span>.readDefaultLine(str, maxLineLength, maxBytesToConsume);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">readDefaultLine</span><span class="params">(Text str, <span class="keyword">int</span> maxLineLength, <span class="keyword">int</span> maxBytesToConsume)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        str.clear();</span><br><span class="line">        <span class="keyword">int</span> txtLength = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> newlineLength = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">boolean</span> prevCharCR = <span class="keyword">false</span>;</span><br><span class="line">        <span class="keyword">long</span> bytesConsumed = <span class="number">0L</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">do</span> &#123;</span><br><span class="line">            <span class="keyword">int</span> startPosn = <span class="keyword">this</span>.bufferPosn;</span><br><span class="line">            <span class="keyword">if</span> (<span class="keyword">this</span>.bufferPosn &gt;= <span class="keyword">this</span>.bufferLength) &#123;</span><br><span class="line">                startPosn = <span class="keyword">this</span>.bufferPosn = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">if</span> (prevCharCR) &#123;</span><br><span class="line">                    ++bytesConsumed;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">this</span>.bufferLength = <span class="keyword">this</span>.fillBuffer(<span class="keyword">this</span>.in, <span class="keyword">this</span>.buffer, prevCharCR);</span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>.bufferLength &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span>(<span class="keyword">this</span>.bufferPosn &lt; <span class="keyword">this</span>.bufferLength) &#123;</span><br><span class="line">                <span class="keyword">if</span> (<span class="keyword">this</span>.buffer[<span class="keyword">this</span>.bufferPosn] == <span class="number">10</span>) &#123;</span><br><span class="line">                    newlineLength = prevCharCR ? <span class="number">2</span> : <span class="number">1</span>;</span><br><span class="line">                    ++<span class="keyword">this</span>.bufferPosn;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (prevCharCR) &#123;</span><br><span class="line">                    newlineLength = <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                prevCharCR = <span class="keyword">this</span>.buffer[<span class="keyword">this</span>.bufferPosn] == <span class="number">13</span>;</span><br><span class="line">                ++<span class="keyword">this</span>.bufferPosn;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> readLength = <span class="keyword">this</span>.bufferPosn - startPosn;</span><br><span class="line">            <span class="keyword">if</span> (prevCharCR &amp;&amp; newlineLength == <span class="number">0</span>) &#123;</span><br><span class="line">                --readLength;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            bytesConsumed += (<span class="keyword">long</span>)readLength;</span><br><span class="line">            <span class="keyword">int</span> appendLength = readLength - newlineLength;</span><br><span class="line">            <span class="keyword">if</span> (appendLength &gt; maxLineLength - txtLength) &#123;</span><br><span class="line">                appendLength = maxLineLength - txtLength;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (appendLength &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                str.append(<span class="keyword">this</span>.buffer, startPosn, appendLength);</span><br><span class="line">                txtLength += appendLength;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">while</span>(newlineLength == <span class="number">0</span> &amp;&amp; bytesConsumed &lt; (<span class="keyword">long</span>)maxBytesToConsume);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (bytesConsumed &gt; <span class="number">2147483647L</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Too many bytes before newline: "</span> + bytesConsumed);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> (<span class="keyword">int</span>)bytesConsumed;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>



<h3 id="（3）切片MapTask的关系"><a href="#（3）切片MapTask的关系" class="headerlink" title="（3）切片MapTask的关系"></a>（3）切片MapTask的关系</h3><p>MapTask 的并发数量与切片相关（决定），ReduceTask数量是可以手动设置的，默认为1</p>
<h3 id="（4）常用的InputFormat"><a href="#（4）常用的InputFormat" class="headerlink" title="（4）常用的InputFormat"></a>（4）常用的InputFormat</h3><h4 id="1）分类"><a href="#1）分类" class="headerlink" title="1）分类"></a>1）分类</h4><ul>
<li><p>FileInputFormat</p>
<ul>
<li><p>TextInputFormat</p>
<ul>
<li>key LongWriteable 行的字节偏移量</li>
<li>value Text 文本</li>
</ul>
<blockquote>
<p>切片：以文件为切分单位，有多少个文件就至少有多少个切片</p>
</blockquote>
</li>
<li><p>NLineInputFormat</p>
<ul>
<li>key LongWriteable 行的字节偏移量</li>
<li>value Text 文本</li>
</ul>
<blockquote>
<p>切片：n行为一个切片，默认1行为一个切片，可以设置</p>
<p>conf.set(“mapreduce.input.lineinputformat.linespermap”,”10”)</p>
<p>NLineInputFormat.setNumLinesPerSplit();</p>
</blockquote>
</li>
<li><p>CombineTextInputFormat</p>
</li>
<li><p>key LongWriteable 行的字节偏移量</p>
<ul>
<li>value Text 文本</li>
</ul>
<blockquote>
<p>切片：按照SplitSize切分，一个切片可能对应多个Block块</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CombineTextInputFormat.setMaxInputSplitSize();</span><br></pre></td></tr></table></figure>
<p>CombineTextInputFormat.setMinInputSplitSize();</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
</blockquote>
</li>
<li><p>SequenceFileInputFormat</p>
<ul>
<li>key 文件名</li>
<li>value 文件数据</li>
</ul>
</li>
</ul>
</li>
<li><p>DBInputFormat（数据库）</p>
</li>
<li><p>TableInputFormat（HBase）</p>
</li>
</ul>
<h4 id="2）NLineInputFormat"><a href="#2）NLineInputFormat" class="headerlink" title="2）NLineInputFormat"></a>2）NLineInputFormat</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.MRJobConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">/*System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");*/</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取Job对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// // 设置jar 类加载器 否则MapReduce框架找不到Map和Reuce</span></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">      <span class="comment">/*  CombineFileInputFormat.setMaxInputSplitSize();</span></span><br><span class="line"><span class="comment">        CombineFileInputFormat.setMinInputSplitSize();*/</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置数据输入输出组件</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setInputFormatClass(NLineInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *设置数据输入输出路径</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        NLineInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\data02"</span>));</span><br><span class="line">        NLineInputFormat.setNumLinesPerSplit(job,<span class="number">3</span>);</span><br><span class="line">        <span class="comment">//TextInputFormat.setInputPaths(job, new Path("/wordcount1.txt"));</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 注意： 此输出路径不能存在</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="comment">//TextOutputFormat.setOutputPath(job, new Path("/baizhi/out8121231233"));</span></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out12"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置MAP 和 REDUCE 处理逻辑</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapperClass(WCMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(WCReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置 map任务和reduce任务的输出泛型</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//  提交</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.submit();</span></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.Serializable;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">* keyIn  LongWritable (Long) 输入文本字节偏移量</span></span><br><span class="line"><span class="comment">* valueIn Text (String)      输入文本行</span></span><br><span class="line"><span class="comment">*  keyOut Text(String)</span></span><br><span class="line"><span class="comment">*  valueOut IntWritable(Int)</span></span><br><span class="line"><span class="comment">* */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCMapper</span>  <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        String[] names = value.toString().split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String name : names) &#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(name),<span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *keyIn Text 与mapper的keyOut的数据类型相对应</span></span><br><span class="line"><span class="comment"> *valeuIn IntWritable   与mapper的ValueOut的数据类型相对应</span></span><br><span class="line"><span class="comment"> * KeyOut</span></span><br><span class="line"><span class="comment"> * valueOut</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WCReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">IntWritable</span>, <span class="title">Text</span>, <span class="title">IntWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;IntWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (IntWritable value : values) &#123;</span><br><span class="line">            sum += value.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="3）CombineTextInputFormat"><a href="#3）CombineTextInputFormat" class="headerlink" title="3）CombineTextInputFormat"></a>3）CombineTextInputFormat</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.mr.test01;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.MRJobConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineFileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.NLineInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">/*System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");*/</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取Job对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// // 设置jar 类加载器 否则MapReduce框架找不到Map和Reuce</span></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置数据输入输出组件</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setInputFormatClass(CombineTextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *设置数据输入输出路径</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        CombineTextInputFormat.setMinInputSplitSize(job, <span class="number">1048576</span>);</span><br><span class="line">        CombineTextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\data"</span>));</span><br><span class="line">        <span class="comment">//NLineInputFormat.setInputPaths(job, new Path("G:\\Note\\Day02-Hadoop\\数据文件\\data02"));</span></span><br><span class="line">        <span class="comment">// NLineInputFormat.setNumLinesPerSplit(job,3);</span></span><br><span class="line">        <span class="comment">//TextInputFormat.setInputPaths(job, new Path("/wordcount1.txt"));</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 注意： 此输出路径不能存在</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="comment">//TextOutputFormat.setOutputPath(job, new Path("/baizhi/out8121231233"));</span></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out111122"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置MAP 和 REDUCE 处理逻辑</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapperClass(WCMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(WCReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置 map任务和reduce任务的输出泛型</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(IntWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//  提交</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.submit();</span></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="4）DBInputFormat"><a href="#4）DBInputFormat" class="headerlink" title="4）DBInputFormat"></a>4）DBInputFormat</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.DBInputFormat;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.db.DBConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.db.DBInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        DBConfiguration.configureDB(conf, <span class="string">"com.mysql.jdbc.Driver"</span>, <span class="string">"jdbc:mysql://hadoopnode00:3306/hadoop"</span>, <span class="string">"root"</span>, <span class="string">"1234"</span>);</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(DBInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        DBInputFormat.setInput(job, User.class, "select id,name from user", "select count(1) from user");</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\src\\main\\java\\com\\baizhi\\DBInputFormat\\out1"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setMapperClass(DBMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(DBReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(LongWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.DBInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DBMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">User</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, User value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        context.write(key, <span class="keyword">new</span> Text(value.toString()));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.DBInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DBReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(LongWritable key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            context.write(NullWritable.get(), value);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.DBInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.lib.db.DBWritable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.sql.ResultSet;</span><br><span class="line"><span class="keyword">import</span> java.sql.SQLException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span> <span class="keyword">implements</span> <span class="title">Writable</span>, <span class="title">DBWritable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> id;</span><br><span class="line"></span><br><span class="line">    String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">User</span><span class="params">(<span class="keyword">int</span> id, String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setId</span><span class="params">(<span class="keyword">int</span> id)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setName</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"User&#123;"</span> +</span><br><span class="line">                <span class="string">"id="</span> + id +</span><br><span class="line">                <span class="string">", name='"</span> + name + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        dataOutput.writeInt(<span class="keyword">this</span>.id);</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.name);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.id = dataInput.readInt();</span><br><span class="line">        <span class="keyword">this</span>.name = dataInput.readUTF();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(PreparedStatement preparedStatement)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line">        preparedStatement.setInt(<span class="number">1</span>, <span class="keyword">this</span>.id);</span><br><span class="line">        preparedStatement.setString(<span class="number">2</span>, <span class="keyword">this</span>.name);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(ResultSet resultSet)</span> <span class="keyword">throws</span> SQLException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.id = resultSet.getInt(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">this</span>.name = resultSet.getString(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<ul>
<li><p>本地运行</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.38<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>JAR 包部署</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">需要在hadoopNode00中添加MySQL的环境</span><br><span class="line">将mysql的jar包放入&#x2F;home&#x2F;hadoop&#x2F;hadoop-2.6.0&#x2F;share&#x2F;hadoop&#x2F;yarn&#x2F; 中即可</span><br></pre></td></tr></table></figure>



<ul>
<li>远程提交</li>
</ul>
<blockquote>
<p>加上相应的配置属性即可</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;root&quot;);</span><br><span class="line">     conf.addResource(&quot;conf2&#x2F;core-site.xml&quot;);</span><br><span class="line">     conf.addResource(&quot;conf2&#x2F;hdfs-site.xml&quot;);</span><br><span class="line">     conf.addResource(&quot;conf2&#x2F;mapred-site.xml&quot;);</span><br><span class="line">     conf.addResource(&quot;conf2&#x2F;yarn-site.xml&quot;);</span><br><span class="line">     conf.set(MRJobConfig.JAR, &quot;G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar&quot;);</span><br><span class="line">     conf.set(&quot;mapreduce.app-submission.cross-platform&quot;, &quot;true&quot;);</span><br></pre></td></tr></table></figure>



<h4 id="5）-自定义InputFormat"><a href="#5）-自定义InputFormat" class="headerlink" title="5） 自定义InputFormat"></a>5） 自定义InputFormat</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">解决小文件存储问题，将多个小文件存放在一个SequenceFile（SequenceFile文件是Hadoop用来存储二进制文件形式的key-value的文件格式），SequenceFile，存储的形式为文件的路径名称为key，文件的内容为value</span><br></pre></td></tr></table></figure>



<p><img src="/2020/02/22/Hadoop/1569136700216.png" alt="1569136700216"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.OutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.baizhi.mr.test01.WCMapper;</span><br><span class="line"><span class="keyword">import</span> com.baizhi.mr.test01.WCReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">/*System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");*/</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取Job对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// // 设置jar 类加载器 否则MapReduce框架找不到Map和Reuce</span></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置数据输入输出组件</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setInputFormatClass(OwnInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(SequenceFileOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *设置数据输入输出路径</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//CombineTextInputFormat.setMinInputSplitSize(job, 1048576);</span></span><br><span class="line">        OwnInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\data"</span>));</span><br><span class="line">        <span class="comment">//NLineInputFormat.setInputPaths(job, new Path("G:\\Note\\Day02-Hadoop\\数据文件\\data02"));</span></span><br><span class="line">        <span class="comment">// NLineInputFormat.setNumLinesPerSplit(job,3);</span></span><br><span class="line">        <span class="comment">//TextInputFormat.setInputPaths(job, new Path("/wordcount1.txt"));</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 注意： 此输出路径不能存在</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="comment">//TextOutputFormat.setOutputPath(job, new Path("/baizhi/out8121231233"));</span></span><br><span class="line">        SequenceFileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out12313"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置MAP 和 REDUCE 处理逻辑</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapperClass(FileMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FileReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置 map任务和reduce任务的输出泛型</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(BytesWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//  提交</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.submit();</span></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.OutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(Text key, BytesWritable value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        context.write(key, value);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.OutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>, <span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;BytesWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (BytesWritable value : values) &#123;</span><br><span class="line"></span><br><span class="line">            context.write(key, value);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.OutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.ByteWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.JobContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnInputFormat</span> <span class="keyword">extends</span> <span class="title">FileInputFormat</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordReader&lt;Text, BytesWritable&gt; <span class="title">createRecordReader</span><span class="params">(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        OwnRecordReader ownRecordReader = <span class="keyword">new</span> OwnRecordReader();</span><br><span class="line">        ownRecordReader.initialize(inputSplit, taskAttemptContext);</span><br><span class="line">        <span class="keyword">return</span> ownRecordReader;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">boolean</span> <span class="title">isSplitable</span><span class="params">(JobContext context, Path filename)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.OutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataInputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.BytesWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.InputSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordReader;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnRecordReader</span> <span class="keyword">extends</span> <span class="title">RecordReader</span>&lt;<span class="title">Text</span>, <span class="title">BytesWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    FileSplit fileSplit;</span><br><span class="line">    Configuration conf;</span><br><span class="line">    BytesWritable value = <span class="keyword">new</span> BytesWritable();</span><br><span class="line">    Text key = <span class="keyword">new</span> Text();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">boolean</span> isProgress = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">initialize</span><span class="params">(InputSplit inputSplit, TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        fileSplit = (FileSplit) inputSplit;</span><br><span class="line"></span><br><span class="line">        conf = taskAttemptContext.getConfiguration();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">nextKeyValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (isProgress) &#123;</span><br><span class="line">            <span class="keyword">byte</span>[] bytes = <span class="keyword">new</span> <span class="keyword">byte</span>[(<span class="keyword">int</span>) fileSplit.getLength()];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment">//获取fs 对象</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             * 当前文件的路径</span></span><br><span class="line"><span class="comment">             * */</span></span><br><span class="line">            Path path = fileSplit.getPath();</span><br><span class="line"></span><br><span class="line">            FileSystem fileSystem = path.getFileSystem(conf);</span><br><span class="line"></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             * 获取到文件的数据流</span></span><br><span class="line"><span class="comment">             * */</span></span><br><span class="line">            FSDataInputStream inputStream = fileSystem.open(path);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            IOUtils.readFully(inputStream, bytes, <span class="number">0</span>, bytes.length);</span><br><span class="line"></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             * 封装value</span></span><br><span class="line"><span class="comment">             * */</span></span><br><span class="line">            value.set(bytes, <span class="number">0</span>, bytes.length);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            key.set(path.toString());</span><br><span class="line"></span><br><span class="line">            IOUtils.closeStream(inputStream);</span><br><span class="line"></span><br><span class="line">            isProgress = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Text <span class="title">getCurrentKey</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.key;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> BytesWritable <span class="title">getCurrentValue</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.value;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="（5）Partitioner-组件"><a href="#（5）Partitioner-组件" class="headerlink" title="（5）Partitioner 组件"></a>（5）Partitioner 组件</h3><p><img src="/2020/02/22/Hadoop/1569138853383.png" alt="1569138853383"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">将不同地区的数据输出到不同的文件中</span><br><span class="line">18611781163 700000 10000 hn</span><br><span class="line">18611781161 123 123123 bj  </span><br><span class="line">18611781163 700000 10000 hn</span><br><span class="line">18236529965 123 1223123 tj</span><br><span class="line">18236529964 123123 123 hb</span><br><span class="line">18236529965 546 45645 tj</span><br><span class="line">18611781163 300000 70000 hn</span><br><span class="line">18236529965 123 234523 tj</span><br><span class="line">18236529965 31243 436543 tj</span><br></pre></td></tr></table></figure>

<p><img src="/2020/02/22/Hadoop/1569140142429.png" alt="1569140142429"></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnPartitioner</span>&lt;<span class="title">KEY</span>, <span class="title">VALUE</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">KEY</span>, <span class="title">VALUE</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> HashMap&lt;String, Integer&gt; areaMap = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        areaMap.put(<span class="string">"hn"</span>, <span class="number">0</span>);</span><br><span class="line">        areaMap.put(<span class="string">"henna"</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        areaMap.put(<span class="string">"bj"</span>, <span class="number">1</span>);</span><br><span class="line">        areaMap.put(<span class="string">"tj"</span>, <span class="number">2</span>);</span><br><span class="line">        areaMap.put(<span class="string">"hb"</span>, <span class="number">3</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(KEY key, VALUE value, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> areaMap.get(key.toString()) == <span class="keyword">null</span> ? <span class="number">5</span> : areaMap.get(key.toString());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">Writable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String phone;</span><br><span class="line">    <span class="keyword">private</span> Long upFlow;</span><br><span class="line">    <span class="keyword">private</span> Long downFlow;</span><br><span class="line">    <span class="keyword">private</span> Long sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(String phone, Long upFlow, Long downFlow, Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPhone</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPhone</span><span class="params">(String phone)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(Long upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(Long downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span> +</span><br><span class="line">                <span class="string">"phone='"</span> + phone + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">" upFlow="</span> + upFlow +</span><br><span class="line">                <span class="string">" downFlow="</span> + downFlow +</span><br><span class="line">                <span class="string">" sumFlow="</span> + sumFlow ;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 序列化 编码</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.phone);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.upFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.downFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 反序列化  解码</span></span><br><span class="line"><span class="comment">     *f */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.phone = dataInput.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] data = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *  phone</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        context.write(<span class="keyword">new</span> Text(data[<span class="number">3</span>]), <span class="keyword">new</span> FlowBean(data[<span class="number">0</span>], Long.valueOf(data[<span class="number">1</span>]), Long.valueOf(data[<span class="number">2</span>]), (Long.valueOf(data[<span class="number">1</span>]) + Long.valueOf(data[<span class="number">2</span>]))));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 18611781163  FlowBean[]</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;FlowBean&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        Long up = <span class="number">0L</span>;</span><br><span class="line">        Long down = <span class="number">0L</span>;</span><br><span class="line">        Long sum = <span class="number">0L</span>;</span><br><span class="line">        String phone = <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (FlowBean flowBean : values) &#123;</span><br><span class="line"></span><br><span class="line">            up += flowBean.getUpFlow();</span><br><span class="line">            down += flowBean.getDownFlow();</span><br><span class="line">            sum += flowBean.getSumFlow();</span><br><span class="line">            phone = flowBean.getPhone();</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.write(key, <span class="keyword">new</span> FlowBean(phone, up, down, sum));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.MRJobConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">       <span class="comment">/* conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setJarByClass(FlowRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setPartitionerClass(OwnPartitioner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\flow02.dat"</span>));</span><br><span class="line"></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out131"</span>));</span><br><span class="line">        job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="（6）OutputFormat"><a href="#（6）OutputFormat" class="headerlink" title="（6）OutputFormat"></a>（6）OutputFormat</h3><p>自定义输出</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.outformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        context.write(NullWritable.get(), value);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.outformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FileReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(NullWritable key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line">            context.write(NullWritable.get(), value);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.outformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.baizhi.mr.test01.WCMapper;</span><br><span class="line"><span class="keyword">import</span> com.baizhi.mr.test01.WCReducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取配置对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="comment">/*System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");*/</span></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 获取Job对象</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// // 设置jar 类加载器 否则MapReduce框架找不到Map和Reuce</span></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置数据输入输出组件</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(OwnOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *设置数据输入输出路径</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//CombineTextInputFormat.setMinInputSplitSize(job, 1048576);</span></span><br><span class="line">        <span class="comment">//CombineTextInputFormat.setInputPaths(job, new Path(" v  "));</span></span><br><span class="line">        <span class="comment">//NLineInputFormat.setInputPaths(job, new Path("G:\\Note\\Day02-Hadoop\\数据文件\\data02"));</span></span><br><span class="line">        <span class="comment">// NLineInputFormat.setNumLinesPerSplit(job,3);</span></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\flow.dat"</span>));</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 注意： 此输出路径不能存在</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="comment">//TextOutputFormat.setOutputPath(job, new Path("/baizhi/out8121231233"));</span></span><br><span class="line">        OwnOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\outbaizhi"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置MAP 和 REDUCE 处理逻辑</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapperClass(FileMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FileReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 设置 map任务和reduce任务的输出泛型</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        job.setMapOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//  提交</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//job.submit();</span></span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.outformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnOutputFormat</span> <span class="keyword">extends</span> <span class="title">FileOutputFormat</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> RecordWriter&lt;NullWritable, Text&gt; <span class="title">getRecordWriter</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> OwnRecordWriter(taskAttemptContext);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.outformat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FSDataOutputStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IOUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.RecordWriter;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.TaskAttemptContext;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnRecordWriter</span> <span class="keyword">extends</span> <span class="title">RecordWriter</span>&lt;<span class="title">NullWritable</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line">    FileSystem fileSystem;</span><br><span class="line"></span><br><span class="line">    FSDataOutputStream outputStream;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OwnRecordWriter</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        fileSystem = FileSystem.get(taskAttemptContext.getConfiguration());</span><br><span class="line"></span><br><span class="line">        outputStream = fileSystem.create(<span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\testoutputforamt.txt"</span>));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(NullWritable nullWritable, Text text)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        outputStream.write(text.getBytes());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(TaskAttemptContext taskAttemptContext)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        IOUtils.closeStream(outputStream);</span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="（7）Combiner-组件"><a href="#（7）Combiner-组件" class="headerlink" title="（7）Combiner 组件"></a>（7）Combiner 组件</h3><ul>
<li>Conbiner是MR程序中Mapper和Reducer之外的一种组件</li>
<li>Combiner的组件的父类就是Reducer</li>
<li>Combiner和Reucer的区别在于运行的位置</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Combiner是在每一个MapTask的节点上运行  （局部汇总）</span><br><span class="line">Reducer是接收全局的所有的Mapper结果再进行处理 （全局汇总）</span><br></pre></td></tr></table></figure>

<ul>
<li>Combiner的意义就是对于每一个MapTask的输出进行局部汇总，减少网络传输量</li>
<li>Combiner能够运用的前提是不能影响最终业务结果（累加操作不会影响）而且 Combiner的输出KV 应该能跟Reducer的KV相对应</li>
</ul>
<h4 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h4><p>Combiner并不是适用于所有的场景</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. 并不是所有场景都可以使用Combiner，必须满足结果可以累加</span><br><span class="line">2. 适合求和，但不适合求平均数   Avg（0,20,10,25,15）&#x3D;14 | avg（0,20，10）&#x3D;10  avg（25,15）&#x3D;20   avg（10,20）&#x3D;15，通过上述案例可以发现显然这里不适合使用Combiner</span><br></pre></td></tr></table></figure>

<h4 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h4><ul>
<li>新建CombinerClass继承Reducer ，job.setCombinerClass();</li>
<li>直接使用ReducerClass 作为CombinerClass</li>
</ul>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 核心代码  注意必须满足累加</span></span><br><span class="line">job.setCombinerClass(WCReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>没有使用Combiner</li>
</ul>
<p><img src="/2020/02/22/Hadoop/1569206721162.png" alt="1569206721162"></p>
<ul>
<li>使用Combiner</li>
</ul>
<p><img src="/2020/02/22/Hadoop/1569206773669.png" alt="1569206773669"></p>
<h2 id="4-10-MR-过程"><a href="#4-10-MR-过程" class="headerlink" title="4.10 MR 过程"></a>4.10 MR 过程</h2><p><img src="/2020/02/22/Hadoop/1569208873544.png" alt="1569208873544"></p>
<p><img src="/2020/02/22/Hadoop/1558528156916.png" alt="1558528156916"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">MR框架是使用InputFormat为map所需的数据进行预处理，并为其提供数据。两个功能：切片，封装keyvalue</span><br><span class="line">因为InputSplit为逻辑切分而非物理拆分，所以说还需要RecoderReader根据InputSplit中的信息里处理InputSplit中的具体信息，加载数据并转换为合适的Map任务的keyvalue，输入给Map任务</span><br><span class="line"></span><br><span class="line">Map是自定义的逻辑，根据InputFormat给定的相应数据结合场景进行相应的处理</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">为了让Reducer可以并处理Map的处理结果，需要对map的输出结果进行一定的分区（Partition）、排序（Sort）、合并（Combine）、归并（Merge）等操作，得到keyvalue形式的中间结果，再交给Reducer处理，这个过程就是Shuffle，从无序的keyvalue到有序有分区的keyvalue，这个过程称之为Shuffle很形象。</span><br><span class="line"></span><br><span class="line">Reducer是自定义的逻辑，根据从不同的MapTask 节点拿过来的给定的相应数据结合场景进行相应的处理</span><br><span class="line"></span><br><span class="line">OutputFormat进行输出，输出至分布式文件系统</span><br></pre></td></tr></table></figure>

<h2 id="4-11-Shuffle"><a href="#4-11-Shuffle" class="headerlink" title="4.11 Shuffle"></a>4.11 Shuffle</h2><p>Shuffle过程是MapReducer的核心，描述这数据从map task输出到reduce task的过程。</p>
<p>Hadoop的集群环境，大部分的map task和reduce task 是执行在不同的节点上的，那么reduce就要取得map的输出结果，一搬就需要在不同的节点上去拉取；那么集群中运行的很多个Job，task的执行会对集群中网络资源消耗严重，虽说这种消耗是正常的，不可便面的，但是可以采取措施减少不必要的网络消耗，另一方面，每个节点内部，向对比于内存，磁盘IO对Job的完成时间影响较大。</p>
<p>所以说：从以上进行分析，shuffle的过程基本要求：</p>
<ul>
<li>完整的从map task 端拉取数据到reduce task端</li>
<li>在拉取数据的过程中，尽可能减少网络消耗 </li>
<li>尽可能的减少磁盘IO 对task执行效率的影响</li>
</ul>
<p>shuffle过程</p>
<h3 id="（1）Map端的shuffle"><a href="#（1）Map端的shuffle" class="headerlink" title="（1）Map端的shuffle"></a>（1）Map端的shuffle</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.mapred.MapTask</span><br><span class="line">-|MapOutputBuffer</span><br><span class="line"> -|init()</span><br></pre></td></tr></table></figure>



<p>Map的输出结果首先被缓存到内存，当缓存区（环状缓冲区）达到80% （默认大小为100MB），就会启动溢写操作，当前启动溢写操作时，首先把缓存中的数据进行分区，对每个分区的数据进行排序和合并。之后再写入到磁盘中，每次溢写 都会生成新的磁盘文件，随着Job执行，被溢写出到磁盘的文件会越来越多，在Map任务全部结束之前，这些溢写文件会被归并成一个大的磁盘文件，然后通知相应的Reduce任务来领取属于自己的数据。   </p>
<p><img src="/2020/02/22/Hadoop/1569221806545.png" alt="1569221806545"></p>
<ul>
<li>map输入结果写入缓冲区</li>
<li>缓冲区达到阈值（触发溢写的百分比），溢写到磁盘中</li>
<li>分区内排序合并最后归并成大文件（key，value[]）</li>
</ul>
<h3 id="（2）Reduce-端的Shuffle"><a href="#（2）Reduce-端的Shuffle" class="headerlink" title="（2）Reduce 端的Shuffle"></a>（2）Reduce 端的Shuffle</h3><p>Reduce任务从Map端的不用的Map机器领回属于自己的处理那部分数据，然后对数据进行处理</p>
<p><img src="/2020/02/22/Hadoop/1569222214803.png" alt="1569222214803"></p>
<ul>
<li>领取数据</li>
<li>归并数据</li>
<li>数据给reduce任务</li>
</ul>
<h2 id="4-11-编程案例"><a href="#4-11-编程案例" class="headerlink" title="4.11 编程案例"></a>4.11 编程案例</h2><h3 id="（1）WordCount"><a href="#（1）WordCount" class="headerlink" title="（1）WordCount"></a>（1）WordCount</h3><p>略</p>
<h3 id="（2）PV-UV-的统计"><a href="#（2）PV-UV-的统计" class="headerlink" title="（2）PV UV 的统计"></a>（2）PV UV 的统计</h3><p>pv  网站的总访问数量  算总数</p>
<p>uv 独立活跃用户（日活，月活）  去重（UUID 能代表用户唯一为key）</p>
<h3 id="（3）流量统计之对象输出"><a href="#（3）流量统计之对象输出" class="headerlink" title="（3）流量统计之对象输出"></a>（3）流量统计之对象输出</h3><p>略</p>
<h3 id="（4）流量统计之对象排序输出"><a href="#（4）流量统计之对象排序输出" class="headerlink" title="（4）流量统计之对象排序输出"></a>（4）流量统计之对象排序输出</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Writable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String phone;</span><br><span class="line">    <span class="keyword">private</span> Long upFlow;</span><br><span class="line">    <span class="keyword">private</span> Long downFlow;</span><br><span class="line">    <span class="keyword">private</span> Long sumFlow;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(String phone, Long upFlow, Long downFlow, Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPhone</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPhone</span><span class="params">(String phone)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(Long upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(Long downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span> +</span><br><span class="line">                <span class="string">"phone='"</span> + phone + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">" upFlow="</span> + upFlow +</span><br><span class="line">                <span class="string">" downFlow="</span> + downFlow +</span><br><span class="line">                <span class="string">" sumFlow="</span> + sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 序列化 编码</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.phone);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.upFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.downFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.sumFlow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 反序列化  解码</span></span><br><span class="line"><span class="comment">     *f */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.phone = dataInput.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.sumFlow ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] data = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        context.write(<span class="keyword">new</span> FlowBean(data[<span class="number">0</span>], Long.valueOf(data[<span class="number">1</span>]), Long.valueOf(data[<span class="number">2</span>]), (Long.valueOf(data[<span class="number">2</span>]) + Long.valueOf(data[<span class="number">1</span>]))), NullWritable.get());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 18611781163  FlowBean[]</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">NullWritable</span>, <span class="title">FlowBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">       context.write(key,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序输出;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">       <span class="comment">/* conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\flow.dat"</span>));</span><br><span class="line"></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out4"</span>));</span><br><span class="line">        job.setMapperClass(FlowMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(FlowReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(FlowBean<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="（5）流量统计之对象排序分区输出"><a href="#（5）流量统计之对象排序分区输出" class="headerlink" title="（5）流量统计之对象排序分区输出"></a>（5）流量统计之对象排序分区输出</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序分区输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">FlowBean</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String phone;</span><br><span class="line">    <span class="keyword">private</span> Long upFlow;</span><br><span class="line">    <span class="keyword">private</span> Long downFlow;</span><br><span class="line">    <span class="keyword">private</span> Long sumFlow;</span><br><span class="line">    <span class="keyword">private</span> String area;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FlowBean</span><span class="params">(String phone, Long upFlow, Long downFlow, Long sumFlow, String area)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">        <span class="keyword">this</span>.area = area;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getPhone</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setPhone</span><span class="params">(String phone)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.phone = phone;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getUpFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setUpFlow</span><span class="params">(Long upFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.upFlow = upFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getDownFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setDownFlow</span><span class="params">(Long downFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.downFlow = downFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Long <span class="title">getSumFlow</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setSumFlow</span><span class="params">(Long sumFlow)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = sumFlow;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getArea</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> area;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setArea</span><span class="params">(String area)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.area = area;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"FlowBean&#123;"</span> +</span><br><span class="line">                <span class="string">"phone='"</span> + phone + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", upFlow="</span> + upFlow +</span><br><span class="line">                <span class="string">", downFlow="</span> + downFlow +</span><br><span class="line">                <span class="string">", sumFlow="</span> + sumFlow +</span><br><span class="line">                <span class="string">", area='"</span> + area + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 序列化 编码</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.phone);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.upFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.downFlow);</span><br><span class="line">        dataOutput.writeLong(<span class="keyword">this</span>.sumFlow);</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.area);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 反序列化  解码</span></span><br><span class="line"><span class="comment">     *f */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">this</span>.phone = dataInput.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.upFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.downFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.sumFlow = dataInput.readLong();</span><br><span class="line">        <span class="keyword">this</span>.area = dataInput.readUTF();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(FlowBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.sumFlow &gt; o.sumFlow ? -<span class="number">1</span> : <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序分区输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">FlowBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * 18611781163 700000 10000 hn</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String line = value.toString();</span><br><span class="line"></span><br><span class="line">        String[] data = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         *  phone</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        context.write(<span class="keyword">new</span> FlowBean(data[<span class="number">0</span>], Long.valueOf(data[<span class="number">1</span>]), Long.valueOf(data[<span class="number">2</span>]), (Long.valueOf(data[<span class="number">1</span>]) + Long.valueOf(data[<span class="number">2</span>])),data[<span class="number">3</span>]),NullWritable.get());</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序分区输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * 18611781163  FlowBean[]</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FlowReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">FlowBean</span>, <span class="title">NullWritable</span>, <span class="title">FlowBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(FlowBean key, Iterable&lt;NullWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        context.write(key, NullWritable.get());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.流量统计之对象排序分区输出;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OwnPartitioner</span>&lt;<span class="title">KEY</span>, <span class="title">VALUE</span>&gt; <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">KEY</span>, <span class="title">VALUE</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> HashMap&lt;String, Integer&gt; areaMap = <span class="keyword">new</span> HashMap&lt;String, Integer&gt;();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        areaMap.put(<span class="string">"hn"</span>, <span class="number">0</span>);</span><br><span class="line">        areaMap.put(<span class="string">"henna"</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        areaMap.put(<span class="string">"zz"</span>, <span class="number">1</span>);</span><br><span class="line">        areaMap.put(<span class="string">"kf"</span>, <span class="number">2</span>);</span><br><span class="line">        areaMap.put(<span class="string">"bj"</span>, <span class="number">3</span>);</span><br><span class="line">        areaMap.put(<span class="string">"xy"</span>, <span class="number">4</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(KEY key, VALUE value, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        FlowBean flowBean = (FlowBean) key;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> areaMap.get(flowBean.getArea()) == <span class="keyword">null</span> ? <span class="number">5</span> : areaMap.get(flowBean.getArea());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="（6）学生成绩之合并文件（表连接）"><a href="#（6）学生成绩之合并文件（表连接）" class="headerlink" title="（6）学生成绩之合并文件（表连接）"></a>（6）学生成绩之合并文件（表连接）</h3><p>需求：</p>
<p>student_info.txt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gjf 00001</span><br><span class="line">gzy 00002</span><br><span class="line">jzz 00003</span><br><span class="line">zkf 00004</span><br></pre></td></tr></table></figure>

<p>student_info_class.txt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">00001 yuwen</span><br><span class="line">00001 shuxue</span><br><span class="line">00002 yinyue</span><br><span class="line">00002 yuwen</span><br><span class="line">00003 tiyu</span><br><span class="line">00003 shengwu</span><br><span class="line">00004 tiyu</span><br><span class="line">00004 wuli</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">00001 gjf yuwen shuxue</span><br><span class="line">00002 gzy yinyue yuwen</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.合并文件表连接;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobRunner</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//System.setProperty("HADOOP_USER_NAME", "root");</span></span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">       <span class="comment">/* conf.addResource("conf2/core-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/hdfs-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/mapred-site.xml");</span></span><br><span class="line"><span class="comment">        conf.addResource("conf2/yarn-site.xml");</span></span><br><span class="line"><span class="comment">        conf.set(MRJobConfig.JAR, "G:\\IDEA_WorkSpace\\BigData\\Hadoop_Test\\target\\Hadoop_Test-1.0-SNAPSHOT.jar");</span></span><br><span class="line"><span class="comment">        conf.set("mapreduce.app-submission.cross-platform", "true");</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setJarByClass(JobRunner<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        <span class="comment">//job.setPartitionerClass(OwnPartitioner.class);</span></span><br><span class="line"></span><br><span class="line">        job.setInputFormatClass(TextInputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        TextInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\classinfo"</span>));</span><br><span class="line"></span><br><span class="line">        TextOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(<span class="string">"G:\\Note\\Day02-Hadoop\\数据文件\\out7"</span>));</span><br><span class="line">        job.setMapperClass(StuMapper<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setReducerClass(StuReducer<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setMapOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setMapOutputValueClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        job.setOutputKeyClass(Text<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        job.setOutputValueClass(NullWritable<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line"></span><br><span class="line">        job.setNumReduceTasks(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">        job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.合并文件表连接;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileSplit;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.yarn.webapp.hamlet.Hamlet;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StuMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String STU_INFO = <span class="string">"student_info.txt"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String STU_INFO_CLASS = <span class="string">"student_info_class.txt"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String STU_INFO_FLAG = <span class="string">"a"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String STU_INFO_CLASS_FLAG = <span class="string">"b"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        FileSplit inputSplit = (FileSplit) context.getInputSplit();</span><br><span class="line"></span><br><span class="line">        String filname = inputSplit.getPath().getName();</span><br><span class="line"></span><br><span class="line">        String[] data = value.toString().split(<span class="string">" "</span>);</span><br><span class="line">        String userid = <span class="string">""</span>;</span><br><span class="line">        String flag = <span class="string">""</span>;</span><br><span class="line">        String valueName = <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (filname.contains(STU_INFO)) &#123;</span><br><span class="line"></span><br><span class="line">            userid = data[<span class="number">1</span>];</span><br><span class="line">            flag = STU_INFO_FLAG;</span><br><span class="line"></span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">             * 名字</span></span><br><span class="line"><span class="comment">             * */</span></span><br><span class="line">            valueName = data[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (filname.contains(STU_INFO_CLASS)) &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            userid = data[<span class="number">0</span>];</span><br><span class="line">            flag = STU_INFO_CLASS_FLAG;</span><br><span class="line">            <span class="comment">/*</span></span><br><span class="line"><span class="comment">            学科</span></span><br><span class="line"><span class="comment">            * */</span></span><br><span class="line">            valueName = data[<span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        context.write(<span class="keyword">new</span> Text(userid), <span class="keyword">new</span> Text(flag + <span class="string">" "</span> + valueName));</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.baizhi.合并文件表连接;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> *userid  | 标示+学科或者名字</span></span><br><span class="line"><span class="comment"> * 0001 |a gjf</span></span><br><span class="line"><span class="comment"> * 0001 |b yuwen</span></span><br><span class="line"><span class="comment"> * 0001 |b shuxue</span></span><br><span class="line"><span class="comment"> * */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StuReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        String classList = <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line">        String name = <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Text value : values) &#123;</span><br><span class="line"></span><br><span class="line">            String[] data = value.toString().split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (data[<span class="number">0</span>].equals(<span class="string">"a"</span>)) &#123;</span><br><span class="line">                name = data[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (data[<span class="number">0</span>].equals(<span class="string">"b"</span>)) &#123;</span><br><span class="line"></span><br><span class="line">                classList += <span class="string">" "</span> + data[<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        context.write(<span class="keyword">new</span> Text(key.toString() + <span class="string">" "</span> + name + classList), NullWritable.get());</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p><img src="/2020/02/22/Hadoop/1569228212103.png" alt="1569228212103"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Tabla de contenidos
        </li>
        <li class="sidebar-nav-overview">
          Inicio
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="骚白"
      src="/./source/image/xixi.gif">
  <p class="site-author-name" itemprop="name">骚白</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">entradas</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">骚白</span>
</div>
  <div class="powered-by">Creado mediante <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Tema – <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.7.1
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
